[
  {
    "objectID": "posts/Statistics/BasicTesting.html",
    "href": "posts/Statistics/BasicTesting.html",
    "title": "Introduction To Statistical Testing",
    "section": "",
    "text": "from fastcore.all import *\nfrom IPython.display import clear_output\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom polygon import RESTClient\nfrom utils import view_source_code, get_dollars\nfrom datetime import datetime, timedelta, date\nimport math, time\n\n\npath = Path('../data')\n\n\n\nIn a previous post we created models and created actions we want to take for multiple approaches for stock trading. The question now is, how do we know if they are profitable? How should we measure them? How do we know if we simply got lucky, or if they are reliable?\nTesting is the most important part of the process. If done well you have a good way to determine what strategies should be implemented, and if done poorly you run the risk of implementing non-profitable strategies. I believe you should strive to never sacrifice testing principles because effective testing is your only objective indication to whether you are doing something useful or not. Without effective testing you are “flying blind”.\nThis post will lay the groundwork and cover the basics of testing. The goal of this post is to introduce concept and the three core things that need to be carefully considered for effective testing.\n\nWhat data should you use for testing?\nWhat metric should you use for testing?\nWhat test should you use?\n\nWe are going to walk through the concepts to see each step and understand the importance.\n\n\n\nThe first question we have to ask is what data to we use for testing? Ideally we have 3 subsets of our data (training, validation, and test). Let’s go through what they are used for and why they are important.\n\n\nThe training set is unique because it has no restrictions on what we can do with it. We can look at any piece of data in it. We can normalize data using values in the training set. We can train machine learning models on the training set. This is often the largest subset of our data.\nThis training set is pretty explanatory - we use this for understanding our data and developing our model.\nWe can load it in using the same method as we did in the previous post\n\nraw = pd.read_csv(path/'eod-quotemedia.csv',parse_dates=['date'])\ndf = raw.pivot(index='date', columns='ticker',values='adj_close')\ntrain = df.loc[:pd.Timestamp('2017-1-1')]\n\n\n\n\nThe goal of creating a trading strategy is to have it perform well on data that it was not developed using. We may use data from 2015 - 2020 to create a trading strategy, but the goal is to apply it to 2021 and 2022 to make a profit.\nBecause we want our model to perform on unseen data, we create some restriction to how we use the validation set. We do not train any models on it, and we do not use statistics or data from the validation set when creating our model. It’s data our model has never seen. The validation set is something we can only use to see how well our strategy or model performs.\nThe entire purpose of the validation set is to give us unseen data to evaluate our approaches on. By having this separate validation set we can more accurately determine what works and what doesn’t.\nWe can get our validation set using the same method as we did in the previous post\n\nvalid = df.loc[pd.Timestamp('2017-1-1'):]\n\n\n\n\nThe Test set is very similar to the validation set, but it takes things a step further. It has further restrictions in that is is the final model step before deployment. The main difference is how often you can use it. For the validation set, you can test anything on the validation set as many times as you want. For the test set you only get to look at the test set once for your particular approach.\nFor example, you may try 300 different approaches and parameter changes to your strategy to see what works best. You can check the profitability on each of them using the validation set. Then once you have chosen a strategy, you do a final check to ensure it also performs on the test set. Once you have done that you need a new test set or your project is over.\nThe reason this is important is that you want to ensure that you didn’t get lucky and find a configuration out of your 300 attempts that just happens to work on the validation set but doesn’t work elsewhere. If you try enough combinations eventually you will find something that works, but the test set gives you confidence that your model works because it’s a good strategy and not that you just tried enough things to find something that works on coincidence.\n:::{note} Many people re-use or have more lax rules on the test set. Many people do not use one at all. In this text I am laying out the ideal state I believe we should strive for. If you choose to loosen these restrictions on the test set or do without one, I would strongly encourage you to think hard about it.\nTo get our test set, we could have split our initial data into 3. Because we are a bit concerned about survivorship bias, let’s pull a new test set that uses recent data to and test how these strategies would perform over the last year and a half.\nWe need to get adjusted close price. There are a variety of services that have APIs to pull from, I have picked polgygon to use here because it’s free for what we need.\n\nclient = RESTClient(polygon_free_api_key)\n\n\nif not (path/'polytest_eod-quotemedia.csv').exists():\n    dfs = L()\n    errors = L()\n    for ticker in valid:\n        try:\n            aggs = client.get_aggs(ticker, 1, \"day\", \"2021-01-01\", \"2022-05-31\",adjusted=True)\n            close = {ticker:[o.close for o in aggs]}\n            \n            # Convert millisecond time stamp to date\n            date = L(o.timestamp/1e3 for o in aggs).map(datetime.fromtimestamp)\n            dfs.append(pd.DataFrame(close,index=date))\n        except:\n            errors.append(aggs)\n            print(f\"FAILURE: {ticker}\")\n        \n        # Free api gives 5 API calls / minute - so we need to pace our api calls!\n        time.sleep(60/5)\n    df_test = pd.concat(dfs,axis=1)\n    df_test.to_csv(path/'polytest_eod-quotemedia.csv')\n\ndf_test = pd.read_csv(path/'polytest_eod-quotemedia.csv',index_col=0,parse_dates=True)\ndf_test.index.name='date'\ndf_test.columns.name='ticker'\n\n\ndf_test.iloc[:5,:5]\n\n\n\n\n\n  \n    \n      ticker\n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-01-04\n      118.64\n      15.13\n      157.34\n      129.41\n      105.41\n    \n    \n      2021-01-05\n      119.61\n      15.43\n      157.17\n      131.01\n      106.50\n    \n    \n      2021-01-06\n      122.89\n      15.52\n      166.25\n      126.60\n      105.58\n    \n    \n      2021-01-07\n      126.16\n      15.38\n      167.67\n      130.92\n      106.71\n    \n    \n      2021-01-08\n      127.06\n      15.13\n      170.06\n      132.05\n      107.27\n    \n  \n\n\n\n\n\n\n\n\nNow that we understand what data we will use for testing, let’s start figuring out how well our first model from the previous post performs\nThe next step is to figure out an appropriate metric. There are a variety of ways to measure this and we will walk through a few first steps in this section\n\n\nLet’s take our first model from the previous post and measure how well it does in terms of dollars. After all dollars is what we want to make, so it seems like a reasonable starting point.\n\nfrom SimpleTimeSeries import get_momentum_actions\n\n\nvalid_mom = get_momentum_actions(valid,28,0.08).reset_index()\n\n\ntransactions = pd.melt(valid_mom,id_vars='date',value_name='action')\ntransactions = transactions.loc[transactions.action != '']\ntransactions.columns = ['open_date'] + L(*transactions.columns[1:])\n\n\ntransactions.head()\n\n\n\n\n\n  \n    \n      \n      open_date\n      ticker\n      action\n    \n  \n  \n    \n      0\n      2017-02-14\n      A\n      Buy\n    \n    \n      2\n      2017-02-16\n      A\n      Buy\n    \n    \n      11\n      2017-03-02\n      A\n      Buy\n    \n    \n      13\n      2017-03-04\n      A\n      Buy\n    \n    \n      14\n      2017-03-07\n      A\n      Buy\n    \n  \n\n\n\n\nNow we have a dataframe with all the positions we are going to take and when to take them. But we are missing one crucial piece! When should we close those positions. We cannot make money by simplying buying a stock (ignoring dividends for now) - the profit comes when we actually close the position and sell the stock. Let’s close the position 28 days after opening.\n\ntransactions['close_date'] = transactions.open_date + timedelta(28)\ntransactions = transactions.loc[transactions.open_date < (transactions.open_date.max() - timedelta(28))]\n\n\ntransactions.head()\n\n\n\n\n\n  \n    \n      \n      open_date\n      ticker\n      action\n      close_date\n    \n  \n  \n    \n      0\n      2017-02-14\n      A\n      Buy\n      2017-03-14\n    \n    \n      2\n      2017-02-16\n      A\n      Buy\n      2017-03-16\n    \n    \n      11\n      2017-03-02\n      A\n      Buy\n      2017-03-30\n    \n    \n      13\n      2017-03-04\n      A\n      Buy\n      2017-04-01\n    \n    \n      14\n      2017-03-07\n      A\n      Buy\n      2017-04-04\n    \n  \n\n\n\n\nNext we need to get the stock price on the date of our initial action when we open to position, as well as when we close our position. Let’s start with the price on the day we open.\n\ndf_valid_long = valid.melt(var_name='ticker',value_name='adj_close',ignore_index=False).reset_index()\ndf_valid_long.columns = ['dte','ticker','adj_close']\n\n\ntransactions['open_date'] = pd.to_datetime(transactions.open_date)\ndf_valid_long['dte']      = pd.to_datetime(df_valid_long.dte)\npd.merge(left=transactions,left_on=['open_date','ticker'],\n         right=df_valid_long,right_on=['dte','ticker'],\n         how='left').head(10)\n\n\n\n\n\n  \n    \n      \n      open_date\n      ticker\n      action\n      close_date\n      dte\n      adj_close\n    \n  \n  \n    \n      0\n      2017-02-14\n      A\n      Buy\n      2017-03-14\n      2017-02-14\n      49.703267\n    \n    \n      1\n      2017-02-16\n      A\n      Buy\n      2017-03-16\n      2017-02-16\n      50.147135\n    \n    \n      2\n      2017-03-02\n      A\n      Buy\n      2017-03-30\n      2017-03-02\n      50.679775\n    \n    \n      3\n      2017-03-04\n      A\n      Buy\n      2017-04-01\n      NaT\n      NaN\n    \n    \n      4\n      2017-03-07\n      A\n      Buy\n      2017-04-04\n      2017-03-07\n      50.512092\n    \n    \n      5\n      2017-03-11\n      A\n      Buy\n      2017-04-08\n      NaT\n      NaN\n    \n    \n      6\n      2017-03-16\n      A\n      Buy\n      2017-04-13\n      2017-03-16\n      52.327016\n    \n    \n      7\n      2017-03-18\n      A\n      Buy\n      2017-04-15\n      NaT\n      NaN\n    \n    \n      8\n      2017-05-24\n      A\n      Buy\n      2017-06-21\n      2017-05-24\n      58.568656\n    \n    \n      9\n      2017-05-25\n      A\n      Buy\n      2017-06-22\n      2017-05-25\n      58.637875\n    \n  \n\n\n\n\nUh oh - We have a join that isn’t working correctly and get NaT and NaN! We created our model assuming that we could make transactions any day we want, but the stock market is not open every day. There are limitations to when we can trade openly in the stock market we need to start accounting for.\nWhen we trade using the adjusted close price we added a day because we wouldn’t be able to actually place the trade until the following day. If that day ended up being a Saturday in reality we would have to wait until Monday to place that trade (assuming that monday isn’t a holiday).\nLet’s fix that by getting the next available trading day for each date. Because we know this same thing applies to our close_date, we will fix it there as well.\n\ndef get_next_trading_day(dte,unique_dates):\n    _dates_dict = unique_dates.val2idx()\n    for i in range(10):\n        out = _dates_dict.get(dte.date()+timedelta(i),False) \n        if out != False: return unique_dates[out]\n    raise Exception\n\n\nf = bind(get_next_trading_day,unique_dates=L(*valid.index.date))\n\ntransactions['open_date'] = transactions.open_date.apply(f)\ntransactions['close_date'] = transactions.close_date.apply(f)\n\n\ntransactions['open_date'] = pd.to_datetime(transactions.open_date)\ntransactions['close_date'] = pd.to_datetime(transactions.close_date)\n\nNow we can merge in the price correctly!\n\ntransactions = pd.merge(left=transactions,left_on=['open_date','ticker'],\n                         right=df_valid_long,right_on=['dte','ticker'],\n                          how='left')\ntransactions = pd.merge(left=transactions,left_on=['close_date','ticker'],\n                         right=df_valid_long,right_on=['dte','ticker'],\n                          how='left',\n                          suffixes=('_atOpen','_atClose'))\n\n\ntransactions[['ticker','action',\n             'dte_atOpen','adj_close_atOpen',\n             'dte_atClose','adj_close_atClose']].head(3)\n\n\n\n\n\n  \n    \n      \n      ticker\n      action\n      dte_atOpen\n      adj_close_atOpen\n      dte_atClose\n      adj_close_atClose\n    \n  \n  \n    \n      0\n      A\n      Buy\n      2017-02-14\n      49.703267\n      2017-03-14\n      51.498464\n    \n    \n      1\n      A\n      Buy\n      2017-02-16\n      50.147135\n      2017-03-16\n      52.327016\n    \n    \n      2\n      A\n      Buy\n      2017-03-02\n      50.679775\n      2017-03-30\n      52.593336\n    \n  \n\n\n\n\nOk - now let’s figure out out profit. I am going to create various columns\n\ndef f_committed(x):\n    if x.action in ('Buy','Short'): return x.adj_close_atOpen  \n    else: return 0\ntransactions['committed'] = transactions.apply(f_committed,axis=1)\n\ndef f_revenue(x):\n    if x.action==\"Buy\": return x.adj_close_atClose\n    else:               return x.adj_close_atOpen\ntransactions['revenue'] = transactions.apply(f_revenue,axis=1)\n\ndef f_cost(x):\n    if x.action == 'Buy': return x.adj_close_atOpen  \n    else:                 return x.adj_close_atClose\ntransactions['cost'] = transactions.apply(f_cost,axis=1)\n\ntransactions['profit'] = transactions.revenue - transactions.cost\n\n\ntransactions['committed'] = transactions.apply(f_committed,axis=1)\ntransactions['revenue'] = transactions.apply(f_revenue,axis=1)\ntransactions['cost'] = transactions.apply(f_cost,axis=1)\ntransactions['profit'] = transactions.revenue - transactions.cost\n\n\nget_dollars(transactions[transactions.action=='Buy'].profit.sum()), \\\nget_dollars(transactions[transactions.action=='Short'].profit.sum()), \\\nget_dollars(transactions.profit.sum())\n\n('$7457.36', '$190.98', '$7648.35')\n\n\nGreat! So according to our validation set we made a good chunk of profit (pre-tax). We could buy/short in higher volumes (ie Buy = 10x buys, Short = 10x shorts) to make this profit larger.\nHowever, this really isn’t enough information to determine whether that is a good idea of feasible. I would love to loan someone 100 dollars if they would give me one-thousand dollars back a week later. I would hate to loan someone 1,000,000 dollars on the promise that they would pay me 1,000,900 dollars back in 20 years. The reward just wouldn’t be worth the risk, and I can use that money better in a 20-year span than that.\nLet’s see if we can come up with a better metric that accounts for this.\n\n\n\nInstead of measuring raw dollars, lets consider how much money (capital) we needed in order to make that 90 dollars profit. To do this we need to keep track of our money more carefully than just looking at how much we made at the end. Let’s track out financials by day instead of by transaction o calculate this.\n\n\n\n\n\n\nNote\n\n\n\nI am using “committed” to be the amount we have invested + the amount leveraged. For now, let’s assume that we won’t take out debt and borrow stocks (shorting) if we do not have the capital to cover the initial price we borrowed at\n\n\n\ndf = pd.DataFrame()\nfor cols in [['open_date','committed'],\n             ['close_date','profit'],\n             ['close_date','revenue'],\n             ['open_date','cost']]:\n    _tmp = transactions[cols].groupby(cols[0]).sum()\n    df = pd.merge(df,_tmp,how='outer',left_index=True,right_index=True)\ndf.fillna(0,inplace=True)\ndf.sort_index(inplace=True)\ndf.sample(8)\n\n\n\n\n\n  \n    \n      \n      committed\n      profit\n      revenue\n      cost\n    \n  \n  \n    \n      2017-03-08\n      10584.689356\n      0.000000\n      0.000000\n      10583.600318\n    \n    \n      2017-03-29\n      5225.468769\n      -93.792090\n      11510.549118\n      5250.575701\n    \n    \n      2017-02-16\n      13205.019794\n      0.000000\n      0.000000\n      13187.821518\n    \n    \n      2017-05-04\n      12577.980970\n      -17.387456\n      3561.027174\n      12536.763156\n    \n    \n      2017-03-30\n      3812.650344\n      -42.581923\n      16811.036857\n      3830.179057\n    \n    \n      2017-05-16\n      13842.232885\n      114.058575\n      4165.332503\n      13868.190505\n    \n    \n      2017-05-01\n      6789.531992\n      154.664868\n      5354.190170\n      6754.560876\n    \n    \n      2017-06-15\n      0.000000\n      203.997769\n      10639.412827\n      0.000000\n    \n  \n\n\n\n\nNow we can easily look at when we had the most committed. We are subtracting revenue because once we get money back we can reinvest rather than using new money.\n\ncapital_needed = (df.committed.cumsum()-df.revenue.cumsum()).max()\nget_dollars(capital_needed)\n\n'$274744.63'\n\n\nAnd of course our profit is still the same as we had before because we are just aggregating the data differently.\n\n\n\n\n\n\nTip\n\n\n\nThis is the first time we are using the fastcore’s testing framework. It has several handy and easy to use testing functions, such as testing if 2 numbers are arbitrarily close (useful for floats).\n\n\n\ntest_close(transactions.profit.sum(),df.profit.sum())\nget_dollars(df.profit.sum())\n\n'$7648.35'\n\n\nNow that we see our capital needed and our profit, let’s calculate a percent return\n\nf\"Percent Return: {(df.profit.sum() / capital_needed) * 100:.2f}%\"\n\n'Percent Return: 2.78%'\n\n\n\n\n\nMore commonly rather than using the percent return we want to use the log return. There are a lot of reasons they are advantageous to use, but for now we will cover one that is immediately useful to us.\nSymmetry / Additive\n\nPercent Return\n\nInvest 100 dollars\nGet 50% Return on investment 1 and reinvest\nGet -50% Return on investment 2\nEnd with 75 dollars\n\nLog Return\n\nInvest 100 dollars\nGet 50% Return on investment 1 and reinvest\nGet -50% Return on investment 2\nEnd with 100 dollars\n\n\nThis property where a positive return + an equal-sized negative return = no return makes it much easier to look at returns and figure out if you are winning or losing. Just add up all your log returns to get your overall log return. You have to be much more careful with percent returns.\n\npt = capital_needed + df.profit.sum()\npt_1 = capital_needed\nlog_return = math.log(pt/pt_1)\nf\"{math.log(pt/pt_1)*100:.2f}%\"\n\n'2.75%'\n\n\nAs we calculate the log return we see we get a very similar value to our percent return, but it’s not exactly the same. The advantage of using the log return however is we can accurate get an estimated annualized return.\nThis is great because we can easily have everything thought of as an annualized return so that we have a common time frame to compare investment strategies more easily.\n\ntime_period = (df.index.max().date() - df.index.min().date()).days\n\n\nf\"{(log_return/time_period * 365)*100:.2f}%\"\n\n'7.37%'\n\n\nNow we can just convert to normal return to compare very simply to S&P 500 annual return\n\nf\"{(np.exp(log_return/time_period * 365)-1)*100:.2f}%\"\n\n'7.65%'\n\n\nGo ahead an look up S&P 500 annual returns for each year online and compare. How does this fare?\n\n\n\n\n\n\nIf you bought fifty 1.50 dollar lottery tickets and won 10 million dollars in the lottery, what could you say about your chances to win? Well let’s calculate our rate of return.\n\\[\\frac{10,000,000 - 75}{75} = 133,332.33\\]\nSo based on our calculations, the rate of return for playing the lottery is fantastic. But we know that this doesn’t really reflect reality or mean that it’s a good safe investment strategy. We know that you would’ve just gotten lucky.\nSo how do we determine if our trading strategy is a good strategy, or we just got lucky this time? This is where statistical testing comes in.\nI will cover the basics that I think are key, but if you’d like more detail and practice I reading Statistics: Unlocking the Power of Data. Unlike most statistics books it is extremely applied and focused on data, with lots of real world examples.\n\n\n\n\n\nStatistical testing can be done be done using 2 general approaches. The first is the classical approach with is the most widely known. The second is through bootstrapping. This post will focus on bootstrapping because in my opinion it is the first that should be learned.\nBootstrapping is less commonly accepted but is the more powerful and flexible of the two. In bootstrapping you create a sampling distribution by by taking many samples and performing an experiment. In traditional testing you use clever algebra to approximate a sampling distribution. With todays computer you can almost always use bootstrapping and I believe it it the more powerful and flexible approach.\nWith Bootstrapping you start with just the data and make no other assumptions about the data. With classical methods you start with the data and some assumptions about the data in order to arrive at an answer. If you make incorrect assumptions then you get an incorrect answer - and determining which assumptions are safe to make is not always trivial.\nThe idea of bootstrapping and the power of it has also been spoken of by many of the statistical greats for almost 100 years, much longer than it was feasible to do, as pointed out in this article.\nFor example, that article points out that in 1936 Sir R.A. Fisher spoke about using this bootstrapping approach:\n\nActually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.\n\nHowever while these methods were tedious in 1936, they are trivial thanks to modern computers. We no longer have to do clever algebraic tricks to approximate a sampling distribution - we can just create a sampling distribution.\nThat said, I do believe it’s good and useful to have the ability to use both approaches. Luckily, traditional testing is just algebraic tricks to approximate what we will do here and so it will not be too hard to pick that up on top of bootstrapping. In this section we will focus on Bootstrapping because in my opinion it is the one that should be learned first.\n\n\n\nWith any type of testing we need to understand what we are testing. We are going to have 2 competing hypothesis. The first in the null hypothesis. The null hypothesis is what we assume to be true, until we have evidence otherwise.\nIn the context of trading strategies we might make the hypothesis that our strategy will yield returns equal to that of randomly picking stocks. If we randomly pick stocks we know that it will roughly match the market so that seems like a reasonable thing to want to beat. The null hypothesis is important because our goal of testing is to determine whether we have sufficient evident to reject that null hypothesis.\nThe alternate hypothesis is what we are testing for. For example, if the null hypothesis is that our strategy will yield returns equal that that of investing in random stocks the alternate hypothesis could be that our strategy will yield returns greater than if we were to invest in random stocks.\nFirst we need a few functions:\n\nA function to run our null hypothesis model (random)\nA function to run the model we created and have been exploring throughout the post\nA function to measure the statistic of each run (log return of the models)\n\n\n\nLet’s design a Null Hypothesis. There are many Null Hypothesis’ you can choose, and one of the beauty of bootstrapping is that you can think about it very intuitively and design the experiment exactly how you want easier.\n\ndef run_random(input_df,buy_chance,short_chance,hold_time):\n    input_df_l = input_df.melt(var_name='ticker',value_name='adj_close',\n                                  ignore_index=False).reset_index()\n    \n    # Buy/Short stocks using random chance, probabilities to be passed as arguements\n    buys = input_df_l.loc[np.random.rand(input_df_l.shape[0])<buy_chance].copy()\n    shorts = input_df_l.loc[np.random.rand(input_df_l.shape[0])<short_chance].copy()\n\n    # Concatenate into 1 dataframe\n    buys['action'] = 'Buy'\n    shorts['action'] = 'Short'\n    trans = pd.concat([buys[['date','ticker','action']],\n                     shorts[['date','ticker','action']]])\n    \n    # Close the positions\n    trans['close_date'] = trans.date + timedelta(hold_time)\n    trans = trans.loc[trans.date < (trans.date.max() - timedelta(hold_time))]\n    \n    # Adjust dates for when market is open\n    f = bind(get_next_trading_day,unique_dates=L(*input_df.index.date))\n    \n    trans['date'] = pd.to_datetime(trans.date.apply(f))\n    trans['close_date'] = pd.to_datetime(trans.close_date.apply(f))\n    return trans\n\n\n\n\nWe also need a model that we are hoping performs well that we are trying to evaluate whether it is better than our random null hypothesis or not. For this, we will just use the model we have been working on throughout the post.\n\ndef run_model(input_df,n_periods,threshold,hold_time):\n    valid_mom = get_momentum_actions(input_df,n_periods,threshold).reset_index()\n    trans = pd.melt(valid_mom,id_vars='date',value_name='action')\n    trans = trans.loc[trans.action != '']\n    trans['close_date'] = trans.date + timedelta(hold_time)\n    trans = trans.loc[trans.date < (trans.date.max() - timedelta(hold_time))]\n\n    f = bind(get_next_trading_day,unique_dates=L(*input_df.index.date))\n    trans['date'] = pd.to_datetime(trans.date.apply(f))\n    trans['close_date'] = pd.to_datetime(trans.close_date.apply(f))\n    return trans\n\n\n\n\nNext, we need a function that can take our 2 different model outputs (for null and alternate hypothesis) and calculate the statistic we care about (log return). We do this by drawing on our work earlier in the post.\n\ndef calculate_returns(trans,input_df):\n    input_df_l = input_df.melt(var_name='ticker',value_name='adj_close',\n                                  ignore_index=False).reset_index()\n    # Merge in stock prices\n    trans = pd.merge(left=trans,left_on=['date','ticker'],\n                             right=input_df_l,right_on=['date','ticker'],\n                              how='left')\n\n    trans = pd.merge(left=trans,left_on=['close_date','ticker'],\n                             right=input_df_l,right_on=['date','ticker'],\n                              how='left',\n                              suffixes=('_atOpen','_atClose'))\n    trans = trans[['ticker','action',\n                   'date_atOpen' ,'adj_close_atOpen',\n                   'date_atClose','adj_close_atClose']]\n\n    # Calculate profit for each transaction\n    trans['committed'] = trans.apply(f_committed,axis=1)\n    trans['revenue'] = trans.apply(f_revenue,axis=1)\n    trans['cost'] = trans.apply(f_cost,axis=1)\n    trans['profit'] = trans.revenue - trans.cost\n\n    # Create Daily dataframe\n    df_daily = pd.DataFrame()\n    for cols in [['date_atOpen','committed'],['date_atClose','profit'],\n                 ['date_atClose','revenue'],['date_atOpen','cost']]:\n        _tmp = trans[cols].groupby(cols[0]).sum()\n        df_daily = pd.merge(df_daily,_tmp,how='outer',\n                            left_index=True,right_index=True)\n    df_daily.fillna(0,inplace=True)\n    df_daily.sort_index(inplace=True)\n    \n    # Calculate Log Return\n    pt_1 = (df_daily.committed.cumsum()-df_daily.revenue.cumsum()).max()\n    pt = pt_1 + df_daily.profit.sum()\n    return math.log(pt/pt_1)\n\ncalculate_returns(run_model(valid.iloc[:,:5],28,0.08,28),valid.iloc[:,:5]),\\\ncalculate_returns(run_random(valid.iloc[:,:5],.1,0.05,28),valid.iloc[:,:5]),\n\n(0.01999690938292933, 0.018885915383428743)\n\n\n\n\n\nNow that we have the setup let’s start our test. First for our random model we need to give it a buy and short chance probability. Let’s assume roughly even volume of trades to keep things simple and get the probabilities from our model. In this way our random model trades at the same volume and frequency, just using random selection instead of momentum selection.\n\ndf_long = valid.melt(var_name='ticker',value_name='adj_close',\n                     ignore_index=False).reset_index()\nt = run_model(valid,28,0.08,28)\nbuy_chance = sum(t.action=='Buy')/len(df_long)\nshort_chance = sum(t.action=='Short')/len(df_long)\nprint(f\"Model buy percentage={buy_chance}\")\nprint(f\"Model buy percentage={short_chance}\")\n\nModel buy percentage=0.08691717171717171\nModel buy percentage=0.04311919191919192\n\n\nGreat - Let’s bind these to a function. This is a functional programming principle that is extremely common in functional languages in the form of currying. Python isn’t quite as convenient so we use fastcore’s bind for this (which is like the more common functool partial, with a few added conveniences).\nThis allows us to pass our model functions as parameters which will be convenient when we want to try different parameters. We could also just have all the parameters for either function as arguments in the main run_bootstrap_test function, but that can get rather clunky and confusing.\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,\n                short_chance=short_chance,\n                hold_time=28)\n\nNow we get to the meat of bootstrapping. Really all we are doing is tacking different samples and running both models many times on different samples. In classical statistics the normal distribution or some other distribution is assumed so that we can approximate a sampling distribution. In bootstrapping we just create the sampling distribution directly.\n\ndef run_bootstrap_test(input_df,model_h0,runs,sample_size):\n    r_h0 = L()\n    for i in range(runs):\n        ticker_sample = np.random.choice(input_df.columns,sample_size) \n        valid_sample = input_df[ticker_sample]\n        r_h0.append(calculate_returns(model_h0(valid_sample),input_df))\n    return r_h0\n\n\n\n\nIt’s always helpful to visualize whatever you can. Visualizing the data will help you build intuition, generate additional ideas, spot outliers, and identify possible errors in your code. Let’s start with plotting our initial parameters we are using.\n\nfig,ax = plt.subplots(figsize=(6,6))\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,short_chance=short_chance,hold_time=28)\n\nmodel_h1 = bind(run_model,\n                n_periods=28,threshold=0.08,hold_time=28)\n    \nruns, ss = 200,100\nh0 = run_bootstrap_test(valid,model_h0,runs=runs,sample_size=ss)\nax.set_xlim(-.15,.15)\n\nax.hist(h0,alpha=0.5,bins=60,label = 'h0',range=(-.15,.15))\nx = calculate_returns(model_h1(valid),valid)\nax.scatter(x=x,y=0,marker=\"P\",color='red',s=300)\n\ntitle_r1 = f\"h0 mean={np.array(h0).mean():.4f}\\n\"\ntitle_r2 = f\"h1 value={x:.4f}\"\n\nax.set_title(title_r1 + title_r2)\nax.legend(loc ='upper left') \nplt.show()\n\n\n\n\nThe blue histogram is the distribution of our random sampling. The Red is our models return. Visually the red point seems perfectly in line with the random distribution, but let’s be a bit more scientific about it. Let’s calculate the p-value, and talk about what that means.\n\np_value = (np.array(h0)>x).sum()/runs\nprint(p_value)\n\n0.425\n\n\nThe p-value tells us the percentage of samples from our randomized null hypothesis trials that are more extreme than our model’s return. Roughly 40% of the time we get a more unusual result just using our random selection - which tells us that from this look our model is pretty indistinguishable from our random model.\nSaid in statistical jargon, we failed to reject the null hypothesis.\nLet’s try something else\n\n\n\n\nfig,ax = plt.subplots(figsize=(6,6))\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,short_chance=short_chance,hold_time=14)\nmodel_h1 = bind(run_model,\n                n_periods=28,threshold=0.08,hold_time=14)\n    \nruns, ss = 200,100\nh0 = run_bootstrap_test(valid,model_h0,runs=runs,sample_size=ss)\nax.set_xlim(-.15,.15)\n\nax.hist(h0,alpha=0.5,bins=60,label = 'h0',range=(-.15,.15))\nx = calculate_returns(model_h1(valid),valid)\nax.scatter(x=x,y=0,marker=\"P\",color='red',s=300)\n\ntitle_r1 = f\"h0 mean={np.array(h0).mean():.4f}\\n\"\ntitle_r2 = f\"h1 value={x:.4f}\"\n\nax.set_title(title_r1 + title_r2)\nax.legend(loc ='upper left') \nplt.show()\n\n\n\n\n\np_value = (np.array(h0)>x).sum()/runs\nprint(p_value)\n\n0.33\n\n\nThis isn’t any better. But that’s ok, because that’s how the process works. Most of your experiments will fail, and this was only our first model. Let’s try one more idea.\n\n\n\n\nfig,ax = plt.subplots(figsize=(6,6))\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,short_chance=short_chance,hold_time=28)\nmodel_h1 = bind(run_model,\n                n_periods=56,threshold=0.08,hold_time=28)\n    \nruns, ss = 200,100\nh0 = run_bootstrap_test(valid,model_h0,runs=runs,sample_size=ss)\nax.set_xlim(-.15,.15)\n\nax.hist(h0,alpha=0.5,bins=60,label = 'h0',range=(-.15,.15))\nx = calculate_returns(model_h1(valid),valid)\nax.scatter(x=x,y=0,marker=\"P\",color='red',s=300)\n\ntitle_r1 = f\"h0 mean={np.array(h0).mean():.4f}\\n\"\ntitle_r2 = f\"h1 value={x:.4f}\"\n\nax.set_title(title_r1 + title_r2)\nax.legend(loc ='upper left') \nplt.show()\n\n\n\n\n\np_value = (np.array(h0)>x).sum()/runs\nprint(p_value)\n\n0.015\n\n\nNow we’re talking! A very small percentage of the values from our random model are more extreme than the one we got with our model.\nBut now this invites a new question - how small does the p-value need to be to reject the null hypothesis? 0.05 is the most common and many people use this by default, but what your p-value threshold is is a design choice and trede-off that has real implications on your testing. This is something to carefully consider. Remember, testing is our only objective signal for whether we are doing something right or not so it deserves the most care and consideration.\nLet’s consider the extremes so that we can easily thing about the trade-off.\nWhat if the p-value threshold we pick is large (ie 0.5)?*\nWith a p-value threshold of 0.5, we accept anything where less than 50% of the values from the randomized null hypothesis tests are more extreme. The risk of this means that we will have a lot of false positives. Said another way, LOTS of stuff will pass our test many of them just because of luck.\nSaid simply, we will think a lot of bad ideas are good ideas.\nWhat if the p-value threshold we pick is small (ie 0.000001)?*\nWith a p-value this small almost nothing will pass the test, but if it does we can be pretty confident in it! The problem with this is many good ideas will fail the test just because of bad luck.\nSaid simply, we will have a lot of good ideas that we won’t realize are good ideas. These are missed opportunities.\nThe Tradeoff\nSo really the P value is a trade-off. A lower P value is more cautious and safe but you will miss out on some opportunities. A high P value is riskier and more of your approaches will turn out to fail.\nIt is important to understand that this is a tradeoff. For a more detailed guide to testing read the hypothesis testing material in Statistics: Unlocking the Power of Data\n\n\n\n\n\n\nNow that we have our best model, we will test it on the test set. This is the only time we can look at it - so while we got to try lots of things out on our validation set we only get 1 shot at our test set. Let’s see how it performs\n\nfig,ax = plt.subplots(figsize=(6,6))\n\ndf_test_small = df_test[df_test.index < '2021-05-31']\n\nmodel_h0 = bind(run_random,\n                buy_chance=buy_chance,short_chance=short_chance,hold_time=28)\nmodel_h1 = bind(run_model,\n                n_periods=56,threshold=0.08,hold_time=28)\n    \nruns, ss = 200,100\nh0 = run_bootstrap_test(df_test_small,model_h0,runs=runs,sample_size=ss)\nax.set_xlim(-.15,.15)\n\nax.hist(h0,alpha=0.5,bins=60,label = 'h0',range=(-.15,.15))\nx = calculate_returns(model_h1(df_test_small),df_test_small)\nax.scatter(x=x,y=0,marker=\"P\",color='red',s=300)\n\ntitle_r1 = f\"h0 mean={np.array(h0).mean():.4f}\\n\"\ntitle_r2 = f\"h1 value={x:.4f}\"\n\nax.set_title(title_r1 + title_r2)\nax.legend(loc ='upper left') \nplt.show()\n\n\n\n\n\np_value = (np.array(h0)>x).sum()/runs\nprint(p_value)\n\n0.685\n\n\nOh man, suddenly our model failed the test on the test set after it passed on the validation set. I think it’s helpful to keep in mind this is not a “bad” result. Testing is designed to catch errors and it did - it prevented us from deploying a bad model into production and betting real dollars on it.\nIf a test never caught errors, there’d be no point in performing the test. We do this step BECAUSE it catches things like this.\n\n\n\nEntry-Level:\n\nTest different parameters of the model in the post. What is your best model on the validation set? Ho does it perform on our test set?\nModify model to only place “Buy” order and not “Short” orders. How does this effect return?\n\nMid-Level:\n\nTest the other models in post 1 and create a blog post or write up of your findings\nFind something that could have been done differently with one of the models. Change and test it and create a blog post of your findings.\nCreate an ensemble of multiple momentum models and measure the performance. Create a blog post with findings.\n\nSenior-Level:\n\nFind another type of data that momentum can be applied to, and create a model to use that as a signal. For example, maybe momentum of job postings listed by the company could be a signal of stock performance. You should:\n\nCreate train/valid/test split\nVerify data cleanliness and highlight any potential concerns for incorrectness or biases\nExplain why you are choosing the model you did and why you think it’s a good thing to test\nCreate the model and test various configurations on the validation set\nTest the best one on the test set\nWrite a blog post on what you did, why, choices you made along the way, future research that could be done on this data, and whether you think this could be a productive area for a trading firm to look at more."
  },
  {
    "objectID": "posts/APL/TabularData.html",
    "href": "posts/APL/TabularData.html",
    "title": "Tabular Data Intro",
    "section": "",
    "text": "This article will get us started reading and working with tabular data. The goal is to read in some csv data, do a few basic operations (filtering, aggregation, etc.), and create some basic plots.\n\n]box on -style=max\n\n┌→────────────────┐\n│Was ON -style=max│\n└─────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#load-data-from-csv",
    "href": "posts/APL/TabularData.html#load-data-from-csv",
    "title": "Tabular Data Intro",
    "section": "2.1 Load Data from CSV",
    "text": "2.1 Load Data from CSV\nAPL has a CSV function to read csv data in that we can use. I am using simple stock ticker data for a moving average.\n\neod ← 'eod-quotemedia.csv'\neod_ar ← ⎕CSV eod '' 4\n⍴ eod_ar ⍝ Check Size\n\n┌→───────┐\n│490738 3│\n└~───────┘\n\n\n\nWe can get the head of the dataframe by selecting the first 5 rows and 3 columns. It’s handy to have a small sized piece of data as you develop so we store in it’s own array also. ↑ Lets us specify the head slice.\n\n⎕←eod_ar_s ← 5 3 ↑ eod_ar\n\n┌→──────────────────────────────────┐\n↓ ┌→───┐       ┌→─────┐ ┌→────────┐ │\n│ │date│       │ticker│ │adj_close│ │\n│ └────┘       └──────┘ └─────────┘ │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-01│ │A│      29.9942     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-02│ │A│      29.6501     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-03│ │A│      29.7052     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-05│ │A│      30.4346     │\n│ └──────────┘ └─┘                  │\n└∊──────────────────────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#filter-for-1-ticker",
    "href": "posts/APL/TabularData.html#filter-for-1-ticker",
    "title": "Tabular Data Intro",
    "section": "2.2 Filter for 1 ticker",
    "text": "2.2 Filter for 1 ticker\nNext, we need to filter for a particular ticker. The first thing we need to know is where the column we want to sort on is located. We can of course see that it’s the second column, but let’s pretend we need to use APL for this to get some practice.\n\n⎕ ← ticker_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'\n\n \n2\n \n\n\n\nLet’s break this down a bit:\n\n1⌷eod_ar Gets the first row - this is our header row\n∊ ⊂'ticker' Determines where ‘ticker’ is and returns a mask ([0 1 0]\n⍸ is applied to get the location of the true values (2)\n,/ Applies a concatenation to flatten from an array to a value\n\nThe next step is getting a truth mask (ie [0 1 1 0 0]) that tells us which rows contain the value to filter for, in this case AAPL\n\ntickers ← eod_ar[;ticker_loc] ∊ ⊂ 'AAPL'\n\nSimilar to our first filtering section, ∊ ⊂ 'AAPL' is checking membership of eod_ar[;ticker_loc] to return the mask.\nWe can see we found 1009 rows out of 490,738 that had this ticker.\n\n⍝ We have a truth array (0/1) of whether row contains 'AAPL' or not\n('size',⍴ tickers) , 'Found',+/tickers \n'min',(⌈/ tickers) , 'max',⌊/tickers\n\n┌→─────────────────────┐\n│size 490738 Found 1009│\n└+─────────────────────┘\n\n\n\n┌→──────────┐\n│min 1 max 0│\n└+──────────┘\n\n\n\n+/ distributes the + sign between each item in the list giving us a sum of the vector. The same approach can be used for ⌊ and ⌈ to get minimum and maximum values (0 and 1). ⍴ gives us our shape.\nNext we need to use this mask to actually filter out data. This is quite easy and we can pass our mask with our full array to do that filtering using ⌿\n\nAAPL ← tickers ⌿ eod_ar\n\nSo when we put that together our full solution is:\n\ncol_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'           ⍝ Column Location\nAAPL ← eod_ar ⌿⍨ eod_ar[;col_loc] ∊ ⊂ 'AAPL'  ⍝ Filter Array\n5 3 ↑ AAPL                                    ⍝ Head of array\n\n┌→────────────────────────────┐\n↓ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-01│ │AAPL│ 53.1092 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-02│ │AAPL│ 54.3122 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-03│ │AAPL│ 54.612  │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-05│ │AAPL│ 54.1734 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-08│ │AAPL│ 53.8658 │\n│ └──────────┘ └────┘         │\n└∊────────────────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#calculate-moving-average",
    "href": "posts/APL/TabularData.html#calculate-moving-average",
    "title": "Tabular Data Intro",
    "section": "2.3 Calculate Moving Average",
    "text": "2.3 Calculate Moving Average\nWe will calculate a 90 period moving average over the stock prices for the AAPL ticker we just filtered for.\n\nws ← 90\n\nLet’s start with calculating a moving sum. Instead of using +/ to sum over the full list, we can use +⌿ to get a moving sum based on the ws we give it.\n\nmovsum ← ws +⌿ AAPL[;3]\n\nWhen we have a moving sum, a moving average is easy. Simply divide the moving sum by the window size to get an average.\n\nmovavg ← movsum ÷ ws\n'movavg length:',⍴ movavg\n'AAPL length:', ⍴ AAPL[;3]\n\n┌→─────────────────┐\n│movavg length: 920│\n└+─────────────────┘\n\n\n\n┌→────────────────┐\n│AAPL length: 1009│\n└+────────────────┘\n\n\n\nOur moving average is shorter than our original data because we are not calculating when we don’t have enough data at the beginning of our time period. Let’s pad the beginning with the raw values.\n\npadded ← ((ws-1) ↑ AAPL[;3]) , movavg\n'padded length:',⍴ padded\n'AAPL length:', ⍴ AAPL[;3]\n\n┌→──────────────────┐\n│padded length: 1009│\n└+──────────────────┘\n\n\n\n┌→────────────────┐\n│AAPL length: 1009│\n└+────────────────┘\n\n\n\nSo the full moving average with padding back to original size looks like this\n\nmovavg ← (ws ↑ AAPL[;3]) , ws ÷⍨ ws +⌿ AAPL[;3]"
  },
  {
    "objectID": "posts/APL/TabularData.html#plot",
    "href": "posts/APL/TabularData.html#plot",
    "title": "Tabular Data Intro",
    "section": "2.4 Plot",
    "text": "2.4 Plot\nWe can not plot our original data with our moving average. It’s a bit annoying I have to pass in movavg twice to get it to work. It is probably something I am not understanding about how plotting works in APL.\n\n]Plot AAPL[;3] movavg movavg\n\n\nCreated by Causeway SVG engine - SharpPlot v3.71.0\n\nPaint the paper =====\n \n  \n \nBorder =====\nRegion =====\nX-Axis Ticks =====\nX-Axis tickmarks\n \nY-Axis Ticks =====\nY-Axis tickmarks\n \nAxes =====\n \nY-axis labels\n \n  50\n  60\n  70\n  80\n  90\n  100\n  110\n  120\n  130\n  140\n  150\n  160\n \nfor X-axis labels\n \n  0\n  100\n  200\n  300\n  400\n  500\n  600\n  700\n  800\n  900\n  1000\n  1100\n \nHeading, subheading and footnotes =====\nStart of Line Chart ===========\nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nReset to original origin\n\n\n\n\n┌⊖┐\n⌽0│\n└~┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#full-solution",
    "href": "posts/APL/TabularData.html#full-solution",
    "title": "Tabular Data Intro",
    "section": "2.5 Full Solution",
    "text": "2.5 Full Solution\n\neod_ar ← ⎕CSV 'eod-quotemedia.csv' '' 4\ncol_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'       \nAAPL ← eod_ar ⌿⍨ eod_ar[;col_loc] ∊ ⊂ 'AAPL'  \nmovavg ← (ws ↑ AAPL[;3]) , ws ÷⍨ (ws←90) +⌿ AAPL[;3]\n]Plot AAPL[;3] movavg movavg\n\n\nCreated by Causeway SVG engine - SharpPlot v3.71.0\n\nPaint the paper =====\n \n  \n \nBorder =====\nRegion =====\nX-Axis Ticks =====\nX-Axis tickmarks\n \nY-Axis Ticks =====\nY-Axis tickmarks\n \nAxes =====\n \nY-axis labels\n \n  50\n  60\n  70\n  80\n  90\n  100\n  110\n  120\n  130\n  140\n  150\n  160\n \nfor X-axis labels\n \n  0\n  100\n  200\n  300\n  400\n  500\n  600\n  700\n  800\n  900\n  1000\n  1100\n \nHeading, subheading and footnotes =====\nStart of Line Chart ===========\nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nReset to original origin\n\n\n\n\n┌⊖┐\n⌽0│\n└~┘"
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html",
    "href": "posts/APL/MatrixMultiplication.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "I am working through a couple of linear algebra books/courses as I write this. All content in here will be heavily inspired by those resources:\n\nGilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA.\nApplied Linear Algebra with A. P. L. by Garry Helzer (Author)\n\nI have 2 main goals: + Learn Dyalog APL: APL works very differently than other languages I have done. By learning it I will learn another way of thinking and approaching problems. By having more ways to think and approach problems I become smarter. I want to be smarter. + Improve my mathematical foundation\nTo do this I plan to go through math material and courses in Dyalog APL. In the beginning I will be reviewing basic math while learning APL, but eventually I will get to content where I am both learning APL and math at the same time. This is where I will document what I do.\nWhere to learn APL\nCheck out the fastai apl study group, accompanying videos, and anki decks if you want to learn APL ."
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html#manual-calculation",
    "href": "posts/APL/MatrixMultiplication.html#manual-calculation",
    "title": "Matrix Multiplication",
    "section": "3.1 Manual Calculation",
    "text": "3.1 Manual Calculation\nWe take the rows of N times the columns of M to do a linear combination to do matrix multiplication.\n\\(1\\begin{bmatrix}2\\\\1\\end{bmatrix} + 2\\begin{bmatrix}5\\\\3\\end{bmatrix}\\)\nWe can do this exactly in APL and see our answer.\n\n⎕ ← (col1 ← M[;1] × N[1;]) + (col2 ← M[;2] × N[2;])\n\n┌→───┐\n│12 7│\n└~───┘"
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html#apl-calculation",
    "href": "posts/APL/MatrixMultiplication.html#apl-calculation",
    "title": "Matrix Multiplication",
    "section": "3.2 APL Calculation",
    "text": "3.2 APL Calculation\n\n3.2.1 Dot\nin APL we would normally not write is all out but would write it using the dot (.) function. Let’s figure out what that is and how it works.\nThe . applies the operators the surround it (⍺⍺ and ⍵⍵) in a specific way and order called an inner product.\nIn our matrix multiplication problem it looks like this. \\(\\begin{bmatrix}(1⍵⍵2)⍺⍺(2⍵⍵5)\\\\(1⍵⍵1)⍺⍺(2⍵⍵3)\\end{bmatrix}\\)\nLet’s walk through this in our matrix multiplication example above one operator at a time to understand it a bit better\n\n\n3.2.2 Apply the ⍵⍵ argument\nI like to visualize the first step like this:\n\\(\\begin{bmatrix}⍵⍵&&1\\\\&⍵⍵&2\\\\2&5&\\end{bmatrix}\\)\n\\(\\begin{bmatrix}⍵⍵&&1\\\\&⍵⍵&2\\\\1&3&\\end{bmatrix}\\)\nWe first apply whatever the ⍵⍵ parameter is and combine elements. Just as we did above manually we need to do multiplication here so we know the ⍵⍵ parameter must be ×. These calculation are:\nMatrix 1:\n\n1 × 2 = 2\n2 × 5 = 10\n\nMatrix 2:\n\n1 × 1 = 1\n2 × 3 = 6\n\nSo far we have ⍺⍺.×. We can show the result of our calculations above in matrices.\n\\(\\begin{bmatrix}2&⍺⍺\\\\&10\\end{bmatrix}\\)\n\\(\\begin{bmatrix}1&⍺⍺\\\\&6\\end{bmatrix}\\)\n\n\n3.2.3 Apply the ⍺⍺ argument\nThe next thing the . operator does is combine all the numbers in each of step 1 resulting matrices using ⍺⍺. To get the linear combination we did above we need to add the numbers in each matrix, so the ⍺⍺ operator must be +.\nIf we do that addition:\n\nMatrix 1: 2 + 10 = 12\nMatrix 2: 1 + 6 = 7\n\nLeaving us with our answer of \\(\\begin{bmatrix}12\\\\7\\end{bmatrix}\\)\nSo to do matrix multiplication we simply need to use:\n\nM +.× N\n\n┌→────┐\n↓17 24│\n│10 14│\n└~────┘\n\n\n\n\n\n3.2.4 Dot is flexible\nThis was just 1 example of using the . operator. We used + as ⍺⍺ and × as ⍵⍵ to fit what we needed for this problem.\nNow that we understand that, we can flip our operators and look at ×.+ instead of +.×. We can also do any number of other operators to do lots of different matrix operations. Take a look at the examples below and try calculating them by hand to see what you get!\n\n⍝ using addition.multiplication (normal matrix multiplication) \nM+.×N \n\n┌→─┐\n↓12│\n│ 7│\n└~─┘\n\n\n\n\n⍝ using multiplication.addition\nM×.+N \n\n┌→─┐\n↓21│\n│10│\n└~─┘\n\n\n\n\n⍝ using max.min\nM⌈.⌊N\n\n┌→┐\n↓2│\n│2│\n└~┘\n\n\n\n\n⍝ using addition.subtraction\nM-.+N\n\n┌→─┐\n↓¯4│\n│¯3│\n└~─┘\n\n\n\n\n⍝ using exponent.division\nM*.÷N\n\n┌→──────────┐\n↓5.656854249│\n│1          │\n└~──────────┘\n\n\n\n\n⍝ using factorial.natural_log\nM!.⍟N\n\n┌→───────────┐\n↓1           │\n│0.6309297536│\n└~───────────┘"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html",
    "title": "Numerical Linear Algebra Part 1",
    "section": "",
    "text": "This notebook is a copy of a notebook from the fast.ai fast.ai Numerical linear algebra course. I have modified it to use APL as a learning experience for myself.\nCheck out the original notebook here\nI do not have any business affiliation with fast.ai, and this notebook is not an official fast.ai notebook.\nYou can read an overview of this Numerical Linear Algebra course in this blog post. The course was originally taught in the University of San Francisco MS in Analytics graduate program. Course lecture videos are available on YouTube (note that the notebook numbers and video numbers do not line up, since some notebooks took longer than 1 video to cover).\nYou can ask questions about the course on our fast.ai forums."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#why-study-numerical-linear-algebra",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#why-study-numerical-linear-algebra",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.1 Why study Numerical Linear Algebra?",
    "text": "1.1 Why study Numerical Linear Algebra?\nKey Question of this course: How can we do matrix computations with acceptable speed and acceptable accuracy?\nA list of the Top 10 Algorithms of science and engineering during the 20th century includes: the matrix decompositions approach to linear algebra. It also includes the QR algorithm, which we’ll cover, and Krylov iterative methods which we’ll see an example of. (See here for another take)\n (source: Top 10 Algorithms)\nThere are 4 things to keep in mind when choosing or designing an algorithm for matrix computations: - Memory Use - Speed - Accuracy - Scalability/Parallelization\nOften there will be trade-offs between these categories.\n\n1.1.1 Motivation\nMatrices are everywhere– anything that can be put in an Excel spreadsheet is a matrix, and language and pictures can be represented as matrices as well. Knowing what options there are for matrix algorithms, and how to navigate compromises, can make enormous differences to your solutions. For instance, an approximate matrix computation can often be thousands of times faster than an exact one.\nIt’s not just about knowing the contents of existing libraries, but knowing how they work too. That’s because often you can make variations to an algorithm that aren’t supported by your library, giving you the performance or accuracy that you need. In addition, this field is moving very quickly at the moment, particularly in areas related to deep learning, recommendation systems, approximate algorithms, and graph analytics, so you’ll often find there’s recent results that could make big differences in your project, but aren’t in your library.\nKnowing how the algorithms really work helps to both debug and accelerate your solution."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#matrix-computations",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#matrix-computations",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.2 Matrix Computations",
    "text": "1.2 Matrix Computations\nThere are two key types of matrix computation, which get combined in many different ways. These are: - Matrix and tensor products - Matrix decompositions\nSo basically we’re going to be combining matrices, and pulling them apart again!\n\n1.2.1 Matrix and Tensor Products\n\n1.2.1.1 Matrix-Vector Products:\nThe matrix below gives the probabilities of moving from 1 health state to another in 1 year. If the current health states for a group are: - 85% asymptomatic - 10% symptomatic - 5% AIDS - 0% death\nwhat will be the % in each health state in 1 year?\n(Source: Concepts of Markov Chains)\n\n\n1.2.1.2 Answer\n\n1.2.1.2.1 Set up Data\nCreate category names vector for display purposes\n\n]box on -style=max\n\n┌→────────────────┐\n│Was ON -style=max│\n└─────────────────┘\n\n\n\n\nNames←'Asymptomatic' 'Symptomatic' 'Aids'  'Death'\n\nCreate current health states array\n\nNames⍪Start←1 4⍴.85 .1 .05 0\n\n┌→────────────────────────────────────────────┐\n↓ ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └────────────┘ └───────────┘ └────┘ └─────┘ │\n│                                             │\n│ 0.85           0.1           0.05   0       │\n│                                             │\n└∊────────────────────────────────────────────┘\n\n\n\nCreate transition table/stochastic matrix\n\nTransitions←4 4⍴.9 .07 .02 .01 0 .93 .05 .02 0 0 .85 .15 0 0 0 1\n((⊂'States')⍪Names)⍪(Names,Transitions)\n\n┌→───────────────────────────────────────────────────────────┐\n↓ ┌→─────┐       ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │States│       │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └──────┘       └────────────┘ └───────────┘ └────┘ └─────┘ │\n│ ┌→───────────┐                                             │\n│ │Asymptomatic│ 0.9            0.07          0.02   0.01    │\n│ └────────────┘                                             │\n│ ┌→──────────┐                                              │\n│ │Symptomatic│  0              0.93          0.05   0.02    │\n│ └───────────┘                                              │\n│ ┌→───┐                                                     │\n│ │Aids│         0              0             0.85   0.15    │\n│ └────┘                                                     │\n│ ┌→────┐                                                    │\n│ │Death│        0              0             0      1       │\n│ └─────┘                                                    │\n└∊───────────────────────────────────────────────────────────┘\n\n\n\n\n\n1.2.1.2.2 Answer Calculation\nMultiply together to get ending health states\n\nNames⍪End←Start+.×Transitions\n\n┌→────────────────────────────────────────────┐\n↓ ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └────────────┘ └───────────┘ └────┘ └─────┘ │\n│                                             │\n│ 0.765          0.1525        0.0645 0.018   │\n│                                             │\n└∊────────────────────────────────────────────┘\n\n\n\n\n\n\n1.2.1.3 Matrix-Matrix Products\n(Source: Several Simple Real-world Applications of Linear Algebra Tools)\n\n\n1.2.1.4 Answer\n\n1.2.1.4.1 Set up Data\nLets define the names of all the stuff we are working with for display purposes\n\nitems←'roll' 'bun' 'cake' 'bread'\nshops←'S1' 'S2'\npeople←'P1' 'P2' 'P3'\n\nDemanded quantity of foodstuff\n\nDemanded←3 4⍴6 5 3 1 3 6 2 2 3 4 3 1\n((⊂'')⍪items)⍪(people,Demanded)\n\n┌→─────────────────────────────────┐\n↓ ┌⊖┐  ┌→───┐ ┌→──┐ ┌→───┐ ┌→────┐ │\n│ │ │  │roll│ │bun│ │cake│ │bread│ │\n│ └─┘  └────┘ └───┘ └────┘ └─────┘ │\n│ ┌→─┐                             │\n│ │P1│ 6      5     3      1       │\n│ └──┘                             │\n│ ┌→─┐                             │\n│ │P2│ 3      6     2      2       │\n│ └──┘                             │\n│ ┌→─┐                             │\n│ │P3│ 3      4     3      1       │\n│ └──┘                             │\n└∊─────────────────────────────────┘\n\n\n\n\nPrices←4 2⍴1.5 1 2 2.5 5 4.5 16 17\n((⊂'')⍪shops)⍪(items,Prices)\n\n┌→──────────────────┐\n↓ ┌⊖┐     ┌→─┐ ┌→─┐ │\n│ │ │     │S1│ │S2│ │\n│ └─┘     └──┘ └──┘ │\n│ ┌→───┐            │\n│ │roll│  1.5  1    │\n│ └────┘            │\n│ ┌→──┐             │\n│ │bun│   2    2.5  │\n│ └───┘             │\n│ ┌→───┐            │\n│ │cake│  5    4.5  │\n│ └────┘            │\n│ ┌→────┐           │\n│ │bread│ 16   17   │\n│ └─────┘           │\n└∊──────────────────┘\n\n\n\n\n\n1.2.1.4.2 Answer Calculation\nWe can calculate the price for each shop for each person with matrix multiplication\n\nAns←Demanded+.×Prices\n((⊂'')⍪shops)⍪(people,Ans) ⍝ display\n\n┌→───────────────┐\n↓ ┌⊖┐  ┌→─┐ ┌→─┐ │\n│ │ │  │S1│ │S2│ │\n│ └─┘  └──┘ └──┘ │\n│ ┌→─┐           │\n│ │P1│ 50   49   │\n│ └──┘           │\n│ ┌→─┐           │\n│ │P2│ 58.5 61   │\n│ └──┘           │\n│ ┌→─┐           │\n│ │P3│ 43.5 43.5 │\n│ └──┘           │\n└∊───────────────┘\n\n\n\nThen find the lowest price each person can pay so these folks can budget!\n\nlowest_per_person ← ⌊/Ans\npeople,3 1⍴lowest_per_person ⍝ display\n\n┌→──────────┐\n↓ ┌→─┐      │\n│ │P1│ 49   │\n│ └──┘      │\n│ ┌→─┐      │\n│ │P2│ 58.5 │\n│ └──┘      │\n│ ┌→─┐      │\n│ │P3│ 43.5 │\n│ └──┘      │\n└∊──────────┘\n\n\n\nWe can also identify which store each person should go to based on the min price.\n\n3 2 ⍴ people ⍪ shops ⌷⍨ ⊂ {⍵⍳⌊/⍵}¨↓Ans\n\n┌→──────────┐\n↓ ┌→─┐ ┌→─┐ │\n│ │P1│ │P2│ │\n│ └──┘ └──┘ │\n│ ┌→─┐ ┌→─┐ │\n│ │P3│ │S2│ │\n│ └──┘ └──┘ │\n│ ┌→─┐ ┌→─┐ │\n│ │S1│ │S1│ │\n│ └──┘ └──┘ │\n└∊──────────┘\n\n\n\n\n\n\n1.2.1.5 Image Data\nImages can be represented by matrices.\n (Source: Adam Geitgey)\n\n\n1.2.1.6 Convolution\nConvolutions are the heart of convolutional neural networks (CNNs), a type of deep learning, responsible for the huge advances in image recognitionin the last few years. They are now increasingly being used for speech as well, such as Facebook AI’s results for speech translation which are 9x faster than RNNs (the current most popular approach for speech translation).\nComputers are now more accurate than people at classifying images.\n (Source: Andrej Karpathy)\n (Source: Nvidia)\nYou can think of a convolution as a special kind of matrix product\nThe 3 images below are all from an excellent blog post written by a fast.ai student on CNNs from Different Viewpoints:\nA convolution applies a filter to each section of an image: \nNeural Network Viewpoint: \nMatrix Multiplication Viewpoint: \nLet’s see how convolutions can be used for edge detection in this notebook(originally from the fast.ai Deep Learning Course)\n\n\n\n1.2.2 Matrix Decompositions\nWe will be talking about Matrix Decompositions every day of this course, and will cover the below examples in future lessons:\n\nTopic Modeling (NMF and SVD. SVD uses QR) A group of documents can be represented by a term-document matrix  (source: Introduction to Information Retrieval)  (source: NMF Tutorial)\nBackground removal (robust PCA, which uses truncated SVD) \nGoogle’s PageRank Algorithm (eigen decomposition)\n\n (source: What is in PageRank?)\n\nList of other decompositions and some applications matrix factorization jungle"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#accuracy",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#accuracy",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.3 Accuracy",
    "text": "1.3 Accuracy\n\n1.3.1 Floating Point Arithmetic\nTo understand accuracy, we first need to look at how computers (which are finite and discrete) store numbers (which are infinite and continuous)\n\n1.3.1.1 Exercise\nTake a moment to look at the function \\(f\\) below. Before you try running it, write on paper what the output would be of \\(x_1 = f(\\frac{1}{10})\\). Now, (still on paper) plug that back into \\(f\\) and calculate \\(x_2 = f(x_1)\\). Keep going for 10 iterations.\nThis example is taken from page 107 of Numerical Methods, by Greenbaum and Chartier.\ndef f(x):\n    if x <= 1/2:\n        return 2 * x\n    if x > 1/2:\n        return 2*x - 1\n\n⍝ Translate function to APL\nf←{(2×⍵)-⍵>.5}\n\n\n⍝ Create generator\ngen←{(f⍣⍵)⍺}\n\n\n⍝ First 10 generations\n.1 gen¨ ⍳10\n\n┌→──────────────────────────────────────┐\n│0.2 0.4 0.8 0.6 0.2 0.4 0.8 0.6 0.2 0.4│\n└~──────────────────────────────────────┘\n\n\n\n\n⍝ First 80 generations (left to right top to bottom)\n16 5⍴.1 gen¨ ⍳80\n\n┌→───────────────────────────────────────────────────────────────┐\n↓0.2          0.4          0.8          0.6          0.2         │\n│0.4          0.8          0.6          0.2          0.4         │\n│0.8          0.6          0.2          0.4          0.8         │\n│0.6          0.2          0.4          0.8          0.6         │\n│0.2          0.4          0.8          0.6000000001 0.2000000002│\n│0.4000000004 0.8000000007 0.6000000015 0.200000003  0.400000006 │\n│0.8000000119 0.6000000238 0.2000000477 0.4000000954 0.8000001907│\n│0.6000003815 0.2000007629 0.4000015259 0.8000030518 0.6000061035│\n│0.200012207  0.4000244141 0.8000488281 0.6000976563 0.2001953125│\n│0.400390625  0.80078125   0.6015625    0.203125     0.40625     │\n│0.8125       0.625        0.25         0.5          1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n└~───────────────────────────────────────────────────────────────┘\n\n\n\n\n⍝ Answer after 80 generations\n(f⍣80).1\n\n \n1\n \n\n\n\nWhat went wrong?\n\n\n1.3.1.2 Problem: math is continuous & infinite, but computers are discrete & finite\nTwo Limitations of computer representations of numbers: 1. they can’t be arbitrarily large or small 2. there must be gaps between them\nThe reason we need to care about accuracy, is because computers can’t store infinitely accurate numbers. It’s possible to create calculations that give very wrong answers (particularly when repeating an operation many times, since each operation could multiply the error).\nHow computers store numbers:\n\nThe mantissa can also be referred to as the significand.\nIEEE Double precision arithmetic: - Numbers can be as large as \\(1.79 \\times 10^{308}\\) and as small as \\(2.23 \\times 10^{-308}\\). - The interval \\([1,2]\\) is represented by discrete subset: \\[1, \\: 1+2^{-52}, \\: 1+2 \\times 2^{-52},\\: 1+3 \\times 2^{-52},\\: \\ldots, 2\\]\n\nThe interval \\([2,4]\\) is represented: \\[2, \\: 2+2^{-51}, \\: 2+2 \\times 2^{-51},\\: 2+3 \\times 2^{-51},\\: \\ldots, 4\\]\n\nFloats and doubles are not equidistant:\n Source: What you never wanted to know about floating point but will be forced to find out\nMachine Epsilon\nHalf the distance between 1 and the next larger number. This can vary by computer. IEEE standards for double precision specify \\[ \\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}\\]\nTwo important properties of Floating Point Arithmetic:\n\nThe difference between a real number \\(x\\) and its closest floating point approximation \\(fl(x)\\) is always smaller than \\(\\varepsilon_{machine}\\) in relative terms. For some \\(\\varepsilon\\), where \\(\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}\\), \\[fl(x)=x \\cdot (1 + \\varepsilon)\\]\nWhere * is any operation (\\(+, -, \\times, \\div\\)), and \\(\\circledast\\) is its floating point analogue, \\[ x \\circledast y = (x * y)(1 + \\varepsilon)\\] for some \\(\\varepsilon\\), where \\(\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}\\) That is, every operation of floating point arithmetic is exact up to a relative error of size at most \\(\\varepsilon_{machine}\\)\n\n\n\n1.3.1.3 History\nFloating point arithmetic may seem like a clear choice in hindsight, but there have been many, many ways of storing numbers: - fixed-point arithmetic - logarithmic and semilogarithmic number systems - continued-fractions - rational numbers - possibly infinite strings of rational numbers - level-index number systems - fixed-slash and floating-slash number systems - 2-adic numbers\nFor references, see Chapter 1 (which is free) of the Handbook of Floating-Point Arithmetic. Yes, there is an entire 16 chapter book on floating point!\nTimeline History of Floating Point Arithmetic: - ~1600 BC: Babylonian radix-60 system was earliest floating-point system (Donald Knuth). Represented the significand of a radix-60 floating-point representation (if ratio of two numbers is a power of 60, represented the same) - 1630 Slide rule. Manipulate only significands (radix-10) - 1914 Leonardo Torres y Quevedo described an electromechanical implementation of Babbage’s Analytical Engine with Floating Point Arithmetic. - 1941 First real, modern implementation. Konrad Zuse’s Z3 computer. Used radix-2, with 14 bit significand, 7 bit exponents, and 1 sign bit. - 1985 IEEE 754-1985 Standard for Binary Floating-Point Arithmetic released. Has increased accuracy, reliability, and portability. William Kahan played leading role.\n“Many different ways of approximating real numbers on computers have been introduced.. And yet, floating-point arithmetic is by far the most widely used way of representing real numbers in modern computers. Simulating an infinite, continuous set (the real numbers) with a finite set (the “machine numbers”) is not a straightforward task: clever compromises must be found between, speed, accuracy, dynamic range, ease of use and implementation, and memory. It appears that floating-point arithmetic, with adequately chosen parameters (radix, precision, extremal exponents, etc.), is a very good compromise for most numerical applications.”\nAlthough a radix value of 2 (binary) seems like the pretty clear winner now for computers, a variety of other radix values have been used at various point:\n\nradix-8 used by early machines PDP-10, Burroughs 570 and 6700\nradix-16 IBM 360\nradix-10 financial calculations, pocket calculators, Maple\nradix-3 Russian SETUN computer (1958). Benefits: minimizes beta x p (symbols x digits), for a fixed largest representable number beta^p - 1. Rounding = truncation\nradix-2 most common. Reasons: easy to implement. Studies have shown (with implicit leading bit) this gives better worst-case or average accuracy than all other radices.\n\n\n\n\n1.3.2 Conditioning and Stability\nSince we can not represent numbers exactly on a computer (due to the finiteness of our storage, and the gaps between numbers in floating point architecture), it becomes important to know how small perturbations in the input to a problem impact the output.\n“A stable algorithm gives nearly the right answer to nearly the right question.” –Trefethen\nConditioning: perturbation behavior of a mathematical problem (e.g. least squares)\nStability: perturbation behavior of an algorithm used to solve that problem on a computer (e.g. least squares algorithms, householder, back substitution, gaussian elimination)\nExample: Eigenvalues of a Matrix\n\n1.3.2.0.1 Create matrices\n\n⎕←A←2 2⍴1 1000 0 1\n⎕←B←2 2⍴1 1000 .001 1\n\n┌→─────┐\n↓1 1000│\n│0    1│\n└~─────┘\n\n\n\n┌→─────────┐\n↓1     1000│\n│0.001    1│\n└~─────────┘\n\n\n\n\n\n1.3.2.0.2 Calculate Eigenvalues (quadratic)\n\nI ← {⍵ ⍵ ⍴ 1, ⍵⍴0} ⍝ calculate identity\ntf← {+/+/(⍵×I 2)} ⍝ Calculate Trace\ndf ← {(×/×/(⍵×I 2)+0=I 2)-×/×/(I 2)+⍵×0=I 2} ⍝ Calculate Determinant\n\n⍝ Eigenvalue calculation\ne1←{2÷⍨⍵-.5*⍨(⍵*2)-4×⍺} ⍝ Eigenvalue 1\ne2←{2÷⍨⍵+.5*⍨(⍵*2)-4×⍺} ⍝ Eigenvalue 2\n\n⍝ Get both eigenvalues\neigenvalues←{(d e2 t),(d←df ⍵)e1(t←tf ⍵)} ⍝ Get both eigenvalues\n\n\neigenvalues A\neigenvalues B\n\n┌→──┐\n│1 1│\n└~──┘\n\n\n\n┌→──┐\n│2 0│\n└~──┘\n\n\n\nReminder: Two properties of Floating Point Arithmetic\n\nThe difference between a real number \\(x\\) and its closest floating point approximation \\(fl(x)\\) is always smaller than \\(\\varepsilon_{machine}\\) in relative terms.\nEvery operation \\(+, -, \\times, \\div\\) of floating point arithmetic is exact up to a relative error of size at most \\(\\varepsilon_{machine}\\)\n\nExamples we’ll see: - Classical vs Modified Gram-Schmidt accuracy - Gram-Schmidt vs. Householder (2 different ways of computing QR factorization), how orthogonal the answer is - Condition of a system of equations\n\n\n\n1.3.3 Approximation accuracy\nIt’s rare that we need to do highly accurate matrix computations at scale. In fact, often we’re doing some kind of machine learning, and less accurate approaches can prevent overfitting.\nIf we accept some decrease in accuracy, then we can often increase speed by orders of magnitude (and/or decrease memory use) by using approximate algorithms. These algorithms typically give a correct answer with some probability. By rerunning the algorithm multiple times you can generally increase that probability multiplicatively!\nExample: A bloom filter allows searching for set membership with 1% false positives, using <10 bits per element. This often represents reductions in memory use of thousands of times.\n\nThe false positives can be easily handled by having a second (exact) stage check all returned items - for rare items this can be very effective. For instance, many web browsers use a bloom filter to create a set of blocked pages (e.g. pages with viruses), since blocked web pages are only a small fraction of the whole web. A false positive can be handled here by taking anything returned by the bloom filter and checking against a web service with the full exact list. (See this bloom filter tutorial for more details).\n\n\n1.3.4 Expensive Errors\nThe below examples are from Greenbaum & Chartier.\nEuropean Space Agency spent 10 years and $7 billion on the Ariane 5 Rocket.\nWhat can happen when you try to fit a 64 bit number into a 16 bit space (integer overflow):\n\n⍝from IPython.display import YouTubeVideo\n⍝YouTubeVideo(\"PK_yguLapgA\")\n\nHere is a floating point error that cost Intel $475 million:\n1994 NYTimes article about Intel Pentium Error \nResources: See Lecture 13 of Trefethen & Bau and Chapter 5 of Greenbaum & Chartier for more on Floating Point Arithmetic"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#memory-use",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#memory-use",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.4 Memory Use",
    "text": "1.4 Memory Use\n\n1.4.1 Sparse vs Dense\nAbove we covered how numbers are stored, now let’s talk about how matrices are stored. A key way to save memory (and computation) is not to store all of your matrix. Instead, just store the non-zero elements. This is called sparse storage, and it is well suited to sparse matrices, that is, matrices where most elements are zero.\n\nHere is an example of the matrix from a finite element problem, which shows up in engineering (for instance, when modeling the air-flow around a plane). In this example, the non-zero elements are black and the zero elements are white:  Source\nThere are also special types of structured matrix, such as diagonal, tri-diagonal, hessenberg, and triangular, which each display particular patterns of sparsity, which can be leveraged to reduce memory and computation.\nThe opposite of a sparse matrix is a dense matrix, along with dense storage, which simply refers to a matrix containing mostly non-zeros, in which every element is stored explicitly. Since sparse matrices are helpful and common, numerical linear algebra focuses on maintaining sparsity through as many operations in a computation as possible."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#speed",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#speed",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.5 Speed",
    "text": "1.5 Speed\nSpeed differences come from a number of areas, particularly: - Computational complexity - Vectorization - Scaling to multiple cores and nodes - Locality\n\n1.5.1 Computational complexity\nIf you are unfamiliar with computational complexity and \\(\\mathcal{O}\\) notation, you can read about it on Interview Cake and practice on Codecademy. Algorithms are generally expressed in terms of computation complexity with respect to the number of rows and number of columns in the matrix. E.g. you may find an algorithm described as \\(\\mathcal{O(n^2m)}\\).\n\n\n1.5.2 Vectorization\nModern CPUs and GPUs can apply an operation to multiple elements at once on a single core. For instance, take the exponent of 4 floats in a vector in a single step. This is called SIMD. You will not be explicitly writing SIMD code (which tends to require assembly language or special C “intrinsics”), but instead will use vectorized operations in libraries like numpy, which in turn rely on specially tuned vectorized low level linear algebra APIs (in particular, BLAS, and LAPACK).\n\n1.5.2.1 Matrix Computation Packages: BLAS and LAPACK\nBLAS (Basic Linear Algebra Subprograms): specification for low-level matrix and vector arithmetic operations. These are the standard building blocks for performing basic vector and matrix operations. BLAS originated as a Fortran library in 1979. Examples of BLAS libraries include: AMD Core Math Library (ACML), ATLAS, Intel Math Kernel Library (MKL), and OpenBLAS.\nLAPACK is written in Fortran, provides routines for solving systems of linear equations, eigenvalue problems, and singular value problems. Matrix factorizations (LU, Cholesky, QR, SVD, Schur). Dense and banded matrices are handled, but not general sparse matrices. Real and complex, single and double precision.\n1970s and 1980s: EISPACK (eigenvalue routines) and LINPACK (linear equations and linear least-squares routines) libraries\nLAPACK original goal: make LINAPCK and EISPACK run efficiently on shared-memory vector and parallel processors and exploit cache on modern cache-based architectures (initially released in 1992). EISPACK and LINPACK ignore multi-layered memory hierarchies and spend too much time moving data around.\nLAPACK uses highly optimized block operations implementations (which much be implemented on each machine) LAPACK written so as much of the computation as possible is performed by BLAS.\n\n\n\n1.5.3 Locality\nUsing slower ways to access data (e.g. over the internet) can be up to a billion times slower than faster ways (e.g. from a register). But there’s much less fast storage than slow storage. So once we have data in fast storage, we want to do any computation required at that time, rather than having to load it multiple times each time we need it. In addition, for most types of storage its much faster to access data items that are stored next to each other, so we should try to always use any data stored nearby that we know we’ll need soon. These two issues are known as locality.\n\n1.5.3.1 Speed of different types of memory\nHere are some numbers everyone should know (from the legendary Jeff Dean): - L1 cache reference 0.5 ns - L2 cache reference 7 ns - Main memory reference/RAM 100 ns - Send 2K bytes over 1 Gbps network 20,000 ns - Read 1 MB sequentially from memory 250,000 ns - Round trip within same datacenter 500,000 ns - Disk seek 10,000,000 ns - Read 1 MB sequentially from network 10,000,000 ns - Read 1 MB sequentially from disk 30,000,000 ns - Send packet CA->Netherlands->CA 150,000,000 ns\nAnd here is an updated, interactive version, which includes a timeline of how these numbers have changed.\nKey take-away: Each successive memory type is (at least) an order of magnitude worse than the one before it. Disk seeks are very slow.\nThis video has a great example of showing several ways you could compute the blur of a photo, with various trade-offs. Don’t worry about the C code that appears, just focus on the red and green moving pictures of matrix computation.\nAlthough the video is about a new language called Halide, it is a good illustration the issues it raises are universal. Watch minutes 1-13:\n\n⍝ from IPython.display import YouTubeVideo\n⍝ YouTubeVideo(\"3uiEyEKji0M\")\n\nLocality is hard. Potential trade-offs: - redundant computation to save memory bandwidth - sacrificing parallelism to get better reuse\n\n\n1.5.3.2 Temporaries\nThe issue of “temporaries” occurs when the result of a calculation is stored in a temporary variable in RAM, and then that variable is loaded to do another calculation on it. This is many orders of magnitude slower than simply keeping the data in cache or registers and doing all necessary computations before storing the final result in RAM. This is particularly an issue for us since numpy generally creates temporaries for every single operation or function it does. E.g. \\(a=b\\cdot c^2+ln(d)\\) will create four temporaries (since there are four operations and functions).\n\n\n\n1.5.4 Scaling to multiple cores and nodes\nWe have a separate section for scalability, but it’s worth noting that this is also important for speed - if we can’t scale across all the computing resources we have, we’ll be stuck with slower computation."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#scalability-parallelization",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#scalability-parallelization",
    "title": "Numerical Linear Algebra Part 1",
    "section": "1.6 Scalability / parallelization",
    "text": "1.6 Scalability / parallelization\nOften we’ll find that we have more data than we have memory to handle, or time to compute. In such a case we would like to be able to scale our algorithm across multiple cores (within one computer) or nodes (i.e. multiple computers on a network). We will not be tackling multi-node scaling in this course, although we will look at scaling across multiple cores (called parallelization). In general, scalable algorithms are those where the input can be broken up into smaller pieces, each of which are handled by a different core/computer, and then are put back together at the end."
  },
  {
    "objectID": "posts/APL/Basic Stats.html",
    "href": "posts/APL/Basic Stats.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "This post shows how to calculate statistics in the way I believe should be the default for data scientists, bootstrapping. If you are not familiar with this approach and think it sounds intriguing, check out this page to find a great book to get a fantastic start on bootstrapping and practical statistics.\n\n\n\n\n\n\nNote\n\n\n\nThe quality and simplicity of the APL code in this post was improved thanks to the kind feedback provided by rak1507. It’s awesome to have experienced community members like rak1507 that are willing to read through material written by people newer to array programming and offer feedback in a supportive way.\n\n\n\n\nBecause this opinions seems to put me in the minority of data scientists I am writing a short piece on why bootstrapping here.\nIn classical statistics, very clever algebraic formulas are used to approximate a sampling distribution, and that approximation can be used to calculate a p-value or a confidence interval or other statistics. These formulas rely on assumptions about the data and do not work if those baked in assumptions are not true. In other words they are really shortcuts to calculating an answer that work in specific situations.\nIn modern days, we do not need to approximate a sampling distribution using algebra. We can do something much more elementary, more powerful, and more flexible. Thanks to modern computers, we can just sample our data repeatedly to create an actual sampling distribution and calculate based off of that. You get the same answer. So why do I advocate for a bootstrapping first approach?\n\nIt is simpler and more intuitive. This means it is far easier to craft custom tests and statistics based on whatever you want and reason about what things are.\nBootstrapping assumes nothing other than you have data. Classical statistical formulas are shortcuts that are enabled with baked in assumptions about the data. This means the same boostrapping approach works in basically all situations, where classical statistical formulas only apply in the particular situations they were designed for.\n\nFor this reason I believe it should be the default and you can change to computational shortcuts in the situations where it makes sense (ie you are very confident you understand assumptions, confident they are true in your problem, and the amount of data makes it non-trivial to bootstrap).\n\n\n\n\n\n\nNote\n\n\n\nMuch of this next bit is heavily inspired by Overview of Statistics: UnLocking the Power of Data By Lock, Lock, Lock, Lock, and Lock Published by Wiley (2012). I have summarized key points that I think are relevant to what I want to communicate. For example, the quotes I am using are quotes I originally saw in their article.\n\n\nMany of the top statisticians have known bootstrapping is a more elementary but more flexible approach for longer than the approach was computationally feasible. For example, in 1936 Sir R.A. Fisher (who created the foundations of statistical inference) spoke about using this bootstrapping approach:\n\nActually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.\n\nWhile these methods were tedious in 1936, they are trivial thanks to modern computers. We no longer have to do clever algebraic tricks to approximate a sampling distribution - we can just create a sampling distribution, as George Cobb pointed out in the journal Technology Innovations in Statistical Education.\n\nWhat we teach is largely the technical machinery of numerical approximations based on the normal distribution and its many subsidiary cogs. This machinery was once necessary, because the conceptually simpler alternative based on permutations was computationally beyond our reach. Before computers statisticians had no choice. These days we have no excuse.\n\n\n\n\n\nBecause it’s cool\nBecause most modeling now-a-days is done via array programming and learning, and APL is a fantastic way to get better at that\nBecause it’s a more consistent math noting"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#create-1-sample",
    "href": "posts/APL/Basic Stats.html#create-1-sample",
    "title": "Bootstrapping",
    "section": "2.1 Create 1 Sample",
    "text": "2.1 Create 1 Sample\nLet’s start with creating a sample\n\nCreate some data for us to sample\nGet 5 random numbers between 1 and 10 (no duplicate numbers)\n\n\n⎕←V ← 5?10\n\n┌→─────────┐\n│4 3 10 1 5│\n└~─────────┘\n\n\n\nNext we need to get a random sample of indexes from our data V. We can do that in 3 steps: 1. Get the total number of elements in our data array with ≢V (tally the Vector) 1. Create an array of the size of the sample we want and fill it with ≢V using 10⍴≢V. Create an array of dimension 10 with containing the tally of the vector. APL will broadcase to make all elements equal to ≢V automatically. 1. ? will roll a die for each element between 1 and the value of the element. This gives us random index locations for each sample we want.\nPut that all together and we have code that:\n\nGet random sample of indexes\nGet 10 random numbers between 1 and ≢V (duplicates allowed)\n\n\n⎕←S←?10⍴≢V\n\n┌→──────────────────┐\n│3 2 2 2 2 5 1 2 4 2│\n└~──────────────────┘\n\n\n\nSince that created random index locations, we can look those indexes up in our original vector V to get our random sample.\n\nV[S]\n\n┌→───────────────────┐\n│10 3 3 3 3 5 4 3 1 3│\n└~───────────────────┘\n\n\n\nIf we put that together we get a nice compact way of drawing a sample.\n\nV[?10 ⍴ ≢V]\n\n┌→───────────────────┐\n│3 10 3 3 1 4 3 4 3 3│\n└~───────────────────┘"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#create-sampling-distribution",
    "href": "posts/APL/Basic Stats.html#create-sampling-distribution",
    "title": "Bootstrapping",
    "section": "2.2 Create sampling distribution",
    "text": "2.2 Create sampling distribution\nWe drew a sample, but really what we want to do is draw a whole bunch of samples. All we have to do is create a matrix of indices instead of a vector and the exact same approach works.\nThis is the same as above, except instead of 10 ⍴ to create an array of shape 10, we use 5 10 ⍴ to create an array of shape 5 by 10.\nFor convenience I store the shapes in a variable for later use.\n\nV[?(n←5) (ss←10) ⍴ ≢V]\n\n┌→──────────────────────────┐\n↓ 4 4  3  4  1  5  4 3  3 10│\n│ 1 3 10  5  3  4 10 1  4 10│\n│ 5 1 10 10  5  3  3 4  1  4│\n│10 5 10  1 10  3  1 4 10  1│\n│10 5  5  5  1 10  3 3  1 10│\n└~──────────────────────────┘"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#confidence-intervals",
    "href": "posts/APL/Basic Stats.html#confidence-intervals",
    "title": "Bootstrapping",
    "section": "3.1 Confidence Intervals",
    "text": "3.1 Confidence Intervals\nLets do a bigger sample and calculate our confidence interval using 10000 random numbers between 1 and 100.\n\ndata ← ?10000/100 \n\n10↑data ⍝ look at first 10 values\n\n┌→───────────────────────────┐\n│50 3 15 43 93 60 96 29 71 58│\n└~───────────────────────────┘\n\n\n\nNext we can calculate a sampling distribution and look a a few of them. We use the code from before but with 1000 pulls.\n\nsampling_distribution←data[? (n←1000) (ss←10) ⍴ ≢ data]\n5↑sampling_distribution\n\n┌→─────────────────────────────┐\n↓20 56 92 100 34 89 28 92 10 21│\n│34 95 89  69 35 81 25 25 80 87│\n│77 68 32  77 57 20 10 20 21 95│\n│37 73 19  79 11 88 13  1 90 68│\n│70 42 10  74 62 34 82 17  3 19│\n└~─────────────────────────────┘\n\n\n\nWe want to do a confidence interval on the mean so we need to calculate the mean of each of these samples.\n\n+/ Row-wise sum (trailing axis)\nss÷⍨ divides each element by ss (ss defined when creating sampling distribution)\n\n\nsample_means ← ss÷⍨+/ sampling_distribution\n8↑sample_means\n\n┌→────────────────────────────────────┐\n│54.2 62 47.7 47.9 41.3 63.5 50.7 44.3│\n└~────────────────────────────────────┘\n\n\n\nNow we calculate at 90% confidence interval on our sample mean. That means we are 90% confident our mean will land in the given interval range. This is easy to do because we have calculated the mean of a good sampling distribution so we just need to cut off the top and bottom 5% of values and 90% of the values landed in that range.\n\n⍋ sorts ascending, then cut off first 50 and take the first 900 of that\n\n\norder90 ← 900↑50↓⍋sample_means\n\nGet min and max of middle 90% of sample means, which is our 90% confidence interval. Because our data is sorted we can just get the first and last value.\n\nsample_means[⊣/order90]\nsample_means[⊢/order90]\n\n    \n36.2\n    \n\n\n\n    \n65.5\n    \n\n\n\nWe know we are 90 percent confident that a mean based on a sample size of 10 will land in that range because we did that and found that to be true."
  },
  {
    "objectID": "posts/APL/Basic Stats.html#p-values",
    "href": "posts/APL/Basic Stats.html#p-values",
    "title": "Bootstrapping",
    "section": "3.2 P values",
    "text": "3.2 P values\nLet’s say we have 2 sets of data and we want to know whether some statistics are different between them. We have 10,000 samples of our original data, and we ran an experiment and got 100 datapoints with our new process. We want to calculate a p value to see if it supports our hypothesis that it had a statistically significant impact.\n\n\n\n\n\n\nNote\n\n\n\nStatistically significant impact does not necessarily mean practically significant. This test is doing the basic (are these 2 means different), but often that isn’t really that helpful of a question. Often we want to ask “are these 2 means different by at least X”. After reviewing the simple examples think through how you might be able to design that test via bootstrapping!\n\n\n\n3.2.1 P value on equal means\n\nbaseline ← 1-⍨2×⍨?10000/0\nexperiment ← 0.5-⍨?100/0\n\nThese should have roughly the same means so we should get a large p value and show the difference is not statistically significant\nLet’s run the test and see what we get. First let’s get our statistic from our experiment (mean).\n\n⎕←experiment_mean ← (+/experiment) ÷ ≢experiment\n\n             \n0.02131968282\n             \n\n\n\nNow let’s create our sampling distribution on our baseline.\n\nsampling_distribution←baseline[? (n←1000) (ss←10) ⍴ ≢ baseline]\n\nCalculate the means of each.\n\nsampling_means ← ss ÷⍨ +/sampling_distribution\n\nWe then calculate a p value by seeing what percentage of sample means our experiment mean is more extreme than. We can check this on both ends of the distribution and we would take the smaller one normally.\n\nn ÷⍨ +/ experiment_mean>sampling_means\nn ÷⍨ +/ experiment_mean<sampling_means\n\n     \n0.545\n     \n\n\n\n     \n0.455\n     \n\n\n\n\n\n3.2.2 P value on different means\n\nbaseline ← ?10000/0\nexperiment ← 0.2-⍨?100/0\n\nThese should have different means so we should get a large p value and show the different is not practically significant\nLet’s run the test and see what we get. First let’s get our statistic from our experiment (mean).\n\n⎕←experiment_mean ← (+/experiment) ÷ ≢experiment\n\n           \n0.302065664\n           \n\n\n\nNow let’s create our sampling distribution on our baseline.\n\nsampling_distribution← baseline[? (n←1000) (ss←10) ⍴ ≢ baseline]\n\n\nsampling_means ← ss ÷⍨ +/sampling_distribution\n\nWe then calculate a p value by seeing what percentage of sample means our experiment mean is more extreme than. We can check this on both ends of the distribution, but we would take the smaller one. We can see our p value is quite small - it successfully detected that we likely have a different mean.\n\nn ÷⍨ +/ sampling_means > experiment_mean\nn ÷⍨ +/ sampling_means < experiment_mean\n\n     \n0.993\n     \n\n\n\n     \n0.007"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#belief",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#belief",
    "title": "Simple Time Series (Stock Prices)",
    "section": "4.1 Belief",
    "text": "4.1 Belief\nI believe how a company is doing over the last month can be used to predict how how well it will do in the future. This isn’t much of a leap, but let’s think about a few reasons as to why this may be true.\n\nAvailable Capital: If a company is doing well, it typically means they have more profit. More profit means more that can be reinvested. More reinvestment can mean faster growth.\nEconomies of Scale: Often the more successful a company is the more they can drive down cost in some areas. For example, General Mills can buy sugar at a lower price than a small business due to their buying power. As buying power increase, they can leverage that to drive down costs.\nBrand Recognition: The more successful a business is and the larger it grows, the more brand recognition it has. The more brand recognition it has the more it can leverage it’s brand to grow."
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#hypothesis",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#hypothesis",
    "title": "Simple Time Series (Stock Prices)",
    "section": "4.2 Hypothesis",
    "text": "4.2 Hypothesis\nThe hypothesis for this post is that recent stock performance can be used on its own to predict future stock performance. Regardless of how much we believe that to be true, we should not trade based on this belief until we have evidence. This post will explore several options for using this hypothesis to make trades.\nThe next post will give a foundation in testing and show how we can test and evaluate how well these approaches perform and determine if these are ideas worth keeping."
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#load-data",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#load-data",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.1 Load data",
    "text": "5.1 Load data\nFirst let’s take a look at the data we will be using and talk a bit about it.\n\nraw = pd.read_csv(path/'eod-quotemedia.csv',parse_dates=['date'])\nraw.head(3)\n\n\n\n\n\n  \n    \n      \n      date\n      ticker\n      adj_close\n    \n  \n  \n    \n      0\n      2013-07-01\n      A\n      29.994186\n    \n    \n      1\n      2013-07-02\n      A\n      29.650137\n    \n    \n      2\n      2013-07-03\n      A\n      29.705185\n    \n  \n\n\n\n\n\nL(*raw.ticker.unique())\n\n(#495) ['A','AAL','AAP','AAPL','ABBV','ABC','ABT','ACN','ADBE','ADI'...]\n\n\nWe can see that for each day we have a ticker.\n\n\n\n\n\n\nTip\n\n\n\nA ticker is a symbol associated with a company. For example Apple has the ticker AAPL. To buy shares in Apple you would buy AAPL.\n\n\nFor each of these day|ticker combinations we have an adj_close, or adjusted close price. After every transaction, the price of a stock changes slightly. The adjusted close price is the last stock price of the day. While this is not as detailed as having the price at a more granular level (second, minute, hour, etc.), called tick data, we can use daily close price to test many types of strategies.\n::{note} This is the stock price for the ticker. adjusted means that the prices have been adjusted to account for various actions, such as stock splits (more discussion on this later). close means that it is the price at close of market.\nA good first step after getting tabular data is to use pandas’ describe method. As we do this we see a few good pieces of information to keep in mind: + Overall size of dataset - 409K rows + Very big range in values (~1 - ~1K), which most of them before $100\n\nraw.describe().transpose()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      adj_close\n      490737.0\n      75.100472\n      75.438804\n      1.59\n      36.782424\n      57.499593\n      87.4\n      1011.34"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#null-values",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#null-values",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.2 Null Values",
    "text": "5.2 Null Values\nLet’s take a look and make sure we don’t have any null values to handle. This is one of those things you need to do with every dataset. This is also a great opportunity to show how you can add in simple tests into your code as you go using an assert statement, which will help you catch issues as you iterate and change things.\n\nassert np.array([o==0 for o in raw.isnull().sum()]).all() == True"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#survivorship-bias",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#survivorship-bias",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.3 Survivorship bias",
    "text": "5.3 Survivorship bias\nWe also want to take a quick look at the non-numeric columns to get an idea of what time frame we have and how many tickers. This is often known from the dataset, but it is good practice to look at the dataset and ensure that your understanding of the dataset aligns with what you see in the data.\n\nprint(f\"Date column contains dates from {raw.date.min().date()} to {raw.date.max().date()}\")\n\nDate column contains dates from 2013-07-01 to 2017-06-30\n\n\nLet’s see if all tickers in the dataset have the same start and end date.\n\nticker_cnt = len(raw.ticker.unique())\n\n_min = raw[['ticker','date']].groupby('ticker').min()\n_min = _min[_min.date != '2013-07-01'].count().date\n\n_max = raw[['ticker','date']].groupby('ticker').max()\n_max = _max[_max.date != '2017-06-30'].count().date\n\nprint(f'''Out of {ticker_cnt} tickers:\n  + 20 do do not start on 2013-07-01\n  + 0 do not have an entry for 2017-06-30''')\n\nOut of 495 tickers:\n  + 20 do do not start on 2013-07-01\n  + 0 do not have an entry for 2017-06-30\n\n\nGood thing we checked! Let’s think through what these two data points mean:\n\n20 do do not start on 2013-07-01: This makes sense because some of the companies may not have been founded or fit our criteria until after the start date of the dataset. Maybe they were private companies that went public, or maybe they grew to a large enough size to be part of our universe.\n0 do not have an entry for 2017-06-30: While it’s not definitive proof of an issue, it is cause for concern. This dataset may have a survivorship bias built in. Let’s talk about what survivorship bias is and why this could be a problem.\n\nIn our dataset we see that every ticker has a close price on the last day of the dataset. This means that all of the companies are active at the end of our dataset. What this means is that either:\n\nNo company went out of business or failed in our universe between our July 2013 and June 2017 dates\nSome companies did fail during our universe time period and our dataset does not reflect that.\n\nWhile either are possible, the second option is a common problem. Let’s talk about why failed companies not being a part of our universe is a problem.\nLet’s say I look at the S&P 500 companies in 2022 and build a dataset of their stock prices for 2015 - 2022. We want to tests how well a strategy would have performed in that time range. Any trade has many outcomes for example:\n\nYou could buy a stock and then the company goes out of business and you lose lots of money\nYou could buy a stock and then the price goes up and you profit\nYou could buy a stock and then the price goes down and you lose money\n\nThe problem is that option #1 is impossible in this dataset. We know none of the business went out of business between 2015 and 2022 because they were all in the S&P 500 in 2022. Option #1 is more likely than it should be because we already know the companies on the list are among the largest companies in 2022, so regardless of what company we pick we know it ends up a large company. Option #3 is less likely than it should be because if a company shrunk in size to the point it’s not in the S&P 500 in 2022 it’s not even in the dataset. Our strategies may perform extremely well on our dataset, but when we run it on real data with real money we could be in for a shock!\nBottom line is we need point in time data, or rather data for a given date should only be as accurate as you could have known on that date.\nWhen we see all tickers in the universe have a stock price on the last day, it’s important to verify that this did not happen in your dataset. When we talk about testing later, we will talk about how we can test to ensure we have accurate results."
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#reformat",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#reformat",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.4 Reformat",
    "text": "5.4 Reformat\nNow that we have an basic idea of what’s in our data we can reformat it to a format that will be easier to use for analysis. For what we are doing we will be applying things based on ticker, so let’s give each ticker it’s own column.\n\ndf = raw.pivot(index='date', columns='ticker',values='adj_close')\ndf.iloc[:,:5].head(3)\n\n\n\n\n\n  \n    \n      ticker\n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-07-01\n      29.994186\n      16.176093\n      81.138217\n      53.109173\n      34.924478\n    \n    \n      2013-07-02\n      29.650137\n      15.819834\n      80.722073\n      54.312247\n      35.428076\n    \n    \n      2013-07-03\n      29.705185\n      16.127950\n      81.237299\n      54.612043\n      35.444862\n    \n  \n\n\n\n\nWe can use the same describe as above to see statistics about each ticker.\n\ndf.iloc[:,:5].describe()\n\n\n\n\n\n  \n    \n      ticker\n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n  \n  \n    \n      count\n      1009.000000\n      1009.000000\n      1009.000000\n      1009.000000\n      1009.000000\n    \n    \n      mean\n      40.983757\n      37.811501\n      141.576280\n      100.360320\n      52.977953\n    \n    \n      std\n      5.850163\n      8.816410\n      26.260390\n      22.660593\n      7.897264\n    \n    \n      min\n      29.650137\n      14.770314\n      78.393647\n      53.109173\n      34.924478\n    \n    \n      25%\n      37.656517\n      34.383874\n      125.561609\n      87.186576\n      46.981317\n    \n    \n      50%\n      39.700762\n      39.218491\n      147.450711\n      102.884811\n      53.983957\n    \n    \n      75%\n      43.944859\n      43.681272\n      159.019446\n      114.257784\n      59.212432\n    \n    \n      max\n      60.131015\n      54.071539\n      199.374304\n      153.694280\n      70.780784"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#validation-set",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#validation-set",
    "title": "Simple Time Series (Stock Prices)",
    "section": "5.5 Validation Set",
    "text": "5.5 Validation Set\nWe are going to split our data into 2 groups. This way we have a secondary unseen dataset to test my strategies. I want to develop models on one set of data but evaluate them on different data. This may not seem important but it is absolutely crucial and perhaps the most important concept. The more complex your model and analysis the more important this becomes. This will be discussed in much greater detail in [the next post]((../Statistics/BasicTesting.ipynb), but for now just take my word for it.\nWith time series you generally want your validation set to be the most recent data. This reflects reality best; creating a model on data with the intent to use it on future data.\n\ntrain = df.loc[:pd.Timestamp('2017-1-1 01:00:00')]\nvalid = df.loc[pd.Timestamp('2017-1-1 01:00:00'):]\n\n\nprint(f\"Train Dates: {train.index.min().date()} thru {train.index.max().date()}\")\nprint(f\"Valid Dates: {valid.index.min().date()} thru {valid.index.max().date()}\")\n\nTrain Dates: 2013-07-01 thru 2016-12-30\nValid Dates: 2017-01-03 thru 2017-06-30"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#basic-momentum",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#basic-momentum",
    "title": "Simple Time Series (Stock Prices)",
    "section": "6.1 Basic Momentum",
    "text": "6.1 Basic Momentum\nAs our first model let’s use percent change over recent stock price history. We will take the percent difference between the current stock price and the stock price from a set time in the past. As we think about this approach there are several levers we can pull time find tune our approach:\n\nTime Range: We could use the past 5 days, or past 30 days, or the past year. How far back should we be comparing?\nThreshold: What threshold do we need to cross before we consider it momentous enough to take an action? Is a 1%, 5%, 10%?\nWhat action? Is it just buy and sell? Could we use this to short?\nWhen do we close our position Is it a set time period? Or based on another threshold?\n\nWe will use 28 days for the time range and 8% for our threshold in this example to demonstrate the concept, but in the testing section we will show how to test different parameters.\n\nfrom SimpleTimeSeries import get_momentum_actions\nview_source_code(get_momentum_actions)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef get_momentum_actions(df, n_periods,threshold):\n    _x = df.shift(n_periods)\n    \n    # Calculate percent change\n    momentum_rate = df.apply(lambda x: (x-x.shift(n_periods))/x.shift(n_periods))[n_periods:]\n\n    # Select Action Based on Threshold\n    actions = pd.DataFrame(np.where(momentum_rate < -threshold, 'Short',\n                           np.where(momentum_rate > threshold,  'Buy',\n                                                                 '')),\n                   columns=momentum_rate.columns,index=momentum_rate.index)\n    \n    # Because we use close price, we can't make the trade action until the following day\n    actions.index = actions.index + timedelta(1)\n    \n    return actions\n\n\n\n\n\n\nactions = get_momentum_actions(train.iloc[:,:5],n_periods=28,threshold=0.08)\nactions.head(10)\n\n\n\n\n\n  \n    \n      ticker\n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-08-10\n      \n      Buy\n      \n      Buy\n      Buy\n    \n    \n      2013-08-13\n      \n      Buy\n      \n      Buy\n      \n    \n    \n      2013-08-14\n      Buy\n      \n      \n      Buy\n      \n    \n    \n      2013-08-15\n      \n      \n      \n      Buy\n      \n    \n    \n      2013-08-16\n      \n      \n      \n      Buy\n      \n    \n    \n      2013-08-17\n      \n      \n      \n      Buy\n      \n    \n    \n      2013-08-20\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-21\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-22\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-23\n      \n      Short\n      \n      Buy\n      \n    \n  \n\n\n\n\nThis leaves us with a table of what actions we are going to execute each day for each stock. Let’s look at some other options for using this momentum-esque concept, and then we can test them all and compare how they perform at the end."
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#regression-momentum",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#regression-momentum",
    "title": "Simple Time Series (Stock Prices)",
    "section": "6.2 Regression Momentum",
    "text": "6.2 Regression Momentum\nOur previous approach was just the percent change between 2 dates. But what if one of those days is an outlier? Should we really make a decision based on just 2 data points? To address this concerns we will define momentum slightly differently with the slope of a fit regression.\nFirst, let’s understand the general concept better. Creating these minimal examples and visuals is not just something educational for a post - you should do this in your own projects as well. It will help you think more deeply about your problem.\nBelow I took the stock price for Apple in a 10 day period and plotted it as a scatter plot. Every 4 data points I fit a regression trend line fit to them. We can see that in some groups the trend a very aggressive upward slope, others it’s more neutral, and in others it is a strong negative slope. By using that slope we can determine how much momentum the group of points has. In this way we use all the recent data points to influence momentum and not just the first and last in a period.\n\n# Create Figure\nfig,ax = plt.subplots(figsize=(15,8))\nfig.suptitle(\"Window Regression Lines\",fontsize=50)\n\n# Get 10 data points\n_ = train['AAPL'].iloc[:10] \n\nx1 = _.index.day.values\ny1 = _.values\n\nsz = 4 # Window Size\n\n# Get Windows for x and y\nregr_y = [y1[i:i+sz] for i in range(0,len(y1)-(sz-1))]\nregr_x = [x1[i:i+sz].reshape(-1, 1) for i in range(0,len(x1)-(sz-1))]\n\n# Create Regression lines\nregr = [LinearRegression().fit(x,y).predict(x) for x,y in zip(regr_x,regr_y)]\n\n# Pad Regression Lines for Plotting\nregr_padded = [[None]*i+list(r)+[None]*(len(x1)-sz-i) for i,r in enumerate(regr)]\n\n# Plot\nax.scatter(_.index,y1)\nfor i in range(len(regr)): ax.plot(_.index,regr_padded[i])\n\nplt.show()\n\n\n\n\nSo we take the slope of the trend line to be our momentum. There are very similar levers as before we can change to fine tune out approach:\n\nTime Range: We could use the past 5 days, or past 30 days, or the past year. How far back should we be comparing?\nThreshold: What threshold do we need to cross before we consider it momentous enough to take an action? Is a slope of 1? Slope of 10? Should we do something other than a set value?\nWhat action? Is it just buy and sell? Could we use this to short?\nWhen do we close our position Is it a set time period? Or based on another threshold?\n\nWe will use 28 days for the time range and $5 for our threshold in this example to demonstrate the concept, but in the testing section we will show how to test different parameters.\n\n\n\n\n\n\nNote\n\n\n\nWe are assuming that every day is equally spaced in this regression. I reality, the stock market is not open daily. We are ignoring this for this analysis to keep things simple, but it may be something for us to revisit in a future post!\n\n\nIn this section we are using a straight dollar threshold instead of a percentage. This can cause difficulties because a ticker with a share price at 20 dollars increasing to 25 dollars is a HUGE increase. If a stock goes from 500 to 505 dollars that is not nearly as big of a deal. The point of this post is to show variety of options and get you thinking so we will keep this one as is and see how it shakes out when we test it the approach.\nLet’s codify our approach so we have a function we can use to test with later.\n\nfrom SimpleTimeSeries import get_momentum_regr_actions\nview_source_code(get_momentum_regr_actions)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef get_momentum_regr_actions(df, n_periods,threshold):\n    _x = df.shift(n_periods)\n    \n    # Calculate Momentum\n    mom_window = df.rolling(n_periods)\n    mom_rate = mom_window.apply(lambda y: LinearRegression().\n                                    fit(np.array(range(n_periods)).\n                                    reshape(-1, 1),y).coef_)\n    mom_rate = mom_rate[n_periods:]\n    \n    # Select Action Based on Threshold\n    actions = pd.DataFrame(np.where(mom_rate < -threshold, 'Short',\n                           np.where(mom_rate > threshold,  'Buy',\n                                                                 '')),\n                   columns=mom_rate.columns,index=mom_rate.index)\n    \n    # Because we use close price, we can't make the trade action until the following day\n    actions.index = actions.index + timedelta(1)\n    \n    return actions\n\n\n\n\n\n\nactions = get_momentum_regr_actions(train.iloc[:,[3,-3,-8,1,30]],n_periods=28,threshold=.24)\nactions.head(10)\n\n\n\n\n\n  \n    \n      ticker\n      AAPL\n      ZBH\n      XOM\n      AAL\n      ALXN\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-08-10\n      Buy\n      Buy\n      \n      \n      Buy\n    \n    \n      2013-08-13\n      Buy\n      Buy\n      \n      \n      Buy\n    \n    \n      2013-08-14\n      Buy\n      \n      \n      \n      Buy\n    \n    \n      2013-08-15\n      Buy\n      \n      \n      \n      Buy\n    \n    \n      2013-08-16\n      Buy\n      \n      \n      \n      Buy\n    \n    \n      2013-08-17\n      Buy\n      \n      \n      \n      \n    \n    \n      2013-08-20\n      Buy\n      \n      \n      \n      \n    \n    \n      2013-08-21\n      Buy\n      \n      \n      \n      \n    \n    \n      2013-08-22\n      Buy\n      \n      \n      \n      \n    \n    \n      2013-08-23\n      Buy\n      \n      Short"
  },
  {
    "objectID": "posts/TimeSeries/SimpleTimeSeries.html#bollinger-bands",
    "href": "posts/TimeSeries/SimpleTimeSeries.html#bollinger-bands",
    "title": "Simple Time Series (Stock Prices)",
    "section": "6.3 Bollinger Bands",
    "text": "6.3 Bollinger Bands\nA bollinger band uses a rolling standard deviation to determine when the stock price is unusually high or low. In theory if the price is doing something unexpected we can capitalize on that. So rather than a percent change, or a regression line, we are now picking it based on whether it’s unusually high or low per the standard deviation.\nLet’s walk through graphing a bolldinger band this on a couple tickers so we understand what’s going on. Then we can figure out how to use this to create a trading strategy.\n\nfrom SimpleTimeSeries import calculate_bollinger\nview_source_code(calculate_bollinger)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef calculate_bollinger(df, tickers=['AAPL','MSFT'],window_sz=28,band_sz=2):\n    out = {}\n\n    for ticker in tickers:\n        raw = df.loc[:,ticker] \n        \n        # Calculate window statistics\n        _mean = raw.rolling(window_sz).mean()\n        _std = raw.rolling(window_sz).std()\n\n        # Calculate bands based on window statistics\n        upper_band = _mean + (band_sz*_std)\n        lower_band = _mean - (band_sz*_std)\n        \n        # Combine in a dataframe\n        _out = pd.concat([lower_band, raw, upper_band, ],axis=1)\n        _out.columns = ['lower_band','raw','upper_band']\n        _out['lower_limit'] = _out.raw < _out.lower_band\n        _out['upper_limit'] = _out.raw > _out.upper_band\n\n        out[ticker] = _out\n    return out\n\n\n\n\n\n\ncalculate_bollinger(train,['AAPL','MSFT','GOOG','AMZN'])['AAPL'].sample(3)\n\n\n\n\n\n  \n    \n      \n      lower_band\n      raw\n      upper_band\n      lower_limit\n      upper_limit\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2016-07-08\n      89.498384\n      93.405487\n      97.142827\n      False\n      False\n    \n    \n      2014-10-02\n      91.353234\n      93.418008\n      96.999605\n      False\n      False\n    \n    \n      2016-12-06\n      104.282310\n      107.352218\n      111.253007\n      False\n      False\n    \n  \n\n\n\n\n\nfrom SimpleTimeSeries import plot_bollinger\nview_source_code(plot_bollinger)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef plot_bollinger(data,min_date,plt_cols=2):\n    # Create Plot    \n    rows = int(len(data.keys())/plt_cols)\n    fig, ax = plt.subplots(rows,plt_cols,figsize=(20,8*rows))\n    fig.suptitle(\"Bollinger Bands\",fontsize=50)\n\n    \n    for i,(ticker,df) in enumerate(data.items()):\n        # Determind plot location\n        row_num = int(i / plt_cols) if len(data.keys()) > 2 else i\n        col_num = i - (row_num * plt_cols)\n        \n        # Filter for dates\n        _d = data[ticker].loc[df.index>=min_date]\n        \n        # Draw Plots\n        if plt_cols >2: _tmp = ax[row_num,col_num]\n        else: _tmp = ax[row_num]\n        _tmp.set_title(ticker,fontsize=18)\n        _tmp.plot(_d[['lower_band','raw','upper_band']])\n        _tmp.scatter(_d[_d.lower_limit].index,_d[_d.lower_limit].raw,c='red')\n        _tmp.scatter(_d[_d.upper_limit].index,_d[_d.upper_limit].raw,c='red')\n\n\n\n\n\n\nplot_bollinger(calculate_bollinger(train),min_date='2016-01-01')\n\n\n\n\nThese charts show stock prices for Apple and Microsoft over time in orange. The green is our upper band, which in this case is 2 standard deviations above the mean when looking at the last 28 days of the adjusted close price. The blue is 2 standard deviations below the mean.\nWe’ve plotted red dots anywhere the stock price crosses these bounds. This can only happen if there is a significant enough shift for it to be 2 standard deviations from the mean. Let’s code up this third momentum-esque approach as well so we have a 3rd method to test. Where the price crosses a bollinger band we will take an action!\n\nfrom SimpleTimeSeries import get_bollinger_actions\nview_source_code(get_bollinger_actions)\n\n\n\n\n\n  \n  \n  \n\n\n\n\ndef get_bollinger_actions(df,window_sz=28,band_sz=2):\n    \n    # Calculate Statistics\n    bollinger_data = calculate_bollinger(df, tickers=df.columns,window_sz=window_sz,band_sz=band_sz)\n    \n    # Calculate Actions\n    _d = L()\n    for ticker,dataframe in bollinger_data.items():\n        _d.append(pd.DataFrame(np.where(dataframe['lower_limit'] == True, 'Short',\n                           np.where(dataframe['upper_limit'] == True,  'Buy',\n                                                                 '')),\n                   columns=[ticker],index=dataframe.index))\n    bollinger_actions = pd.concat(_d,axis=1)\n    \n    # Because we use close price, we can't make the trade action until the following day\n    bollinger_actions.index = bollinger_actions.index + timedelta(1)\n    \n    return bollinger_actions[window_sz:]\n\n\n\n\n\n\nactions = get_bollinger_actions(train.iloc[:,:5],window_sz=28,band_sz=2)\nactions.head(10)\n\n\n\n\n\n  \n    \n      \n      A\n      AAL\n      AAP\n      AAPL\n      ABBV\n    \n    \n      date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-08-10\n      \n      \n      \n      \n      \n    \n    \n      2013-08-13\n      \n      \n      \n      \n      \n    \n    \n      2013-08-14\n      \n      Short\n      Buy\n      Buy\n      \n    \n    \n      2013-08-15\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-16\n      \n      Short\n      \n      Buy\n      \n    \n    \n      2013-08-17\n      \n      Short\n      Short\n      Buy\n      Short\n    \n    \n      2013-08-20\n      \n      Short\n      Short\n      Buy\n      \n    \n    \n      2013-08-21\n      \n      \n      \n      \n      Short\n    \n    \n      2013-08-22\n      \n      \n      \n      \n      Short\n    \n    \n      2013-08-23\n      Buy"
  },
  {
    "objectID": "posts/Python/Python.html",
    "href": "posts/Python/Python.html",
    "title": "Python Programming Tips",
    "section": "",
    "text": "Code\nfrom functools import partial\nfrom datetime import datetime\nimport logging, string, pandas as pd, sqlparse\nfrom fastcore.all import *\nfrom fastcore.docments import *\nfrom IPython.display import Markdown,display, HTML\nimport pandas as pd\n\nfrom pygments import highlight\nfrom pygments.lexers import PythonLexer\nfrom pygments.formatters import HtmlFormatter\n\ndef print_function_source(fn):\n    fn = print_decorator\n    formatter = HtmlFormatter()\n    display(HTML('<style type=\"text/css\">{}</style>{}'.format(\n        formatter.get_style_defs('.highlight'),\n        highlight(inspect.getsource(fn), PythonLexer(), formatter))))"
  },
  {
    "objectID": "posts/Python/Python.html#parallel-processing",
    "href": "posts/Python/Python.html#parallel-processing",
    "title": "Python Programming Tips",
    "section": "5.1 Parallel Processing",
    "text": "5.1 Parallel Processing\nSee this blog post\n\n5.1.1 Docments\nNice way of documenting code concisely and being able to access info from code. It’s concise, easy to manipulate to display how you want, and easy to read. I much prefer it over the large numpy style docstrings that are big string blocks\n\nfrom fastcore.docments import *\n\ndef distance(pointa:tuple,  # tuple representing the coordinates of the first point (x,y)\n             pointb:tuple=(0,0) # tuple representing the coordinates of the first point (x,y)\n            )->float: # float representing distance between pointa and pointb\n    '''Calculates the distance between pointa and pointb'''\n    edges = np.abs(np.subtract(pointa,pointa))\n    distance = np.sqrt((edges**2).sum())\n    return distance\n\n\ndocstring(distance)\n\n'Calculates the distance between pointa and pointb'\n\n\n\ndocments(distance)\n\n{ 'pointa': 'tuple representing the coordinates of the first point (x,y)',\n  'pointb': 'tuple representing the coordinates of the first point (x,y)',\n  'return': 'float representing distance between pointa and pointb'}\n\n\n\ndocments(distance,full=True)\n\n{ 'pointa': { 'anno': <class 'tuple'>,\n              'default': <class 'inspect._empty'>,\n              'docment': 'tuple representing the coordinates of the first '\n                         'point (x,y)'},\n  'pointb': { 'anno': <class 'tuple'>,\n              'default': (0, 0),\n              'docment': 'tuple representing the coordinates of the first '\n                         'point (x,y)'},\n  'return': { 'anno': <class 'float'>,\n              'default': <class 'inspect._empty'>,\n              'docment': 'float representing distance between pointa and '\n                         'pointb'}}\n\n\n\n\n5.1.2 Testing\nEveryone agrees testing is important. But not all testing is equal. The needs for unit testing the google code base are not the same as the needs a data scientist needs for building and deploying models, libraries, and most software.\nFastcore is a great tool for most of my testing needs. Fast and simple enough that I can add tests as I build and as I am exploring and building models. I want testing to enhance my development workflow, not be something I have to painstakingly build at the end.\nSometimes simple assert statements are sufficient, but there’s small annoyances. For example, a small change in type can mean a failed test. Sometimes that change in type should cause a failure, sometimes I’m ok if it’s a different type if the values are the same\n\nfrom fastcore.test import *\n\n\ntest_eq([1,2],(1,2))\n\nFor floating points it has handy functionality for that, which is very common in data science. For example, we may want .1 + .1 + .1 == .3 to be true, because they are close enough based on floating point precision\n\n.1 + .1 + .1 == .3\n\nFalse\n\n\n\ntest_close(.1 + .1 + .1, .3)\n\nWe can test that something fails, if there are particular situation we want to ensure raise errors.\n\ndef _fail(): raise Exception(\"foobar\")\ntest_fail(_fail)\n\nWe can test if 2 lists have the same values, just in different orders (convenient for testing some situations with random mini-batches).\n\na = list(range(5))\nb = a.copy()\nb.reverse()\ntest_shuffled(a,b)\n\nThere’s more of course, check out the docs\n\n\n5.1.3 L\nL is a replacement for a list, but with lots of adding functionality. Some of it are functional programming concepts, some is numpy like stuff, and some is just niceities (like cleaner printing).\n\nalist = L(1,2,3,4,3)\n\n\nalist.sort()\nalist.sorted()\n\n(#5) [1,2,3,3,4]\n\n\n\nalist.unique()\n\n(#4) [1,2,3,4]\n\n\n\nalist.filter(lambda x: x < 3)\n\n(#2) [1,2]\n\n\n\nalist.map(lambda x: x * 2)\n\n(#5) [2,4,6,8,6]\n\n\n\n\n5.1.4 AttrDict\nAttrdict is another nice thing from fastcore, that makes dictionaries a bit nicer to use.\n\nregdict = {'a':2,'b':3}\nadict = AttrDict({'a':2,'b':3})\n\n\nadict\n\n{'a': 2, 'b': 3}\n\n\n\nadict.a\n\n2\n\n\n\ndef _fail(): return regdict.a\ntest_fail(_fail)"
  },
  {
    "objectID": "posts/Python/Python.html#filter",
    "href": "posts/Python/Python.html#filter",
    "title": "Python Programming Tips",
    "section": "7.1 Filter",
    "text": "7.1 Filter\nFilter is a common higher order function.\n\nL(1,2,3,4,5).filter(lambda x: x>3)\n\n(#2) [4,5]\n\n\nThis is very flexible because we can put filtering logic of any complexity in a function and use that to filter a list of any type."
  },
  {
    "objectID": "posts/Python/Python.html#map",
    "href": "posts/Python/Python.html#map",
    "title": "Python Programming Tips",
    "section": "7.2 Map",
    "text": "7.2 Map\nMap is another very common higher order function.\n\nL(1,2,3,4,5).map(lambda x: x**2)\n\n(#5) [1,4,9,16,25]\n\n\nIt is again super flexible because we can apply a function of any complexity to have it be applied and modify each element of the list.\n\nL(1,2,3,4,5).map(lambda x: string.ascii_lowercase[x])\n\n(#5) ['b','c','d','e','f']"
  },
  {
    "objectID": "posts/Python/Python.html#simple-logging",
    "href": "posts/Python/Python.html#simple-logging",
    "title": "Python Programming Tips",
    "section": "7.3 Simple Logging",
    "text": "7.3 Simple Logging\nWe could make a function for logging, where we can pass a function in that we want to use for logging (ie info vs warning).\n\ndef log_stuff(msg,fn=logger.info,**kwargs): \n    dt = get_current_time()\n    fn(f\"{dt}|{msg}\")\n    for k,v in kwargs.items(): fn(f\"{dt}|{k}={v}\")\n\n\nlog_stuff('abcd',a=1,b=55)\n\n\n!tail -3 mylog.log\n\nINFO:root:20221106_193211|abcd\nINFO:root:20221106_193211|a=1\nINFO:root:20221106_193211|b=55\n\n\n\nlog_stuff('something might be awry',fn=logger.critical,a=1,b=55)\n\n\n!tail -3 mylog.log\n\nCRITICAL:root:20221106_193211|something might be awry\nCRITICAL:root:20221106_193211|a=1\nCRITICAL:root:20221106_193211|b=55"
  },
  {
    "objectID": "posts/Python/Python.html#file-processor",
    "href": "posts/Python/Python.html#file-processor",
    "title": "Python Programming Tips",
    "section": "7.4 File Processor",
    "text": "7.4 File Processor\nYou can also make a generic file processor that you can pass callbacks to. This file processor can include log statements to log what you’re doing, so you can minimize repeating lots of code. For now, we’ll do a simple processor, and callbacks to clean and format a messy sql file.\n\ndef process_file(fpath,callbacks): \n    with open(fpath, \"r\") as f: contents = f.read()\n    for callback in callbacks: contents = callback(contents)\n    return contents"
  },
  {
    "objectID": "posts/Python/Python.html#format-and-clean-sql-file",
    "href": "posts/Python/Python.html#format-and-clean-sql-file",
    "title": "Python Programming Tips",
    "section": "7.5 Format and clean SQL file",
    "text": "7.5 Format and clean SQL file\n\nsql_formatter_cb = partial(sqlparse.format,\n                strip_comments=True,comma_first=True,\n                keyword_case='upper', identifier_case='lower',\n                reindent=True, indent_width=4,)\n\n\n\nqrys = process_file('test.sql',[sql_formatter_cb,sqlparse.split])\n\n\ndef sql_pprint(sql): display(Markdown(f\"```sql\\n\\n{sql}\\n\\n```\"))\nfor qry in qrys: sql_pprint(qry)\n\n\nSELECT top 25 *\nFROM some_table;\n\n\n\nSELECT count(1)\nFROM another TABLE ;\n\n\n\nSELECT date_time\n     , mbr_id\n     , transactions\n     , count(1)\nFROM table3\nWHERE date_time > '2021-02-02'\nGROUP BY 1\n       , 2\n       , 3;"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example",
    "href": "posts/Python/Python.html#silly-simple-example",
    "title": "Python Programming Tips",
    "section": "8.1 Silly Simple Example",
    "text": "8.1 Silly Simple Example\n\ndef add_another(func):\n    def wrapper(number):\n        print(f\"The decorator took over!\")\n        print(f\"I could log the original number ({number}) here!\")\n        print(f\"Or I could log the original answer ({func(number)}) here!\")\n        return func(number) + 1\n    return wrapper\n    \n@add_another\ndef add_one(number): return number + 1\n\nSo when we use a decorator, the code in the wrapper function is called instead of the original function. Typically the wrapper function calls the original function (otherwise there would be no point in decorating it as you’d just have a new unrelated function)."
  },
  {
    "objectID": "posts/Python/Python.html#useful-example",
    "href": "posts/Python/Python.html#useful-example",
    "title": "Python Programming Tips",
    "section": "8.2 Useful Example",
    "text": "8.2 Useful Example\nFor example, maybe you want to print (or log) particular function call times and the args. See this decorator that does just that (and can be used on methods too)\n\nfrom datetime import datetime\n\n\ndef print_decorator(func):\n    def wrapper(*args, **kwargs):\n        print(f\"{datetime.now()}:{func}:args={args}:kwargs={kwargs}\")\n        return func(*args, **kwargs)\n    return wrapper\n\n\n@print_decorator\ndef simple_add(a,b): return a + b\n\n\nsimple_add(2,4)\n\n2022-11-02 14:18:56.635936:<function simple_add>:args=(2, 4):kwargs={}\n\n\n6\n\n\n\n@print_decorator\ndef complex_add(a,b,*args,**kwargs): \n    out = a + b\n    for arg in args: out = out + arg\n    for kwarg in kwargs.values(): out = out + kwarg\n    return out\n\n\ncomplex_add(5,2,3,foo=6,bar=10)\n\n2022-11-02 14:18:57.716085:<function complex_add>:args=(5, 2, 3):kwargs={'foo': 6, 'bar': 10}\n\n\n26"
  },
  {
    "objectID": "posts/Python/Python.html#use-on-existing-functions",
    "href": "posts/Python/Python.html#use-on-existing-functions",
    "title": "Python Programming Tips",
    "section": "8.3 Use on Existing Functions",
    "text": "8.3 Use on Existing Functions\nWhat we have seen is applying a decorator to functions we fully define but we can also apply them to previously existing functions like ones we import from a library. This is helpful not just in understanding one way you can extend an existing libraries functionality, but also in understanding what decorators are. They aren’t magical.\nLet’s add logging to pd.DataFrame using our existing decorator so we can see when a dataframe is constructed.\n\nLoggingDataFrame = print_decorator(pd.DataFrame)\ndf = LoggingDataFrame([1,2,3])\n\n2022-11-02 14:53:16.323144:<class 'pandas.core.frame.DataFrame'>:args=([1, 2, 3],):kwargs={}\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n  \n\n\n\n\nThe key thing to notice here is that the @ syntax really isn’t doing anything magical. It’s just passing the function into the decorator and using that as the function definition. It’s just syntactic sugar for a higher order function that takes a function and returns a function.\nTo understand why this works, think through what our decorator is doing. 1. It’s a function that takes a function as an argument 2. It creates a new function called wrapper. This wrapper function called the argument passed into it, but also has other code. 3. It returns that function as the output\n\nprint_function_source(print_decorator)\n\ndef print_decorator(func):\n    def wrapper(*args, **kwargs):\n        print(f\"{datetime.now()}:{func}:args={args}:kwargs={kwargs}\")\n        return func(*args, **kwargs)\n    return wrapper"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example-1",
    "href": "posts/Python/Python.html#silly-simple-example-1",
    "title": "Python Programming Tips",
    "section": "9.1 Silly Simple Example",
    "text": "9.1 Silly Simple Example\n\nclass aClass: a = 2\n    \nclass bClass(aClass): pass\n    \naClass.a == bClass.a\n\nTrue"
  },
  {
    "objectID": "posts/Python/Python.html#useful-examples",
    "href": "posts/Python/Python.html#useful-examples",
    "title": "Python Programming Tips",
    "section": "9.2 Useful Examples",
    "text": "9.2 Useful Examples\nIn many cases there are common things we want to inherit in lots of classes. One example is having access to the date. Often you want this for logging, or printing, or any number of things. By subclassing you don’t have to reformat the date each time in your classes.\n\nclass DateMinuteMixin:\n    date_format='%Y%m%d_%H%M%S'\n    dte = datetime.now()\n\n    @property\n    def date_str(self): return self.dte.strftime(self.date_format)\n\nAnother handy use is to have generic behavior for handling different file types. In this case, we have a mixin where it opens and reads a sql file. Rather than rewriting this code for every class that needs to read a sql file, you can inherit from a class when you need that functionality.\n\n\n\n\n\n\nTip\n\n\n\nYou can define an abstract property like below to let users know that after inheriting this class, they need to define that property. In this case, they define the sql_filepath, and they get the contents of the file for free via the other methods.\n\n\n\nimport abc\n\nclass SqlFileMixin:\n    @abc.abstractproperty\n    def sql_filepath(self):\n        pass\n\n    @property\n    def sql_file(self):\n        return open(self.sql_filepath)\n\n    @property\n    def query(self):\n        return self.sql_file.read()"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example-2",
    "href": "posts/Python/Python.html#silly-simple-example-2",
    "title": "Python Programming Tips",
    "section": "11.1 Silly Simple Example",
    "text": "11.1 Silly Simple Example\n\ndef mapper(items,fn):\n    for item in items: yield item\n\n\nit = mapper([2,4,6,8],square)\nit\n\n<generator object mapper>\n\n\n\nnext(it), next(it), next(it)\n\n(2, 4, 6)\n\n\nYou can also process it sequentially in a loop.\n\nfor item in mapper([2,4,6,8],square): \n    print(item)\n\n2\n4\n6\n8"
  },
  {
    "objectID": "posts/Python/Python.html#useful-example-1",
    "href": "posts/Python/Python.html#useful-example-1",
    "title": "Python Programming Tips",
    "section": "11.2 Useful Example",
    "text": "11.2 Useful Example\n\n11.2.1 File Streaming\n\nprint_plus = partial(print,end='\\n++++++\\n')\n\nwith open('test.txt', 'rb') as f:\n    iterator = iter(partial(f.read, 64), b'')\n    print_plus(type(iterator))\n    for block in iterator: print_plus(block)\n\n<class 'callable_iterator'>\n++++++\nb'one\\ntwo\\nthree\\nfour\\nfive\\nsix\\nseven\\neight\\nnine\\nten\\neleven\\ntwelve\\nt'\n++++++\nb'hirteen\\nninety nine thousand nine hundred ninety\\nninety nine tho'\n++++++\nb'usand nine hundred ninety one\\nninety nine thousand nine hundred '\n++++++\nb'ninety two\\nninety nine thousand nine hundred ninety three\\nninety'\n++++++\nb' nine thousand nine hundred ninety four\\nninety nine thousand nin'\n++++++\nb'e hundred ninety five\\nninety nine thousand nine hundred ninety s'\n++++++\nb'ix\\nninety nine thousand nine hundred ninety seven\\nninety nine th'\n++++++\nb'ousand nine hundred ninety eight\\nninety nine thousand nine hundr'\n++++++\nb'ed ninety nine\\n'\n++++++"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html",
    "href": "posts/Clustering/KMeansFromScratch.html",
    "title": "K-Means From Scratch",
    "section": "",
    "text": "import math, random, matplotlib.pyplot as plt, operator, torch\nfrom functools import partial\nfrom fastcore.all import *\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\n\ntorch.manual_seed(42)\ntorch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n\n\ndef plot_data(centroids:torch.Tensor,# Centroid coordinates\n              data:torch.Tensor, # Data Coordinates\n              n_samples:int, # Number of samples\n              ax:plt.Axes=None # Matplotlib Axes object\n             )-> None:\n    '''Creates a visualization of centroids and data points for clustering problems'''\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#calculate-distance",
    "href": "posts/Clustering/KMeansFromScratch.html#calculate-distance",
    "title": "K-Means From Scratch",
    "section": "3.1 Calculate Distance",
    "text": "3.1 Calculate Distance\nIn order to initialize our centroids we need to be able to calculate distances, so let’s do that first.\nGiven a tensor of centroid coordinates and a tensor of data coordinates we calculate distance by: + Subtract centroids coordinates from data points coordinates + Take absolute value of distances + Pythagorean Calculation + Square coordinates + Add them together + Take the Square Root\nThat gives us the euclidean distance between each data point and each centroid.\n\ndef calculate_distances(centroids:torch.Tensor, # Centroid coordinates\n                        data:torch.Tensor # Data points you want to cluster\n                       )-> torch.Tensor: # Tensor containing euclidean distance between each centroid and data point    \n    '''Calculate distance between centroids and each datapoint'''\n    axis_distances = data.reshape(-1,1,2).sub(centroids.reshape(1,-1,2)).abs()\n    euclid_distances = axis_distances.square().sum(axis=-1).sqrt()\n    return euclid_distances"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#initialize-centroids",
    "href": "posts/Clustering/KMeansFromScratch.html#initialize-centroids",
    "title": "K-Means From Scratch",
    "section": "3.2 Initialize Centroids",
    "text": "3.2 Initialize Centroids\nWhere we initialize our centroids is really important. If we don’t have good initialization we are very likely to get stuck in a local optimum. Especially with 6 centroids. One option is to run the algorithm many times and pick the best solution, but it’s a much better idea to try to have good initializations.\nWe pick centroid locations in the following way:\n\nPick a random data point and use those coordinates as the first centroid\nLoop to create remaining centroids\n\nCalculate the distance between existing centroids and data points.\nGet the distance from each data point to it’s closest centroid\nPlace the next centroid at the point with the max distance from previous step\n\n\nThis ensures we get initialization that are nice and far away from each other and spread out amonth the data, minimizing the risk of hitting local optimums.\n\ndef initialize_centroids(data:torch.Tensor,# Data points you want to cluster\n                         k:torch.Tensor # Number of centroids you want to initialize\n                        )->torch.Tensor: # Returns starting centroid coordinates\n    '''Initialize starting points for centroids as far from each other as possible.'''\n    pred_centroids = data[random.sample(range(0,len(data)),1)]\n    for i in range(k-1): \n        _centroid = data[calculate_distances(pred_centroids,data).min(axis=1).values.argmax()]\n        pred_centroids = torch.stack([*pred_centroids,_centroid])\n    return pred_centroids"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#classify-data-points",
    "href": "posts/Clustering/KMeansFromScratch.html#classify-data-points",
    "title": "K-Means From Scratch",
    "section": "3.3 Classify Data Points",
    "text": "3.3 Classify Data Points\nOnce we have centroids (or updated centroids), we need to assign a centroid to each data point. We do this by calculating the distance between each data point and each centroid, and assigning each datapoint to it’s closes centroid.\n\ndef assign_centroids(centroids:torch.Tensor, # Centroid coordinates\n                     data:torch.Tensor # Data points you want to cluster\n                    )->torch.Tensor: # Tensor containing new centroid assignments for each data point\n    '''Based on distances update centroid assignments'''\n    euclid_distances = calculate_distances(centroids,data)\n    assigned_cluster = euclid_distances.squeeze().argmin(axis=1)\n    return assigned_cluster"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#update-centroids",
    "href": "posts/Clustering/KMeansFromScratch.html#update-centroids",
    "title": "K-Means From Scratch",
    "section": "3.4 Update Centroids",
    "text": "3.4 Update Centroids\nTo update the centroid locations, we take the mean of all the data point assigned to that centroid. We make the new centroid that point.\n\ndef update_centroids(centroid_assignments:torch.Tensor, # Centroid coordinates\n                     data:torch.Tensor # Data points you want to cluster\n                    )->torch.Tensor: # Tensor containing updated centroid coodinates\n    '''Update centroid locations'''\n    n_centroids = len(centroid_assignments.unique())\n    pred_centroids = [data[centroid_assignments==i].mean(axis=0) for i in range(n_centroids)]\n    return torch.stack(pred_centroids)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac’s Tech Blog",
    "section": "",
    "text": "Python Programming Tips\n\n\n\n\n\n\n\nPython\n\n\nProgramming\n\n\n\n\nA list of handy tips and tricks when programming in python\n\n\n\n\n\n\nNov 6, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nK-Means From Scratch\n\n\n\n\n\n\n\nClustering\n\n\n\n\nA deep dive on K-Means where smart initialization and the full algorithm is implemented from scratch using pytorch\n\n\n\n\n\n\nNov 5, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction To Statistical Testing\n\n\n\n\n\n\n\nTime Series\n\n\nTesting\n\n\n\n\nAn introduction to Statistical Testing applied to Stock Trading\n\n\n\n\n\n\nSep 30, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nBootstrapping\n\n\n\n\n\n\n\nAPL\n\n\nStatistics\n\n\n\n\nSampling and Bootstrapping Statistics in APL\n\n\n\n\n\n\nSep 20, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nSimple Time Series (Stock Prices)\n\n\n\n\n\n\n\nTime Series\n\n\n\n\nAn introduction to basic time series for stock prediction\n\n\n\n\n\n\nAug 5, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Multiplication\n\n\n\n\n\n\n\nAPL\n\n\nLinear Algebra\n\n\n\n\nMatrix multiplication deep dive in APL\n\n\n\n\n\n\nJul 10, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nNumerical Linear Algebra Part 1\n\n\n\n\n\n\n\nAPL\n\n\nLinear Algebra\n\n\n\n\nNumerical Linear Algebra Part 1 with translations to APL\n\n\n\n\n\n\nJul 10, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nTabular Data Intro\n\n\n\n\n\n\n\nAPL\n\n\nTabular Data\n\n\n\n\nIntroduction to tabular data analysis in APL\n\n\n\n\n\n\nJul 7, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist with a varied background that likes doing varied things. It’s also given me a wide range of skills that prove useful in all sorts of surprising ways.\nMy most important ability is understanding strategic goals and being able to translate that into all the detailed steps needed to get there (and seeing what steps that are not needed). I developed this skill by getting experience in very different roles in lots of different parts of the business.\nMy primary hobby is dance. I used to teach ballroom dance, but now I mostly dance West Coast Swing as a hobby.\n\n\n\nMy Start: assembly line worker ➢ assembly line management ➢ assembly line efficiency optimization\nMoving up: business process engineering ➢ product management\nTouching on Technical: data analyst ➢ dynamics CRM developer\nTrying New Things: accounting ➢ call center ➢ full-time ballroom dance teacher\nFinding my home ❤️: product owner ➢ machine learning researcher ➢ data science"
  }
]