[
  {
    "objectID": "posts/APL/TabularData.html",
    "href": "posts/APL/TabularData.html",
    "title": "Tabular Data Intro",
    "section": "",
    "text": "This article will get us started reading and working with tabular data. The goal is to read in some csv data, do a few basic operations (filtering, aggregation, etc.), and create some basic plots.\n\n]box on -style=max\n\n┌→────────────────┐\n│Was ON -style=max│\n└─────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#load-data-from-csv",
    "href": "posts/APL/TabularData.html#load-data-from-csv",
    "title": "Tabular Data Intro",
    "section": "Load Data from CSV",
    "text": "Load Data from CSV\nAPL has a CSV function to read csv data in that we can use. I am using simple stock ticker data for a moving average.\n\neod ← 'eod-quotemedia.csv'\neod_ar ← ⎕CSV eod '' 4\n⍴ eod_ar ⍝ Check Size\n\n┌→───────┐\n│490738 3│\n└~───────┘\n\n\n\nWe can get the head of the dataframe by selecting the first 5 rows and 3 columns. It’s handy to have a small sized piece of data as you develop so we store in it’s own array also. ↑ Lets us specify the head slice.\n\n⎕←eod_ar_s ← 5 3 ↑ eod_ar\n\n┌→──────────────────────────────────┐\n↓ ┌→───┐       ┌→─────┐ ┌→────────┐ │\n│ │date│       │ticker│ │adj_close│ │\n│ └────┘       └──────┘ └─────────┘ │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-01│ │A│      29.9942     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-02│ │A│      29.6501     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-03│ │A│      29.7052     │\n│ └──────────┘ └─┘                  │\n│ ┌→─────────┐ ┌→┐                  │\n│ │2013-07-05│ │A│      30.4346     │\n│ └──────────┘ └─┘                  │\n└∊──────────────────────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#filter-for-1-ticker",
    "href": "posts/APL/TabularData.html#filter-for-1-ticker",
    "title": "Tabular Data Intro",
    "section": "Filter for 1 ticker",
    "text": "Filter for 1 ticker\nNext, we need to filter for a particular ticker. The first thing we need to know is where the column we want to sort on is located. We can of course see that it’s the second column, but let’s pretend we need to use APL for this to get some practice.\n\n⎕ ← ticker_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'\n\n \n2\n \n\n\n\nLet’s break this down a bit:\n\n1⌷eod_ar Gets the first row - this is our header row\n∊ ⊂'ticker' Determines where ‘ticker’ is and returns a mask ([0 1 0]\n⍸ is applied to get the location of the true values (2)\n,/ Applies a concatenation to flatten from an array to a value\n\nThe next step is getting a truth mask (ie [0 1 1 0 0]) that tells us which rows contain the value to filter for, in this case AAPL\n\ntickers ← eod_ar[;ticker_loc] ∊ ⊂ 'AAPL'\n\nSimilar to our first filtering section, ∊ ⊂ 'AAPL' is checking membership of eod_ar[;ticker_loc] to return the mask.\nWe can see we found 1009 rows out of 490,738 that had this ticker.\n\n⍝ We have a truth array (0/1) of whether row contains 'AAPL' or not\n('size',⍴ tickers) , 'Found',+/tickers \n'min',(⌈/ tickers) , 'max',⌊/tickers\n\n┌→─────────────────────┐\n│size 490738 Found 1009│\n└+─────────────────────┘\n\n\n\n┌→──────────┐\n│min 1 max 0│\n└+──────────┘\n\n\n\n+/ distributes the + sign between each item in the list giving us a sum of the vector. The same approach can be used for ⌊ and ⌈ to get minimum and maximum values (0 and 1). ⍴ gives us our shape.\nNext we need to use this mask to actually filter out data. This is quite easy and we can pass our mask with our full array to do that filtering using ⌿\n\nAAPL ← tickers ⌿ eod_ar\n\nSo when we put that together our full solution is:\n\ncol_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'           ⍝ Column Location\nAAPL ← eod_ar ⌿⍨ eod_ar[;col_loc] ∊ ⊂ 'AAPL'  ⍝ Filter Array\n5 3 ↑ AAPL                                    ⍝ Head of array\n\n┌→────────────────────────────┐\n↓ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-01│ │AAPL│ 53.1092 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-02│ │AAPL│ 54.3122 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-03│ │AAPL│ 54.612  │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-05│ │AAPL│ 54.1734 │\n│ └──────────┘ └────┘         │\n│ ┌→─────────┐ ┌→───┐         │\n│ │2013-07-08│ │AAPL│ 53.8658 │\n│ └──────────┘ └────┘         │\n└∊────────────────────────────┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#calculate-moving-average",
    "href": "posts/APL/TabularData.html#calculate-moving-average",
    "title": "Tabular Data Intro",
    "section": "Calculate Moving Average",
    "text": "Calculate Moving Average\nWe will calculate a 90 period moving average over the stock prices for the AAPL ticker we just filtered for.\n\nws ← 90\n\nLet’s start with calculating a moving sum. Instead of using +/ to sum over the full list, we can use +⌿ to get a moving sum based on the ws we give it.\n\nmovsum ← ws +⌿ AAPL[;3]\n\nWhen we have a moving sum, a moving average is easy. Simply divide the moving sum by the window size to get an average.\n\nmovavg ← movsum ÷ ws\n'movavg length:',⍴ movavg\n'AAPL length:', ⍴ AAPL[;3]\n\n┌→─────────────────┐\n│movavg length: 920│\n└+─────────────────┘\n\n\n\n┌→────────────────┐\n│AAPL length: 1009│\n└+────────────────┘\n\n\n\nOur moving average is shorter than our original data because we are not calculating when we don’t have enough data at the beginning of our time period. Let’s pad the beginning with the raw values.\n\npadded ← ((ws-1) ↑ AAPL[;3]) , movavg\n'padded length:',⍴ padded\n'AAPL length:', ⍴ AAPL[;3]\n\n┌→──────────────────┐\n│padded length: 1009│\n└+──────────────────┘\n\n\n\n┌→────────────────┐\n│AAPL length: 1009│\n└+────────────────┘\n\n\n\nSo the full moving average with padding back to original size looks like this\n\nmovavg ← (ws ↑ AAPL[;3]) , ws ÷⍨ ws +⌿ AAPL[;3]"
  },
  {
    "objectID": "posts/APL/TabularData.html#plot",
    "href": "posts/APL/TabularData.html#plot",
    "title": "Tabular Data Intro",
    "section": "Plot",
    "text": "Plot\nWe can not plot our original data with our moving average. It’s a bit annoying I have to pass in movavg twice to get it to work. It is probably something I am not understanding about how plotting works in APL.\n\n]Plot AAPL[;3] movavg movavg\n\n\nCreated by Causeway SVG engine - SharpPlot v3.71.0\n\nPaint the paper =====\n \n  \n \nBorder =====\nRegion =====\nX-Axis Ticks =====\nX-Axis tickmarks\n \nY-Axis Ticks =====\nY-Axis tickmarks\n \nAxes =====\n \nY-axis labels\n \n  50\n  60\n  70\n  80\n  90\n  100\n  110\n  120\n  130\n  140\n  150\n  160\n \nfor X-axis labels\n \n  0\n  100\n  200\n  300\n  400\n  500\n  600\n  700\n  800\n  900\n  1000\n  1100\n \nHeading, subheading and footnotes =====\nStart of Line Chart ===========\nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nReset to original origin\n\n\n\n\n┌⊖┐\n⌽0│\n└~┘"
  },
  {
    "objectID": "posts/APL/TabularData.html#full-solution",
    "href": "posts/APL/TabularData.html#full-solution",
    "title": "Tabular Data Intro",
    "section": "Full Solution",
    "text": "Full Solution\n\neod_ar ← ⎕CSV 'eod-quotemedia.csv' '' 4\ncol_loc ← ,/ ⍸ 1⌷eod_ar ∊ ⊂'ticker'       \nAAPL ← eod_ar ⌿⍨ eod_ar[;col_loc] ∊ ⊂ 'AAPL'  \nmovavg ← (ws ↑ AAPL[;3]) , ws ÷⍨ (ws←90) +⌿ AAPL[;3]\n]Plot AAPL[;3] movavg movavg\n\n\nCreated by Causeway SVG engine - SharpPlot v3.71.0\n\nPaint the paper =====\n \n  \n \nBorder =====\nRegion =====\nX-Axis Ticks =====\nX-Axis tickmarks\n \nY-Axis Ticks =====\nY-Axis tickmarks\n \nAxes =====\n \nY-axis labels\n \n  50\n  60\n  70\n  80\n  90\n  100\n  110\n  120\n  130\n  140\n  150\n  160\n \nfor X-axis labels\n \n  0\n  100\n  200\n  300\n  400\n  500\n  600\n  700\n  800\n  900\n  1000\n  1100\n \nHeading, subheading and footnotes =====\nStart of Line Chart ===========\nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nPoints follow ...\nLine\n \n  \n \nReset to original origin\n\n\n\n\n┌⊖┐\n⌽0│\n└~┘"
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html",
    "href": "posts/APL/MatrixMultiplication.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "I am working through a couple of linear algebra books/courses as I write this. All content in here will be heavily inspired by those resources:\n\nGilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA.\nApplied Linear Algebra with A. P. L. by Garry Helzer (Author)\n\nI have 2 main goals: + Learn Dyalog APL: APL works very differently than other languages I have done. By learning it I will learn another way of thinking and approaching problems. By having more ways to think and approach problems I become smarter. I want to be smarter. + Improve my mathematical foundation\nTo do this I plan to go through math material and courses in Dyalog APL. In the beginning I will be reviewing basic math while learning APL, but eventually I will get to content where I am both learning APL and math at the same time. This is where I will document what I do.\nWhere to learn APL\nCheck out the fastai apl study group, accompanying videos, and anki decks if you want to learn APL ."
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html#manual-calculation",
    "href": "posts/APL/MatrixMultiplication.html#manual-calculation",
    "title": "Matrix Multiplication",
    "section": "Manual Calculation",
    "text": "Manual Calculation\nWe take the rows of N times the columns of M to do a linear combination to do matrix multiplication.\n\\(1\\begin{bmatrix}2\\\\1\\end{bmatrix} + 2\\begin{bmatrix}5\\\\3\\end{bmatrix}\\)\nWe can do this exactly in APL and see our answer.\n\n⎕ ← (col1 ← M[;1] × N[1;]) + (col2 ← M[;2] × N[2;])\n\n┌→───┐\n│12 7│\n└~───┘"
  },
  {
    "objectID": "posts/APL/MatrixMultiplication.html#apl-calculation",
    "href": "posts/APL/MatrixMultiplication.html#apl-calculation",
    "title": "Matrix Multiplication",
    "section": "APL Calculation",
    "text": "APL Calculation\n\nDot\nin APL we would normally not write is all out but would write it using the dot (.) function. Let’s figure out what that is and how it works.\nThe . applies the operators the surround it (⍺⍺ and ⍵⍵) in a specific way and order called an inner product.\nIn our matrix multiplication problem it looks like this. \\(\\begin{bmatrix}(1⍵⍵2)⍺⍺(2⍵⍵5)\\\\(1⍵⍵1)⍺⍺(2⍵⍵3)\\end{bmatrix}\\)\nLet’s walk through this in our matrix multiplication example above one operator at a time to understand it a bit better\n\n\nApply the ⍵⍵ argument\nI like to visualize the first step like this:\n\\(\\begin{bmatrix}⍵⍵&&1\\\\&⍵⍵&2\\\\2&5&\\end{bmatrix}\\)\n\\(\\begin{bmatrix}⍵⍵&&1\\\\&⍵⍵&2\\\\1&3&\\end{bmatrix}\\)\nWe first apply whatever the ⍵⍵ parameter is and combine elements. Just as we did above manually we need to do multiplication here so we know the ⍵⍵ parameter must be ×. These calculation are:\nMatrix 1:\n\n1 × 2 = 2\n2 × 5 = 10\n\nMatrix 2:\n\n1 × 1 = 1\n2 × 3 = 6\n\nSo far we have ⍺⍺.×. We can show the result of our calculations above in matrices.\n\\(\\begin{bmatrix}2&⍺⍺\\\\&10\\end{bmatrix}\\)\n\\(\\begin{bmatrix}1&⍺⍺\\\\&6\\end{bmatrix}\\)\n\n\nApply the ⍺⍺ argument\nThe next thing the . operator does is combine all the numbers in each of step 1 resulting matrices using ⍺⍺. To get the linear combination we did above we need to add the numbers in each matrix, so the ⍺⍺ operator must be +.\nIf we do that addition:\n\nMatrix 1: 2 + 10 = 12\nMatrix 2: 1 + 6 = 7\n\nLeaving us with our answer of \\(\\begin{bmatrix}12\\\\7\\end{bmatrix}\\)\nSo to do matrix multiplication we simply need to use:\n\nM +.× N\n\n┌→────┐\n↓17 24│\n│10 14│\n└~────┘\n\n\n\n\n\nDot is flexible\nThis was just 1 example of using the . operator. We used + as ⍺⍺ and × as ⍵⍵ to fit what we needed for this problem.\nNow that we understand that, we can flip our operators and look at ×.+ instead of +.×. We can also do any number of other operators to do lots of different matrix operations. Take a look at the examples below and try calculating them by hand to see what you get!\n\n⍝ using addition.multiplication (normal matrix multiplication) \nM+.×N \n\n┌→─┐\n↓12│\n│ 7│\n└~─┘\n\n\n\n\n⍝ using multiplication.addition\nM×.+N \n\n┌→─┐\n↓21│\n│10│\n└~─┘\n\n\n\n\n⍝ using max.min\nM⌈.⌊N\n\n┌→┐\n↓2│\n│2│\n└~┘\n\n\n\n\n⍝ using addition.subtraction\nM-.+N\n\n┌→─┐\n↓¯4│\n│¯3│\n└~─┘\n\n\n\n\n⍝ using exponent.division\nM*.÷N\n\n┌→──────────┐\n↓5.656854249│\n│1          │\n└~──────────┘\n\n\n\n\n⍝ using factorial.natural_log\nM!.⍟N\n\n┌→───────────┐\n↓1           │\n│0.6309297536│\n└~───────────┘"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html",
    "title": "Numerical Linear Algebra Part 1",
    "section": "",
    "text": "This notebook is a copy of a notebook from the fast.ai fast.ai Numerical linear algebra course. I have modified it to use APL as a learning experience for myself.\nCheck out the original notebook here\nI do not have any business affiliation with fast.ai, and this notebook is not an official fast.ai notebook.\nYou can read an overview of this Numerical Linear Algebra course in this blog post. The course was originally taught in the University of San Francisco MS in Analytics graduate program. Course lecture videos are available on YouTube (note that the notebook numbers and video numbers do not line up, since some notebooks took longer than 1 video to cover).\nYou can ask questions about the course on our fast.ai forums."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#why-study-numerical-linear-algebra",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#why-study-numerical-linear-algebra",
    "title": "Numerical Linear Algebra Part 1",
    "section": "Why study Numerical Linear Algebra?",
    "text": "Why study Numerical Linear Algebra?\nKey Question of this course: How can we do matrix computations with acceptable speed and acceptable accuracy?\nA list of the Top 10 Algorithms of science and engineering during the 20th century includes: the matrix decompositions approach to linear algebra. It also includes the QR algorithm, which we’ll cover, and Krylov iterative methods which we’ll see an example of. (See here for another take)\n (source: Top 10 Algorithms)\nThere are 4 things to keep in mind when choosing or designing an algorithm for matrix computations: - Memory Use - Speed - Accuracy - Scalability/Parallelization\nOften there will be trade-offs between these categories.\n\nMotivation\nMatrices are everywhere– anything that can be put in an Excel spreadsheet is a matrix, and language and pictures can be represented as matrices as well. Knowing what options there are for matrix algorithms, and how to navigate compromises, can make enormous differences to your solutions. For instance, an approximate matrix computation can often be thousands of times faster than an exact one.\nIt’s not just about knowing the contents of existing libraries, but knowing how they work too. That’s because often you can make variations to an algorithm that aren’t supported by your library, giving you the performance or accuracy that you need. In addition, this field is moving very quickly at the moment, particularly in areas related to deep learning, recommendation systems, approximate algorithms, and graph analytics, so you’ll often find there’s recent results that could make big differences in your project, but aren’t in your library.\nKnowing how the algorithms really work helps to both debug and accelerate your solution."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#matrix-computations",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#matrix-computations",
    "title": "Numerical Linear Algebra Part 1",
    "section": "Matrix Computations",
    "text": "Matrix Computations\nThere are two key types of matrix computation, which get combined in many different ways. These are: - Matrix and tensor products - Matrix decompositions\nSo basically we’re going to be combining matrices, and pulling them apart again!\n\nMatrix and Tensor Products\n\nMatrix-Vector Products:\nThe matrix below gives the probabilities of moving from 1 health state to another in 1 year. If the current health states for a group are: - 85% asymptomatic - 10% symptomatic - 5% AIDS - 0% death\nwhat will be the % in each health state in 1 year?\n(Source: Concepts of Markov Chains)\n\n\nAnswer\n\nSet up Data\nCreate category names vector for display purposes\n\n]box on -style=max\n\n┌→────────────────┐\n│Was ON -style=max│\n└─────────────────┘\n\n\n\n\nNames←'Asymptomatic' 'Symptomatic' 'Aids'  'Death'\n\nCreate current health states array\n\nNames⍪Start←1 4⍴.85 .1 .05 0\n\n┌→────────────────────────────────────────────┐\n↓ ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └────────────┘ └───────────┘ └────┘ └─────┘ │\n│                                             │\n│ 0.85           0.1           0.05   0       │\n│                                             │\n└∊────────────────────────────────────────────┘\n\n\n\nCreate transition table/stochastic matrix\n\nTransitions←4 4⍴.9 .07 .02 .01 0 .93 .05 .02 0 0 .85 .15 0 0 0 1\n((⊂'States')⍪Names)⍪(Names,Transitions)\n\n┌→───────────────────────────────────────────────────────────┐\n↓ ┌→─────┐       ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │States│       │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └──────┘       └────────────┘ └───────────┘ └────┘ └─────┘ │\n│ ┌→───────────┐                                             │\n│ │Asymptomatic│ 0.9            0.07          0.02   0.01    │\n│ └────────────┘                                             │\n│ ┌→──────────┐                                              │\n│ │Symptomatic│  0              0.93          0.05   0.02    │\n│ └───────────┘                                              │\n│ ┌→───┐                                                     │\n│ │Aids│         0              0             0.85   0.15    │\n│ └────┘                                                     │\n│ ┌→────┐                                                    │\n│ │Death│        0              0             0      1       │\n│ └─────┘                                                    │\n└∊───────────────────────────────────────────────────────────┘\n\n\n\n\n\nAnswer Calculation\nMultiply together to get ending health states\n\nNames⍪End←Start+.×Transitions\n\n┌→────────────────────────────────────────────┐\n↓ ┌→───────────┐ ┌→──────────┐ ┌→───┐ ┌→────┐ │\n│ │Asymptomatic│ │Symptomatic│ │Aids│ │Death│ │\n│ └────────────┘ └───────────┘ └────┘ └─────┘ │\n│                                             │\n│ 0.765          0.1525        0.0645 0.018   │\n│                                             │\n└∊────────────────────────────────────────────┘\n\n\n\n\n\n\nMatrix-Matrix Products\n(Source: Several Simple Real-world Applications of Linear Algebra Tools)\n\n\nAnswer\n\nSet up Data\nLets define the names of all the stuff we are working with for display purposes\n\nitems←'roll' 'bun' 'cake' 'bread'\nshops←'S1' 'S2'\npeople←'P1' 'P2' 'P3'\n\nDemanded quantity of foodstuff\n\nDemanded←3 4⍴6 5 3 1 3 6 2 2 3 4 3 1\n((⊂'')⍪items)⍪(people,Demanded)\n\n┌→─────────────────────────────────┐\n↓ ┌⊖┐  ┌→───┐ ┌→──┐ ┌→───┐ ┌→────┐ │\n│ │ │  │roll│ │bun│ │cake│ │bread│ │\n│ └─┘  └────┘ └───┘ └────┘ └─────┘ │\n│ ┌→─┐                             │\n│ │P1│ 6      5     3      1       │\n│ └──┘                             │\n│ ┌→─┐                             │\n│ │P2│ 3      6     2      2       │\n│ └──┘                             │\n│ ┌→─┐                             │\n│ │P3│ 3      4     3      1       │\n│ └──┘                             │\n└∊─────────────────────────────────┘\n\n\n\n\nPrices←4 2⍴1.5 1 2 2.5 5 4.5 16 17\n((⊂'')⍪shops)⍪(items,Prices)\n\n┌→──────────────────┐\n↓ ┌⊖┐     ┌→─┐ ┌→─┐ │\n│ │ │     │S1│ │S2│ │\n│ └─┘     └──┘ └──┘ │\n│ ┌→───┐            │\n│ │roll│  1.5  1    │\n│ └────┘            │\n│ ┌→──┐             │\n│ │bun│   2    2.5  │\n│ └───┘             │\n│ ┌→───┐            │\n│ │cake│  5    4.5  │\n│ └────┘            │\n│ ┌→────┐           │\n│ │bread│ 16   17   │\n│ └─────┘           │\n└∊──────────────────┘\n\n\n\n\n\nAnswer Calculation\nWe can calculate the price for each shop for each person with matrix multiplication\n\nAns←Demanded+.×Prices\n((⊂'')⍪shops)⍪(people,Ans) ⍝ display\n\n┌→───────────────┐\n↓ ┌⊖┐  ┌→─┐ ┌→─┐ │\n│ │ │  │S1│ │S2│ │\n│ └─┘  └──┘ └──┘ │\n│ ┌→─┐           │\n│ │P1│ 50   49   │\n│ └──┘           │\n│ ┌→─┐           │\n│ │P2│ 58.5 61   │\n│ └──┘           │\n│ ┌→─┐           │\n│ │P3│ 43.5 43.5 │\n│ └──┘           │\n└∊───────────────┘\n\n\n\nThen find the lowest price each person can pay so these folks can budget!\n\nlowest_per_person ← ⌊/Ans\npeople,3 1⍴lowest_per_person ⍝ display\n\n┌→──────────┐\n↓ ┌→─┐      │\n│ │P1│ 49   │\n│ └──┘      │\n│ ┌→─┐      │\n│ │P2│ 58.5 │\n│ └──┘      │\n│ ┌→─┐      │\n│ │P3│ 43.5 │\n│ └──┘      │\n└∊──────────┘\n\n\n\nWe can also identify which store each person should go to based on the min price.\n\n3 2 ⍴ people ⍪ shops ⌷⍨ ⊂ {⍵⍳⌊/⍵}¨↓Ans\n\n┌→──────────┐\n↓ ┌→─┐ ┌→─┐ │\n│ │P1│ │P2│ │\n│ └──┘ └──┘ │\n│ ┌→─┐ ┌→─┐ │\n│ │P3│ │S2│ │\n│ └──┘ └──┘ │\n│ ┌→─┐ ┌→─┐ │\n│ │S1│ │S1│ │\n│ └──┘ └──┘ │\n└∊──────────┘\n\n\n\n\n\n\nImage Data\nImages can be represented by matrices.\n (Source: Adam Geitgey)\n\n\nConvolution\nConvolutions are the heart of convolutional neural networks (CNNs), a type of deep learning, responsible for the huge advances in image recognitionin the last few years. They are now increasingly being used for speech as well, such as Facebook AI’s results for speech translation which are 9x faster than RNNs (the current most popular approach for speech translation).\nComputers are now more accurate than people at classifying images.\n (Source: Andrej Karpathy)\n (Source: Nvidia)\nYou can think of a convolution as a special kind of matrix product\nThe 3 images below are all from an excellent blog post written by a fast.ai student on CNNs from Different Viewpoints:\nA convolution applies a filter to each section of an image: \nNeural Network Viewpoint: \nMatrix Multiplication Viewpoint: \nLet’s see how convolutions can be used for edge detection in this notebook(originally from the fast.ai Deep Learning Course)\n\n\n\nMatrix Decompositions\nWe will be talking about Matrix Decompositions every day of this course, and will cover the below examples in future lessons:\n\nTopic Modeling (NMF and SVD. SVD uses QR) A group of documents can be represented by a term-document matrix  (source: Introduction to Information Retrieval)  (source: NMF Tutorial)\nBackground removal (robust PCA, which uses truncated SVD) \nGoogle’s PageRank Algorithm (eigen decomposition)\n\n (source: What is in PageRank?)\n\nList of other decompositions and some applications matrix factorization jungle"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#accuracy",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#accuracy",
    "title": "Numerical Linear Algebra Part 1",
    "section": "Accuracy",
    "text": "Accuracy\n\nFloating Point Arithmetic\nTo understand accuracy, we first need to look at how computers (which are finite and discrete) store numbers (which are infinite and continuous)\n\nExercise\nTake a moment to look at the function \\(f\\) below. Before you try running it, write on paper what the output would be of \\(x_1 = f(\\frac{1}{10})\\). Now, (still on paper) plug that back into \\(f\\) and calculate \\(x_2 = f(x_1)\\). Keep going for 10 iterations.\nThis example is taken from page 107 of Numerical Methods, by Greenbaum and Chartier.\ndef f(x):\n    if x <= 1/2:\n        return 2 * x\n    if x > 1/2:\n        return 2*x - 1\n\n⍝ Translate function to APL\nf←{(2×⍵)-⍵>.5}\n\n\n⍝ Create generator\ngen←{(f⍣⍵)⍺}\n\n\n⍝ First 10 generations\n.1 gen¨ ⍳10\n\n┌→──────────────────────────────────────┐\n│0.2 0.4 0.8 0.6 0.2 0.4 0.8 0.6 0.2 0.4│\n└~──────────────────────────────────────┘\n\n\n\n\n⍝ First 80 generations (left to right top to bottom)\n16 5⍴.1 gen¨ ⍳80\n\n┌→───────────────────────────────────────────────────────────────┐\n↓0.2          0.4          0.8          0.6          0.2         │\n│0.4          0.8          0.6          0.2          0.4         │\n│0.8          0.6          0.2          0.4          0.8         │\n│0.6          0.2          0.4          0.8          0.6         │\n│0.2          0.4          0.8          0.6000000001 0.2000000002│\n│0.4000000004 0.8000000007 0.6000000015 0.200000003  0.400000006 │\n│0.8000000119 0.6000000238 0.2000000477 0.4000000954 0.8000001907│\n│0.6000003815 0.2000007629 0.4000015259 0.8000030518 0.6000061035│\n│0.200012207  0.4000244141 0.8000488281 0.6000976563 0.2001953125│\n│0.400390625  0.80078125   0.6015625    0.203125     0.40625     │\n│0.8125       0.625        0.25         0.5          1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n│1            1            1            1            1           │\n└~───────────────────────────────────────────────────────────────┘\n\n\n\n\n⍝ Answer after 80 generations\n(f⍣80).1\n\n \n1\n \n\n\n\nWhat went wrong?\n\n\nProblem: math is continuous & infinite, but computers are discrete & finite\nTwo Limitations of computer representations of numbers: 1. they can’t be arbitrarily large or small 2. there must be gaps between them\nThe reason we need to care about accuracy, is because computers can’t store infinitely accurate numbers. It’s possible to create calculations that give very wrong answers (particularly when repeating an operation many times, since each operation could multiply the error).\nHow computers store numbers:\n\nThe mantissa can also be referred to as the significand.\nIEEE Double precision arithmetic: - Numbers can be as large as \\(1.79 \\times 10^{308}\\) and as small as \\(2.23 \\times 10^{-308}\\). - The interval \\([1,2]\\) is represented by discrete subset: \\[1, \\: 1+2^{-52}, \\: 1+2 \\times 2^{-52},\\: 1+3 \\times 2^{-52},\\: \\ldots, 2\\]\n\nThe interval \\([2,4]\\) is represented: \\[2, \\: 2+2^{-51}, \\: 2+2 \\times 2^{-51},\\: 2+3 \\times 2^{-51},\\: \\ldots, 4\\]\n\nFloats and doubles are not equidistant:\n Source: What you never wanted to know about floating point but will be forced to find out\nMachine Epsilon\nHalf the distance between 1 and the next larger number. This can vary by computer. IEEE standards for double precision specify \\[ \\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}\\]\nTwo important properties of Floating Point Arithmetic:\n\nThe difference between a real number \\(x\\) and its closest floating point approximation \\(fl(x)\\) is always smaller than \\(\\varepsilon_{machine}\\) in relative terms. For some \\(\\varepsilon\\), where \\(\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}\\), \\[fl(x)=x \\cdot (1 + \\varepsilon)\\]\nWhere * is any operation (\\(+, -, \\times, \\div\\)), and \\(\\circledast\\) is its floating point analogue, \\[ x \\circledast y = (x * y)(1 + \\varepsilon)\\] for some \\(\\varepsilon\\), where \\(\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}\\) That is, every operation of floating point arithmetic is exact up to a relative error of size at most \\(\\varepsilon_{machine}\\)\n\n\n\nHistory\nFloating point arithmetic may seem like a clear choice in hindsight, but there have been many, many ways of storing numbers: - fixed-point arithmetic - logarithmic and semilogarithmic number systems - continued-fractions - rational numbers - possibly infinite strings of rational numbers - level-index number systems - fixed-slash and floating-slash number systems - 2-adic numbers\nFor references, see Chapter 1 (which is free) of the Handbook of Floating-Point Arithmetic. Yes, there is an entire 16 chapter book on floating point!\nTimeline History of Floating Point Arithmetic: - ~1600 BC: Babylonian radix-60 system was earliest floating-point system (Donald Knuth). Represented the significand of a radix-60 floating-point representation (if ratio of two numbers is a power of 60, represented the same) - 1630 Slide rule. Manipulate only significands (radix-10) - 1914 Leonardo Torres y Quevedo described an electromechanical implementation of Babbage’s Analytical Engine with Floating Point Arithmetic. - 1941 First real, modern implementation. Konrad Zuse’s Z3 computer. Used radix-2, with 14 bit significand, 7 bit exponents, and 1 sign bit. - 1985 IEEE 754-1985 Standard for Binary Floating-Point Arithmetic released. Has increased accuracy, reliability, and portability. William Kahan played leading role.\n“Many different ways of approximating real numbers on computers have been introduced.. And yet, floating-point arithmetic is by far the most widely used way of representing real numbers in modern computers. Simulating an infinite, continuous set (the real numbers) with a finite set (the “machine numbers”) is not a straightforward task: clever compromises must be found between, speed, accuracy, dynamic range, ease of use and implementation, and memory. It appears that floating-point arithmetic, with adequately chosen parameters (radix, precision, extremal exponents, etc.), is a very good compromise for most numerical applications.”\nAlthough a radix value of 2 (binary) seems like the pretty clear winner now for computers, a variety of other radix values have been used at various point:\n\nradix-8 used by early machines PDP-10, Burroughs 570 and 6700\nradix-16 IBM 360\nradix-10 financial calculations, pocket calculators, Maple\nradix-3 Russian SETUN computer (1958). Benefits: minimizes beta x p (symbols x digits), for a fixed largest representable number beta^p - 1. Rounding = truncation\nradix-2 most common. Reasons: easy to implement. Studies have shown (with implicit leading bit) this gives better worst-case or average accuracy than all other radices.\n\n\n\n\nConditioning and Stability\nSince we can not represent numbers exactly on a computer (due to the finiteness of our storage, and the gaps between numbers in floating point architecture), it becomes important to know how small perturbations in the input to a problem impact the output.\n“A stable algorithm gives nearly the right answer to nearly the right question.” –Trefethen\nConditioning: perturbation behavior of a mathematical problem (e.g. least squares)\nStability: perturbation behavior of an algorithm used to solve that problem on a computer (e.g. least squares algorithms, householder, back substitution, gaussian elimination)\nExample: Eigenvalues of a Matrix\n\nCreate matrices\n\n⎕←A←2 2⍴1 1000 0 1\n⎕←B←2 2⍴1 1000 .001 1\n\n┌→─────┐\n↓1 1000│\n│0    1│\n└~─────┘\n\n\n\n┌→─────────┐\n↓1     1000│\n│0.001    1│\n└~─────────┘\n\n\n\n\n\nCalculate Eigenvalues (quadratic)\n\nI ← {⍵ ⍵ ⍴ 1, ⍵⍴0} ⍝ calculate identity\ntf← {+/+/(⍵×I 2)} ⍝ Calculate Trace\ndf ← {(×/×/(⍵×I 2)+0=I 2)-×/×/(I 2)+⍵×0=I 2} ⍝ Calculate Determinant\n\n⍝ Eigenvalue calculation\ne1←{2÷⍨⍵-.5*⍨(⍵*2)-4×⍺} ⍝ Eigenvalue 1\ne2←{2÷⍨⍵+.5*⍨(⍵*2)-4×⍺} ⍝ Eigenvalue 2\n\n⍝ Get both eigenvalues\neigenvalues←{(d e2 t),(d←df ⍵)e1(t←tf ⍵)} ⍝ Get both eigenvalues\n\n\neigenvalues A\neigenvalues B\n\n┌→──┐\n│1 1│\n└~──┘\n\n\n\n┌→──┐\n│2 0│\n└~──┘\n\n\n\nReminder: Two properties of Floating Point Arithmetic\n\nThe difference between a real number \\(x\\) and its closest floating point approximation \\(fl(x)\\) is always smaller than \\(\\varepsilon_{machine}\\) in relative terms.\nEvery operation \\(+, -, \\times, \\div\\) of floating point arithmetic is exact up to a relative error of size at most \\(\\varepsilon_{machine}\\)\n\nExamples we’ll see: - Classical vs Modified Gram-Schmidt accuracy - Gram-Schmidt vs. Householder (2 different ways of computing QR factorization), how orthogonal the answer is - Condition of a system of equations\n\n\n\nApproximation accuracy\nIt’s rare that we need to do highly accurate matrix computations at scale. In fact, often we’re doing some kind of machine learning, and less accurate approaches can prevent overfitting.\nIf we accept some decrease in accuracy, then we can often increase speed by orders of magnitude (and/or decrease memory use) by using approximate algorithms. These algorithms typically give a correct answer with some probability. By rerunning the algorithm multiple times you can generally increase that probability multiplicatively!\nExample: A bloom filter allows searching for set membership with 1% false positives, using <10 bits per element. This often represents reductions in memory use of thousands of times.\n\nThe false positives can be easily handled by having a second (exact) stage check all returned items - for rare items this can be very effective. For instance, many web browsers use a bloom filter to create a set of blocked pages (e.g. pages with viruses), since blocked web pages are only a small fraction of the whole web. A false positive can be handled here by taking anything returned by the bloom filter and checking against a web service with the full exact list. (See this bloom filter tutorial for more details).\n\n\nExpensive Errors\nThe below examples are from Greenbaum & Chartier.\nEuropean Space Agency spent 10 years and $7 billion on the Ariane 5 Rocket.\nWhat can happen when you try to fit a 64 bit number into a 16 bit space (integer overflow):\n\n⍝from IPython.display import YouTubeVideo\n⍝YouTubeVideo(\"PK_yguLapgA\")\n\nHere is a floating point error that cost Intel $475 million:\n1994 NYTimes article about Intel Pentium Error \nResources: See Lecture 13 of Trefethen & Bau and Chapter 5 of Greenbaum & Chartier for more on Floating Point Arithmetic"
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#memory-use",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#memory-use",
    "title": "Numerical Linear Algebra Part 1",
    "section": "Memory Use",
    "text": "Memory Use\n\nSparse vs Dense\nAbove we covered how numbers are stored, now let’s talk about how matrices are stored. A key way to save memory (and computation) is not to store all of your matrix. Instead, just store the non-zero elements. This is called sparse storage, and it is well suited to sparse matrices, that is, matrices where most elements are zero.\n\nHere is an example of the matrix from a finite element problem, which shows up in engineering (for instance, when modeling the air-flow around a plane). In this example, the non-zero elements are black and the zero elements are white:  Source\nThere are also special types of structured matrix, such as diagonal, tri-diagonal, hessenberg, and triangular, which each display particular patterns of sparsity, which can be leveraged to reduce memory and computation.\nThe opposite of a sparse matrix is a dense matrix, along with dense storage, which simply refers to a matrix containing mostly non-zeros, in which every element is stored explicitly. Since sparse matrices are helpful and common, numerical linear algebra focuses on maintaining sparsity through as many operations in a computation as possible."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#speed",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#speed",
    "title": "Numerical Linear Algebra Part 1",
    "section": "Speed",
    "text": "Speed\nSpeed differences come from a number of areas, particularly: - Computational complexity - Vectorization - Scaling to multiple cores and nodes - Locality\n\nComputational complexity\nIf you are unfamiliar with computational complexity and \\(\\mathcal{O}\\) notation, you can read about it on Interview Cake and practice on Codecademy. Algorithms are generally expressed in terms of computation complexity with respect to the number of rows and number of columns in the matrix. E.g. you may find an algorithm described as \\(\\mathcal{O(n^2m)}\\).\n\n\nVectorization\nModern CPUs and GPUs can apply an operation to multiple elements at once on a single core. For instance, take the exponent of 4 floats in a vector in a single step. This is called SIMD. You will not be explicitly writing SIMD code (which tends to require assembly language or special C “intrinsics”), but instead will use vectorized operations in libraries like numpy, which in turn rely on specially tuned vectorized low level linear algebra APIs (in particular, BLAS, and LAPACK).\n\nMatrix Computation Packages: BLAS and LAPACK\nBLAS (Basic Linear Algebra Subprograms): specification for low-level matrix and vector arithmetic operations. These are the standard building blocks for performing basic vector and matrix operations. BLAS originated as a Fortran library in 1979. Examples of BLAS libraries include: AMD Core Math Library (ACML), ATLAS, Intel Math Kernel Library (MKL), and OpenBLAS.\nLAPACK is written in Fortran, provides routines for solving systems of linear equations, eigenvalue problems, and singular value problems. Matrix factorizations (LU, Cholesky, QR, SVD, Schur). Dense and banded matrices are handled, but not general sparse matrices. Real and complex, single and double precision.\n1970s and 1980s: EISPACK (eigenvalue routines) and LINPACK (linear equations and linear least-squares routines) libraries\nLAPACK original goal: make LINAPCK and EISPACK run efficiently on shared-memory vector and parallel processors and exploit cache on modern cache-based architectures (initially released in 1992). EISPACK and LINPACK ignore multi-layered memory hierarchies and spend too much time moving data around.\nLAPACK uses highly optimized block operations implementations (which much be implemented on each machine) LAPACK written so as much of the computation as possible is performed by BLAS.\n\n\n\nLocality\nUsing slower ways to access data (e.g. over the internet) can be up to a billion times slower than faster ways (e.g. from a register). But there’s much less fast storage than slow storage. So once we have data in fast storage, we want to do any computation required at that time, rather than having to load it multiple times each time we need it. In addition, for most types of storage its much faster to access data items that are stored next to each other, so we should try to always use any data stored nearby that we know we’ll need soon. These two issues are known as locality.\n\nSpeed of different types of memory\nHere are some numbers everyone should know (from the legendary Jeff Dean): - L1 cache reference 0.5 ns - L2 cache reference 7 ns - Main memory reference/RAM 100 ns - Send 2K bytes over 1 Gbps network 20,000 ns - Read 1 MB sequentially from memory 250,000 ns - Round trip within same datacenter 500,000 ns - Disk seek 10,000,000 ns - Read 1 MB sequentially from network 10,000,000 ns - Read 1 MB sequentially from disk 30,000,000 ns - Send packet CA->Netherlands->CA 150,000,000 ns\nAnd here is an updated, interactive version, which includes a timeline of how these numbers have changed.\nKey take-away: Each successive memory type is (at least) an order of magnitude worse than the one before it. Disk seeks are very slow.\nThis video has a great example of showing several ways you could compute the blur of a photo, with various trade-offs. Don’t worry about the C code that appears, just focus on the red and green moving pictures of matrix computation.\nAlthough the video is about a new language called Halide, it is a good illustration the issues it raises are universal. Watch minutes 1-13:\n\n⍝ from IPython.display import YouTubeVideo\n⍝ YouTubeVideo(\"3uiEyEKji0M\")\n\nLocality is hard. Potential trade-offs: - redundant computation to save memory bandwidth - sacrificing parallelism to get better reuse\n\n\nTemporaries\nThe issue of “temporaries” occurs when the result of a calculation is stored in a temporary variable in RAM, and then that variable is loaded to do another calculation on it. This is many orders of magnitude slower than simply keeping the data in cache or registers and doing all necessary computations before storing the final result in RAM. This is particularly an issue for us since numpy generally creates temporaries for every single operation or function it does. E.g. \\(a=b\\cdot c^2+ln(d)\\) will create four temporaries (since there are four operations and functions).\n\n\n\nScaling to multiple cores and nodes\nWe have a separate section for scalability, but it’s worth noting that this is also important for speed - if we can’t scale across all the computing resources we have, we’ll be stuck with slower computation."
  },
  {
    "objectID": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#scalability-parallelization",
    "href": "posts/APL/Fastai_Numerical_Linear_Algebra_1.html#scalability-parallelization",
    "title": "Numerical Linear Algebra Part 1",
    "section": "Scalability / parallelization",
    "text": "Scalability / parallelization\nOften we’ll find that we have more data than we have memory to handle, or time to compute. In such a case we would like to be able to scale our algorithm across multiple cores (within one computer) or nodes (i.e. multiple computers on a network). We will not be tackling multi-node scaling in this course, although we will look at scaling across multiple cores (called parallelization). In general, scalable algorithms are those where the input can be broken up into smaller pieces, each of which are handled by a different core/computer, and then are put back together at the end."
  },
  {
    "objectID": "posts/APL/Basic Stats.html",
    "href": "posts/APL/Basic Stats.html",
    "title": "Bootstrapping",
    "section": "",
    "text": "This post shows how to calculate statistics in the way I believe should be the default for data scientists, bootstrapping. If you are not familiar with this approach and think it sounds intriguing, check out this page to find a great book to get a fantastic start on bootstrapping and practical statistics.\n\n\n\n\n\n\nNote\n\n\n\nThe quality and simplicity of the APL code in this post was improved thanks to the kind feedback provided by rak1507. It’s awesome to have experienced community members like rak1507 that are willing to read through material written by people newer to array programming and offer feedback in a supportive way.\n\n\n\n\nBecause this opinions seems to put me in the minority of data scientists I am writing a short piece on why bootstrapping here.\nIn classical statistics, very clever algebraic formulas are used to approximate a sampling distribution, and that approximation can be used to calculate a p-value or a confidence interval or other statistics. These formulas rely on assumptions about the data and do not work if those baked in assumptions are not true. In other words they are really shortcuts to calculating an answer that work in specific situations.\nIn modern days, we do not need to approximate a sampling distribution using algebra. We can do something much more elementary, more powerful, and more flexible. Thanks to modern computers, we can just sample our data repeatedly to create an actual sampling distribution and calculate based off of that. You get the same answer. So why do I advocate for a bootstrapping first approach?\n\nIt is simpler and more intuitive. This means it is far easier to craft custom tests and statistics based on whatever you want and reason about what things are.\nBootstrapping assumes nothing other than you have data. Classical statistical formulas are shortcuts that are enabled with baked in assumptions about the data. This means the same boostrapping approach works in basically all situations, where classical statistical formulas only apply in the particular situations they were designed for.\n\nFor this reason I believe it should be the default and you can change to computational shortcuts in the situations where it makes sense (ie you are very confident you understand assumptions, confident they are true in your problem, and the amount of data makes it non-trivial to bootstrap).\n\n\n\n\n\n\nNote\n\n\n\nMuch of this next bit is heavily inspired by Overview of Statistics: UnLocking the Power of Data By Lock, Lock, Lock, Lock, and Lock Published by Wiley (2012). I have summarized key points that I think are relevant to what I want to communicate. For example, the quotes I am using are quotes I originally saw in their article.\n\n\nMany of the top statisticians have known bootstrapping is a more elementary but more flexible approach for longer than the approach was computationally feasible. For example, in 1936 Sir R.A. Fisher (who created the foundations of statistical inference) spoke about using this bootstrapping approach:\n\nActually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.\n\nWhile these methods were tedious in 1936, they are trivial thanks to modern computers. We no longer have to do clever algebraic tricks to approximate a sampling distribution - we can just create a sampling distribution, as George Cobb pointed out in the journal Technology Innovations in Statistical Education.\n\nWhat we teach is largely the technical machinery of numerical approximations based on the normal distribution and its many subsidiary cogs. This machinery was once necessary, because the conceptually simpler alternative based on permutations was computationally beyond our reach. Before computers statisticians had no choice. These days we have no excuse.\n\n\n\n\n\nBecause it’s cool\nBecause most modeling now-a-days is done via array programming and learning, and APL is a fantastic way to get better at that\nBecause it’s a more consistent math noting"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#create-1-sample",
    "href": "posts/APL/Basic Stats.html#create-1-sample",
    "title": "Bootstrapping",
    "section": "Create 1 Sample",
    "text": "Create 1 Sample\nLet’s start with creating a sample\n\nCreate some data for us to sample\nGet 5 random numbers between 1 and 10 (no duplicate numbers)\n\n\n⎕←V ← 5?10\n\n┌→─────────┐\n│4 3 10 1 5│\n└~─────────┘\n\n\n\nNext we need to get a random sample of indexes from our data V. We can do that in 3 steps: 1. Get the total number of elements in our data array with ≢V (tally the Vector) 1. Create an array of the size of the sample we want and fill it with ≢V using 10⍴≢V. Create an array of dimension 10 with containing the tally of the vector. APL will broadcase to make all elements equal to ≢V automatically. 1. ? will roll a die for each element between 1 and the value of the element. This gives us random index locations for each sample we want.\nPut that all together and we have code that:\n\nGet random sample of indexes\nGet 10 random numbers between 1 and ≢V (duplicates allowed)\n\n\n⎕←S←?10⍴≢V\n\n┌→──────────────────┐\n│3 2 2 2 2 5 1 2 4 2│\n└~──────────────────┘\n\n\n\nSince that created random index locations, we can look those indexes up in our original vector V to get our random sample.\n\nV[S]\n\n┌→───────────────────┐\n│10 3 3 3 3 5 4 3 1 3│\n└~───────────────────┘\n\n\n\nIf we put that together we get a nice compact way of drawing a sample.\n\nV[?10 ⍴ ≢V]\n\n┌→───────────────────┐\n│3 10 3 3 1 4 3 4 3 3│\n└~───────────────────┘"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#create-sampling-distribution",
    "href": "posts/APL/Basic Stats.html#create-sampling-distribution",
    "title": "Bootstrapping",
    "section": "Create sampling distribution",
    "text": "Create sampling distribution\nWe drew a sample, but really what we want to do is draw a whole bunch of samples. All we have to do is create a matrix of indices instead of a vector and the exact same approach works.\nThis is the same as above, except instead of 10 ⍴ to create an array of shape 10, we use 5 10 ⍴ to create an array of shape 5 by 10.\nFor convenience I store the shapes in a variable for later use.\n\nV[?(n←5) (ss←10) ⍴ ≢V]\n\n┌→──────────────────────────┐\n↓ 4 4  3  4  1  5  4 3  3 10│\n│ 1 3 10  5  3  4 10 1  4 10│\n│ 5 1 10 10  5  3  3 4  1  4│\n│10 5 10  1 10  3  1 4 10  1│\n│10 5  5  5  1 10  3 3  1 10│\n└~──────────────────────────┘"
  },
  {
    "objectID": "posts/APL/Basic Stats.html#confidence-intervals",
    "href": "posts/APL/Basic Stats.html#confidence-intervals",
    "title": "Bootstrapping",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nLets do a bigger sample and calculate our confidence interval using 10000 random numbers between 1 and 100.\n\ndata ← ?10000/100 \n\n10↑data ⍝ look at first 10 values\n\n┌→───────────────────────────┐\n│50 3 15 43 93 60 96 29 71 58│\n└~───────────────────────────┘\n\n\n\nNext we can calculate a sampling distribution and look a a few of them. We use the code from before but with 1000 pulls.\n\nsampling_distribution←data[? (n←1000) (ss←10) ⍴ ≢ data]\n5↑sampling_distribution\n\n┌→─────────────────────────────┐\n↓20 56 92 100 34 89 28 92 10 21│\n│34 95 89  69 35 81 25 25 80 87│\n│77 68 32  77 57 20 10 20 21 95│\n│37 73 19  79 11 88 13  1 90 68│\n│70 42 10  74 62 34 82 17  3 19│\n└~─────────────────────────────┘\n\n\n\nWe want to do a confidence interval on the mean so we need to calculate the mean of each of these samples.\n\n+/ Row-wise sum (trailing axis)\nss÷⍨ divides each element by ss (ss defined when creating sampling distribution)\n\n\nsample_means ← ss÷⍨+/ sampling_distribution\n8↑sample_means\n\n┌→────────────────────────────────────┐\n│54.2 62 47.7 47.9 41.3 63.5 50.7 44.3│\n└~────────────────────────────────────┘\n\n\n\nNow we calculate at 90% confidence interval on our sample mean. That means we are 90% confident our mean will land in the given interval range. This is easy to do because we have calculated the mean of a good sampling distribution so we just need to cut off the top and bottom 5% of values and 90% of the values landed in that range.\n\n⍋ sorts ascending, then cut off first 50 and take the first 900 of that\n\n\norder90 ← 900↑50↓⍋sample_means\n\nGet min and max of middle 90% of sample means, which is our 90% confidence interval. Because our data is sorted we can just get the first and last value.\n\nsample_means[⊣/order90]\nsample_means[⊢/order90]\n\n    \n36.2\n    \n\n\n\n    \n65.5\n    \n\n\n\nWe know we are 90 percent confident that a mean based on a sample size of 10 will land in that range because we did that and found that to be true."
  },
  {
    "objectID": "posts/APL/Basic Stats.html#p-values",
    "href": "posts/APL/Basic Stats.html#p-values",
    "title": "Bootstrapping",
    "section": "P values",
    "text": "P values\nLet’s say we have 2 sets of data and we want to know whether some statistics are different between them. We have 10,000 samples of our original data, and we ran an experiment and got 100 datapoints with our new process. We want to calculate a p value to see if it supports our hypothesis that it had a statistically significant impact.\n\n\n\n\n\n\nNote\n\n\n\nStatistically significant impact does not necessarily mean practically significant. This test is doing the basic (are these 2 means different), but often that isn’t really that helpful of a question. Often we want to ask “are these 2 means different by at least X”. After reviewing the simple examples think through how you might be able to design that test via bootstrapping!\n\n\n\nP value on equal means\n\nbaseline ← 1-⍨2×⍨?10000/0\nexperiment ← 0.5-⍨?100/0\n\nThese should have roughly the same means so we should get a large p value and show the difference is not statistically significant\nLet’s run the test and see what we get. First let’s get our statistic from our experiment (mean).\n\n⎕←experiment_mean ← (+/experiment) ÷ ≢experiment\n\n             \n0.02131968282\n             \n\n\n\nNow let’s create our sampling distribution on our baseline.\n\nsampling_distribution←baseline[? (n←1000) (ss←10) ⍴ ≢ baseline]\n\nCalculate the means of each.\n\nsampling_means ← ss ÷⍨ +/sampling_distribution\n\nWe then calculate a p value by seeing what percentage of sample means our experiment mean is more extreme than. We can check this on both ends of the distribution and we would take the smaller one normally.\n\nn ÷⍨ +/ experiment_mean>sampling_means\nn ÷⍨ +/ experiment_mean<sampling_means\n\n     \n0.545\n     \n\n\n\n     \n0.455\n     \n\n\n\n\n\nP value on different means\n\nbaseline ← ?10000/0\nexperiment ← 0.2-⍨?100/0\n\nThese should have different means so we should get a large p value and show the different is not practically significant\nLet’s run the test and see what we get. First let’s get our statistic from our experiment (mean).\n\n⎕←experiment_mean ← (+/experiment) ÷ ≢experiment\n\n           \n0.302065664\n           \n\n\n\nNow let’s create our sampling distribution on our baseline.\n\nsampling_distribution← baseline[? (n←1000) (ss←10) ⍴ ≢ baseline]\n\n\nsampling_means ← ss ÷⍨ +/sampling_distribution\n\nWe then calculate a p value by seeing what percentage of sample means our experiment mean is more extreme than. We can check this on both ends of the distribution, but we would take the smaller one. We can see our p value is quite small - it successfully detected that we likely have a different mean.\n\nn ÷⍨ +/ sampling_means > experiment_mean\nn ÷⍨ +/ sampling_means < experiment_mean\n\n     \n0.993\n     \n\n\n\n     \n0.007"
  },
  {
    "objectID": "posts/Python/Python.html",
    "href": "posts/Python/Python.html",
    "title": "Python Programming Tips",
    "section": "",
    "text": "Code\nfrom functools import partial\nfrom datetime import datetime\nimport logging, string, pandas as pd, sqlparse\nfrom fastcore.all import *\nfrom fastcore.docments import *\nfrom IPython.display import Markdown,display, HTML\nimport pandas as pd\n\nfrom pygments import highlight\nfrom pygments.lexers import PythonLexer\nfrom pygments.formatters import HtmlFormatter\n\ndef print_function_source(fn):\n    fn = print_decorator\n    formatter = HtmlFormatter()\n    display(HTML('<style type=\"text/css\">{}</style>{}'.format(\n        formatter.get_style_defs('.highlight'),\n        highlight(inspect.getsource(fn), PythonLexer(), formatter))))"
  },
  {
    "objectID": "posts/Python/Python.html#parallel-processing",
    "href": "posts/Python/Python.html#parallel-processing",
    "title": "Python Programming Tips",
    "section": "Parallel Processing",
    "text": "Parallel Processing\nSee this blog post\n\nDocments\nNice way of documenting code concisely and being able to access info from code. It’s concise, easy to manipulate to display how you want, and easy to read. I much prefer it over the large numpy style docstrings that are big string blocks\n\nfrom fastcore.docments import *\n\ndef distance(pointa:tuple,  # tuple representing the coordinates of the first point (x,y)\n             pointb:tuple=(0,0) # tuple representing the coordinates of the first point (x,y)\n            )->float: # float representing distance between pointa and pointb\n    '''Calculates the distance between pointa and pointb'''\n    edges = np.abs(np.subtract(pointa,pointa))\n    distance = np.sqrt((edges**2).sum())\n    return distance\n\n\ndocstring(distance)\n\n'Calculates the distance between pointa and pointb'\n\n\n\ndocments(distance)\n\n{ 'pointa': 'tuple representing the coordinates of the first point (x,y)',\n  'pointb': 'tuple representing the coordinates of the first point (x,y)',\n  'return': 'float representing distance between pointa and pointb'}\n\n\n\ndocments(distance,full=True)\n\n{ 'pointa': { 'anno': <class 'tuple'>,\n              'default': <class 'inspect._empty'>,\n              'docment': 'tuple representing the coordinates of the first '\n                         'point (x,y)'},\n  'pointb': { 'anno': <class 'tuple'>,\n              'default': (0, 0),\n              'docment': 'tuple representing the coordinates of the first '\n                         'point (x,y)'},\n  'return': { 'anno': <class 'float'>,\n              'default': <class 'inspect._empty'>,\n              'docment': 'float representing distance between pointa and '\n                         'pointb'}}\n\n\n\n\nTesting\nEveryone agrees testing is important. But not all testing is equal. The needs for unit testing the google code base are not the same as the needs a data scientist needs for building and deploying models, libraries, and most software.\nFastcore is a great tool for most of my testing needs. Fast and simple enough that I can add tests as I build and as I am exploring and building models. I want testing to enhance my development workflow, not be something I have to painstakingly build at the end.\nSometimes simple assert statements are sufficient, but there’s small annoyances. For example, a small change in type can mean a failed test. Sometimes that change in type should cause a failure, sometimes I’m ok if it’s a different type if the values are the same\n\nfrom fastcore.test import *\n\n\ntest_eq([1,2],(1,2))\n\nFor floating points it has handy functionality for that, which is very common in data science. For example, we may want .1 + .1 + .1 == .3 to be true, because they are close enough based on floating point precision\n\n.1 + .1 + .1 == .3\n\nFalse\n\n\n\ntest_close(.1 + .1 + .1, .3)\n\nWe can test that something fails, if there are particular situation we want to ensure raise errors.\n\ndef _fail(): raise Exception(\"foobar\")\ntest_fail(_fail)\n\nWe can test if 2 lists have the same values, just in different orders (convenient for testing some situations with random mini-batches).\n\na = list(range(5))\nb = a.copy()\nb.reverse()\ntest_shuffled(a,b)\n\nThere’s more of course, check out the docs\n\n\nL\nL is a replacement for a list, but with lots of adding functionality. Some of it are functional programming concepts, some is numpy like stuff, and some is just niceities (like cleaner printing).\n\nalist = L(1,2,3,4,3)\n\n\nalist.sort()\nalist.sorted()\n\n(#5) [1,2,3,3,4]\n\n\n\nalist.unique()\n\n(#4) [1,2,3,4]\n\n\n\nalist.filter(lambda x: x < 3)\n\n(#2) [1,2]\n\n\n\nalist.map(lambda x: x * 2)\n\n(#5) [2,4,6,8,6]\n\n\n\n\nAttrDict\nAttrdict is another nice thing from fastcore, that makes dictionaries a bit nicer to use.\n\nregdict = {'a':2,'b':3}\nadict = AttrDict({'a':2,'b':3})\n\n\nadict\n\n{'a': 2, 'b': 3}\n\n\n\nadict.a\n\n2\n\n\n\ndef _fail(): return regdict.a\ntest_fail(_fail)"
  },
  {
    "objectID": "posts/Python/Python.html#filter",
    "href": "posts/Python/Python.html#filter",
    "title": "Python Programming Tips",
    "section": "Filter",
    "text": "Filter\nFilter is a common higher order function.\n\nL(1,2,3,4,5).filter(lambda x: x>3)\n\n(#2) [4,5]\n\n\nThis is very flexible because we can put filtering logic of any complexity in a function and use that to filter a list of any type."
  },
  {
    "objectID": "posts/Python/Python.html#map",
    "href": "posts/Python/Python.html#map",
    "title": "Python Programming Tips",
    "section": "Map",
    "text": "Map\nMap is another very common higher order function.\n\nL(1,2,3,4,5).map(lambda x: x**2)\n\n(#5) [1,4,9,16,25]\n\n\nIt is again super flexible because we can apply a function of any complexity to have it be applied and modify each element of the list.\n\nL(1,2,3,4,5).map(lambda x: string.ascii_lowercase[x])\n\n(#5) ['b','c','d','e','f']"
  },
  {
    "objectID": "posts/Python/Python.html#simple-logging",
    "href": "posts/Python/Python.html#simple-logging",
    "title": "Python Programming Tips",
    "section": "Simple Logging",
    "text": "Simple Logging\nWe could make a function for logging, where we can pass a function in that we want to use for logging (ie info vs warning).\n\ndef log_stuff(msg,fn=logger.info,**kwargs): \n    dt = get_current_time()\n    fn(f\"{dt}|{msg}\")\n    for k,v in kwargs.items(): fn(f\"{dt}|{k}={v}\")\n\n\nlog_stuff('abcd',a=1,b=55)\n\n\n!tail -3 mylog.log\n\nINFO:root:20221106_193211|abcd\nINFO:root:20221106_193211|a=1\nINFO:root:20221106_193211|b=55\n\n\n\nlog_stuff('something might be awry',fn=logger.critical,a=1,b=55)\n\n\n!tail -3 mylog.log\n\nCRITICAL:root:20221106_193211|something might be awry\nCRITICAL:root:20221106_193211|a=1\nCRITICAL:root:20221106_193211|b=55"
  },
  {
    "objectID": "posts/Python/Python.html#file-processor",
    "href": "posts/Python/Python.html#file-processor",
    "title": "Python Programming Tips",
    "section": "File Processor",
    "text": "File Processor\nYou can also make a generic file processor that you can pass callbacks to. This file processor can include log statements to log what you’re doing, so you can minimize repeating lots of code. For now, we’ll do a simple processor, and callbacks to clean and format a messy sql file.\n\ndef process_file(fpath,callbacks): \n    with open(fpath, \"r\") as f: contents = f.read()\n    for callback in callbacks: contents = callback(contents)\n    return contents"
  },
  {
    "objectID": "posts/Python/Python.html#format-and-clean-sql-file",
    "href": "posts/Python/Python.html#format-and-clean-sql-file",
    "title": "Python Programming Tips",
    "section": "Format and clean SQL file",
    "text": "Format and clean SQL file\n\nsql_formatter_cb = partial(sqlparse.format,\n                strip_comments=True,comma_first=True,\n                keyword_case='upper', identifier_case='lower',\n                reindent=True, indent_width=4,)\n\n\n\nqrys = process_file('test.sql',[sql_formatter_cb,sqlparse.split])\n\n\ndef sql_pprint(sql): display(Markdown(f\"```sql\\n\\n{sql}\\n\\n```\"))\nfor qry in qrys: sql_pprint(qry)\n\n\nSELECT top 25 *\nFROM some_table;\n\n\n\nSELECT count(1)\nFROM another TABLE ;\n\n\n\nSELECT date_time\n     , mbr_id\n     , transactions\n     , count(1)\nFROM table3\nWHERE date_time > '2021-02-02'\nGROUP BY 1\n       , 2\n       , 3;"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example",
    "href": "posts/Python/Python.html#silly-simple-example",
    "title": "Python Programming Tips",
    "section": "Silly Simple Example",
    "text": "Silly Simple Example\n\ndef add_another(func):\n    def wrapper(number):\n        print(f\"The decorator took over!\")\n        print(f\"I could log the original number ({number}) here!\")\n        print(f\"Or I could log the original answer ({func(number)}) here!\")\n        return func(number) + 1\n    return wrapper\n    \n@add_another\ndef add_one(number): return number + 1\n\nSo when we use a decorator, the code in the wrapper function is called instead of the original function. Typically the wrapper function calls the original function (otherwise there would be no point in decorating it as you’d just have a new unrelated function)."
  },
  {
    "objectID": "posts/Python/Python.html#useful-example",
    "href": "posts/Python/Python.html#useful-example",
    "title": "Python Programming Tips",
    "section": "Useful Example",
    "text": "Useful Example\nFor example, maybe you want to print (or log) particular function call times and the args. See this decorator that does just that (and can be used on methods too)\n\nfrom datetime import datetime\n\n\ndef print_decorator(func):\n    def wrapper(*args, **kwargs):\n        print(f\"{datetime.now()}:{func}:args={args}:kwargs={kwargs}\")\n        return func(*args, **kwargs)\n    return wrapper\n\n\n@print_decorator\ndef simple_add(a,b): return a + b\n\n\nsimple_add(2,4)\n\n2022-11-02 14:18:56.635936:<function simple_add>:args=(2, 4):kwargs={}\n\n\n6\n\n\n\n@print_decorator\ndef complex_add(a,b,*args,**kwargs): \n    out = a + b\n    for arg in args: out = out + arg\n    for kwarg in kwargs.values(): out = out + kwarg\n    return out\n\n\ncomplex_add(5,2,3,foo=6,bar=10)\n\n2022-11-02 14:18:57.716085:<function complex_add>:args=(5, 2, 3):kwargs={'foo': 6, 'bar': 10}\n\n\n26"
  },
  {
    "objectID": "posts/Python/Python.html#use-on-existing-functions",
    "href": "posts/Python/Python.html#use-on-existing-functions",
    "title": "Python Programming Tips",
    "section": "Use on Existing Functions",
    "text": "Use on Existing Functions\nWhat we have seen is applying a decorator to functions we fully define but we can also apply them to previously existing functions like ones we import from a library. This is helpful not just in understanding one way you can extend an existing libraries functionality, but also in understanding what decorators are. They aren’t magical.\nLet’s add logging to pd.DataFrame using our existing decorator so we can see when a dataframe is constructed.\n\nLoggingDataFrame = print_decorator(pd.DataFrame)\ndf = LoggingDataFrame([1,2,3])\n\n2022-11-02 14:53:16.323144:<class 'pandas.core.frame.DataFrame'>:args=([1, 2, 3],):kwargs={}\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n  \n\n\n\n\nThe key thing to notice here is that the @ syntax really isn’t doing anything magical. It’s just passing the function into the decorator and using that as the function definition. It’s just syntactic sugar for a higher order function that takes a function and returns a function.\nTo understand why this works, think through what our decorator is doing. 1. It’s a function that takes a function as an argument 2. It creates a new function called wrapper. This wrapper function called the argument passed into it, but also has other code. 3. It returns that function as the output\n\nprint_function_source(print_decorator)\n\ndef print_decorator(func):\n    def wrapper(*args, **kwargs):\n        print(f\"{datetime.now()}:{func}:args={args}:kwargs={kwargs}\")\n        return func(*args, **kwargs)\n    return wrapper"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example-1",
    "href": "posts/Python/Python.html#silly-simple-example-1",
    "title": "Python Programming Tips",
    "section": "Silly Simple Example",
    "text": "Silly Simple Example\n\nclass aClass: a = 2\n    \nclass bClass(aClass): pass\n    \naClass.a == bClass.a\n\nTrue"
  },
  {
    "objectID": "posts/Python/Python.html#useful-examples",
    "href": "posts/Python/Python.html#useful-examples",
    "title": "Python Programming Tips",
    "section": "Useful Examples",
    "text": "Useful Examples\nIn many cases there are common things we want to inherit in lots of classes. One example is having access to the date. Often you want this for logging, or printing, or any number of things. By subclassing you don’t have to reformat the date each time in your classes.\n\nclass DateMinuteMixin:\n    date_format='%Y%m%d_%H%M%S'\n    dte = datetime.now()\n\n    @property\n    def date_str(self): return self.dte.strftime(self.date_format)\n\nAnother handy use is to have generic behavior for handling different file types. In this case, we have a mixin where it opens and reads a sql file. Rather than rewriting this code for every class that needs to read a sql file, you can inherit from a class when you need that functionality.\n\n\n\n\n\n\nTip\n\n\n\nYou can define an abstract property like below to let users know that after inheriting this class, they need to define that property. In this case, they define the sql_filepath, and they get the contents of the file for free via the other methods.\n\n\n\nimport abc\n\nclass SqlFileMixin:\n    @abc.abstractproperty\n    def sql_filepath(self):\n        pass\n\n    @property\n    def sql_file(self):\n        return open(self.sql_filepath)\n\n    @property\n    def query(self):\n        return self.sql_file.read()"
  },
  {
    "objectID": "posts/Python/Python.html#silly-simple-example-2",
    "href": "posts/Python/Python.html#silly-simple-example-2",
    "title": "Python Programming Tips",
    "section": "Silly Simple Example",
    "text": "Silly Simple Example\n\ndef mapper(items,fn):\n    for item in items: yield item\n\n\nit = mapper([2,4,6,8],square)\nit\n\n<generator object mapper>\n\n\n\nnext(it), next(it), next(it)\n\n(2, 4, 6)\n\n\nYou can also process it sequentially in a loop.\n\nfor item in mapper([2,4,6,8],square): \n    print(item)\n\n2\n4\n6\n8"
  },
  {
    "objectID": "posts/Python/Python.html#useful-example-1",
    "href": "posts/Python/Python.html#useful-example-1",
    "title": "Python Programming Tips",
    "section": "Useful Example",
    "text": "Useful Example\n\nFile Streaming\n\nprint_plus = partial(print,end='\\n++++++\\n')\n\nwith open('test.txt', 'rb') as f:\n    iterator = iter(partial(f.read, 64), b'')\n    print_plus(type(iterator))\n    for block in iterator: print_plus(block)\n\n<class 'callable_iterator'>\n++++++\nb'one\\ntwo\\nthree\\nfour\\nfive\\nsix\\nseven\\neight\\nnine\\nten\\neleven\\ntwelve\\nt'\n++++++\nb'hirteen\\nninety nine thousand nine hundred ninety\\nninety nine tho'\n++++++\nb'usand nine hundred ninety one\\nninety nine thousand nine hundred '\n++++++\nb'ninety two\\nninety nine thousand nine hundred ninety three\\nninety'\n++++++\nb' nine thousand nine hundred ninety four\\nninety nine thousand nin'\n++++++\nb'e hundred ninety five\\nninety nine thousand nine hundred ninety s'\n++++++\nb'ix\\nninety nine thousand nine hundred ninety seven\\nninety nine th'\n++++++\nb'ousand nine hundred ninety eight\\nninety nine thousand nine hundr'\n++++++\nb'ed ninety nine\\n'\n++++++"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html",
    "href": "posts/Clustering/KMeansFromScratch.html",
    "title": "K-Means From Scratch",
    "section": "",
    "text": "import math, random, matplotlib.pyplot as plt, operator, torch\nfrom functools import partial\nfrom fastcore.all import *\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch import tensor\n\n\ntorch.manual_seed(42)\ntorch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n\n\ndef plot_data(centroids:torch.Tensor,# Centroid coordinates\n              data:torch.Tensor, # Data Coordinates\n              n_samples:int, # Number of samples\n              ax:plt.Axes=None # Matplotlib Axes object\n             )-> None:\n    '''Creates a visualization of centroids and data points for clustering problems'''\n    if ax is None: _,ax = plt.subplots()\n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples:(i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color='k', mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color='m', mew=2)"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#calculate-distance",
    "href": "posts/Clustering/KMeansFromScratch.html#calculate-distance",
    "title": "K-Means From Scratch",
    "section": "Calculate Distance",
    "text": "Calculate Distance\nIn order to initialize our centroids we need to be able to calculate distances, so let’s do that first.\nGiven a tensor of centroid coordinates and a tensor of data coordinates we calculate distance by: + Subtract centroids coordinates from data points coordinates + Take absolute value of distances + Pythagorean Calculation + Square coordinates + Add them together + Take the Square Root\nThat gives us the euclidean distance between each data point and each centroid.\n\ndef calculate_distances(centroids:torch.Tensor, # Centroid coordinates\n                        data:torch.Tensor # Data points you want to cluster\n                       )-> torch.Tensor: # Tensor containing euclidean distance between each centroid and data point    \n    '''Calculate distance between centroids and each datapoint'''\n    axis_distances = data.reshape(-1,1,2).sub(centroids.reshape(1,-1,2)).abs()\n    euclid_distances = axis_distances.square().sum(axis=-1).sqrt()\n    return euclid_distances"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#initialize-centroids",
    "href": "posts/Clustering/KMeansFromScratch.html#initialize-centroids",
    "title": "K-Means From Scratch",
    "section": "Initialize Centroids",
    "text": "Initialize Centroids\nWhere we initialize our centroids is really important. If we don’t have good initialization we are very likely to get stuck in a local optimum. Especially with 6 centroids. One option is to run the algorithm many times and pick the best solution, but it’s a much better idea to try to have good initializations.\nWe pick centroid locations in the following way:\n\nPick a random data point and use those coordinates as the first centroid\nLoop to create remaining centroids\n\nCalculate the distance between existing centroids and data points.\nGet the distance from each data point to it’s closest centroid\nPlace the next centroid at the point with the max distance from previous step\n\n\nThis ensures we get initialization that are nice and far away from each other and spread out amonth the data, minimizing the risk of hitting local optimums.\n\ndef initialize_centroids(data:torch.Tensor,# Data points you want to cluster\n                         k:torch.Tensor # Number of centroids you want to initialize\n                        )->torch.Tensor: # Returns starting centroid coordinates\n    '''Initialize starting points for centroids as far from each other as possible.'''\n    pred_centroids = data[random.sample(range(0,len(data)),1)]\n    for i in range(k-1): \n        _centroid = data[calculate_distances(pred_centroids,data).min(axis=1).values.argmax()]\n        pred_centroids = torch.stack([*pred_centroids,_centroid])\n    return pred_centroids"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#classify-data-points",
    "href": "posts/Clustering/KMeansFromScratch.html#classify-data-points",
    "title": "K-Means From Scratch",
    "section": "Classify Data Points",
    "text": "Classify Data Points\nOnce we have centroids (or updated centroids), we need to assign a centroid to each data point. We do this by calculating the distance between each data point and each centroid, and assigning each datapoint to it’s closes centroid.\n\ndef assign_centroids(centroids:torch.Tensor, # Centroid coordinates\n                     data:torch.Tensor # Data points you want to cluster\n                    )->torch.Tensor: # Tensor containing new centroid assignments for each data point\n    '''Based on distances update centroid assignments'''\n    euclid_distances = calculate_distances(centroids,data)\n    assigned_cluster = euclid_distances.squeeze().argmin(axis=1)\n    return assigned_cluster"
  },
  {
    "objectID": "posts/Clustering/KMeansFromScratch.html#update-centroids",
    "href": "posts/Clustering/KMeansFromScratch.html#update-centroids",
    "title": "K-Means From Scratch",
    "section": "Update Centroids",
    "text": "Update Centroids\nTo update the centroid locations, we take the mean of all the data point assigned to that centroid. We make the new centroid that point.\n\ndef update_centroids(centroid_assignments:torch.Tensor, # Centroid coordinates\n                     data:torch.Tensor # Data points you want to cluster\n                    )->torch.Tensor: # Tensor containing updated centroid coodinates\n    '''Update centroid locations'''\n    n_centroids = len(centroid_assignments.unique())\n    pred_centroids = [data[centroid_assignments==i].mean(axis=0) for i in range(n_centroids)]\n    return torch.stack(pred_centroids)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac’s Tech Blog",
    "section": "",
    "text": "Python Programming Tips\n\n\n\n\n\n\n\nPython\n\n\nProgramming\n\n\n\n\nA list of handy tips and tricks when programming in python\n\n\n\n\n\n\nNov 6, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nK-Means From Scratch\n\n\n\n\n\n\n\nClustering\n\n\n\n\nA deep dive on K-Means where smart initialization and the full algorithm is implemented from scratch using pytorch\n\n\n\n\n\n\nNov 5, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nBootstrapping\n\n\n\n\n\n\n\nAPL\n\n\nStatistics\n\n\n\n\nSampling and Bootstrapping Statistics in APL\n\n\n\n\n\n\nJul 20, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Multiplication\n\n\n\n\n\n\n\nAPL\n\n\nLinear Algebra\n\n\n\n\nMatrix multiplication deep dive in APL\n\n\n\n\n\n\nJul 10, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nNumerical Linear Algebra Part 1\n\n\n\n\n\n\n\nAPL\n\n\nLinear Algebra\n\n\n\n\nNumerical Linear Algebra Part 1 with translations to APL\n\n\n\n\n\n\nJul 10, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\n  \n\n\n\n\nTabular Data Intro\n\n\n\n\n\n\n\nAPL\n\n\nTabular Data\n\n\n\n\nIntroduction to tabular data analysis in APL\n\n\n\n\n\n\nJul 7, 2022\n\n\nIsaac Flath\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a data scientist with a varied background that likes doing varied things. It’s also given me a wide range of skills that prove useful in all sorts of surprising ways.\nMy most important ability is understanding to goal and being able to see all the detailed steps needed to get there (and seeing the detailed steps that are not needed). I developed this skill by getting experience in very different roles, where I often had to do things I did not have any idea how to do. The best way to do that is to break the task into pieces that you can research separately.\nOften, nobody in the world has done exactly what you are doing. But it’s almost a certainty that whatever you want to do is a combination of things that are already out there. If you see the pieces you can learn what you need as you go.\nMy primary hobby is dance. I used to teach ballroom dance, but now I mostly dance West Coast Swing as a hobby.\n\n\n\n\n**My Start*: assembly line worker ➢ assembly line management ➢ assembly line efficiency optimization\nMoving up: business process engineering ➢ product management\nMy Data Start: data Analyst ➢ dynamics CRM developer\nTrying New Things: accounting ➢ call center ➢ full-time ballroom dance teacher\nFinding my home ❤️: product owner ➢ machine learning researcher ➢ data scientist"
  }
]